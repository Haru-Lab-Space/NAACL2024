{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install scikit-learn\n",
    "# !pip install focal-loss-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxility Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "# Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video, read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Model\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer, BertModel, BertTokenizer, BertGenerationDecoder, GPT2Config, GPT2Model, GPT2LMHeadModel\n",
    "\n",
    "# Training parameter\n",
    "from torch.optim import Adam\n",
    "# Training process\n",
    "from tqdm import tqdm\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import sys\n",
    "from focal_loss.focal_loss import FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "def write_json(path, data):\n",
    "    with open(path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "def mkdir(path):\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(path)\n",
    "        print(\"The new directory checkpoint is created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, config_path, data, option=\"train\"):\n",
    "        super(ConversationDataset, self).__init__()\n",
    "        self.build(config_path)\n",
    "        self.data = data\n",
    "        self.option = option\n",
    "    def build(self, config_path):\n",
    "        config = read_json(config_path)\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.id2label = config[\"id2label\"]\n",
    "        self.tokenizer_name = config[\"tokenizer_name\"]\n",
    "        self.padding = config[\"padding\"]\n",
    "        self.paragraph_max_length = config[\"paragraph_max_length\"]\n",
    "        self.text_max_length = config[\"text_max_length\"]\n",
    "        self.left_padding = config[\"left_padding\"]\n",
    "        self.right_padding = config[\"right_padding\"]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.text_pool_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        # self.text_pool_tokenizer.add_special_tokens({'pad_token': 'eos_token_id'})\n",
    "        self.text_pool_tokenizer.pad_token = self.text_pool_tokenizer.eos_token\n",
    "    def span(self, text, subtext, ss=0):\n",
    "        if len(subtext) == 0:\n",
    "            return []\n",
    "        for i in range(ss, len(text) - len(subtext)):\n",
    "            for j in range(len(subtext)):\n",
    "                if text[i+j] != subtext[j]:\n",
    "                    match = 0\n",
    "                    break\n",
    "                else:\n",
    "                    match += 1\n",
    "                    if match == len(subtext):\n",
    "                        return [i, i+len(subtext)]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        conversation_ID = self.data[index][\"conversation_ID\"]\n",
    "        paragraph = self.tokenizer(self.data[index][\"paragraph\"], padding=self.padding, max_length=self.paragraph_max_length, return_tensors=\"pt\")\n",
    "        paragraph = torch.squeeze(paragraph['input_ids'])\n",
    "        utterance_ID = self.data[index][\"utterance_ID\"]\n",
    "        text_pool = self.data[index][\"text_pool\"]\n",
    "        speaker_pool = self.data[index][\"speaker_pool\"]\n",
    "        token_text_pool = []\n",
    "        token_speaker_pool = []\n",
    "\n",
    "        for i in range(len(text_pool)):\n",
    "            text = self.tokenizer.encode(text_pool[i], padding=self.padding, max_length=self.text_max_length)\n",
    "            speaker = self.tokenizer(speaker_pool[i], padding=self.padding, max_length=8)[\"input_ids\"]\n",
    "            token_text_pool.append(text)\n",
    "            token_speaker_pool.append(speaker)\n",
    "\n",
    "        if len(text_pool) < self.left_padding+self.right_padding+1:\n",
    "            for i in range(len(text_pool), self.left_padding+self.right_padding+1):\n",
    "                text = self.tokenizer.encode(\"\", padding=self.padding, max_length=self.text_max_length)\n",
    "                speaker = self.tokenizer(\"\", padding=self.padding, max_length=8)[\"input_ids\"]\n",
    "                token_text_pool.insert(0, text)\n",
    "                token_speaker_pool.insert(0, speaker)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.option == \"train\":\n",
    "            label = self.label2id[self.data[index][\"emotion\"][\"property\"]]\n",
    "            emotion_label = [0] * len(self.id2label)\n",
    "            emotion_label[label] = 1\n",
    "            emotion_label = torch.FloatTensor(emotion_label)\n",
    "            casual_pool = self.data[index][\"emotion\"][\"casual\"]\n",
    "            token_casual = []\n",
    "\n",
    "\n",
    "            list = []\n",
    "         \n",
    "            for i in range(len(casual_pool)):\n",
    "                casual = [casual_pool[i][\"start\"], casual_pool[i][\"end\"]]\n",
    "                token_casual.append(casual)\n",
    "                token = self.tokenizer.encode(casual_pool[i][\"casual_text\"], add_special_tokens=False)\n",
    "                list.extend(self.span(paragraph.tolist(), token))\n",
    "            # print(\" token_casual: \"+str(len(token_casual)))\n",
    "\n",
    "            for i in range(len(casual_pool), self.left_padding+self.right_padding+1):\n",
    "                casual = [0, 0]\n",
    "                token_casual.insert(0, casual)\n",
    "            print(casual_pool)\n",
    "            print(list)\n",
    "            paragraph_label = [0] * self.paragraph_max_length\n",
    "            for i in list:\n",
    "                paragraph_label[i] = 1\n",
    "\n",
    "\n",
    "            return {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"paragraph\": paragraph,\n",
    "                \"paragraph_label\": torch.FloatTensor(paragraph_label),\n",
    "                \"utterance_ID\": utterance_ID,\n",
    "                \"token_text_pool\": torch.FloatTensor(token_text_pool),\n",
    "                \"token_speaker_pool\": torch.FloatTensor(token_speaker_pool),\n",
    "                \"emotion_label\": emotion_label,\n",
    "                \"span_label\": torch.FloatTensor(token_casual)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"paragraph\": paragraph,\n",
    "                \"utterance_ID\": utterance_ID,\n",
    "                \"token_text_pool\": torch.FloatTensor(token_text_pool),\n",
    "                \"token_speaker_pool\": torch.FloatTensor(token_speaker_pool),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1045, 2572, 26363]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.encode(\"I am Duc\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.layers_to_freeze = []\n",
    "        self.history = {\n",
    "            \"train loss\": [],\n",
    "            \"valid loss\": [],\n",
    "            \"emotion_correct\": [],\n",
    "            \"span_correct\": []\n",
    "        }\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        print(config_path)\n",
    "        config = read_json(config_path)\n",
    "        self.patience = config['patience']\n",
    "        self.min_delta = config['min_delta']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.device = config['device']\n",
    "        self.save_checkpoint_dir = config['output_dir'] + \"/checkpoint\"\n",
    "        self.save_history_dir = config['output_dir'] + \"/history\"\n",
    "        self.emotion_head_layers = config['emotion_head_layers']\n",
    "        self.casual_head_layers = config['casual_head_layers']\n",
    "        self.decoder_layers = config['decoder_layers']\n",
    "        self.encoder_layers = config['encoder_layers']\n",
    "        self.lstm_layers = config['lstm_layers']\n",
    "        self.alpha_focal_loss = config['alpha_focal_loss']\n",
    "        self.gamma_focal_loss = config['gamma_focal_loss']\n",
    "        self.label2id = config['label2id']\n",
    "        self.target_names = self.label2id.keys()\n",
    "        self.early_stopper = EarlyStopper(self.patience, self.min_delta)\n",
    "\n",
    "        mkdir(self.save_history_dir)\n",
    "\n",
    "    def emotion_head_training(self):\n",
    "        self.layers_to_freeze = self.casual_head_layers + self.encoder_layers + self.lstm_layers\n",
    "        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n",
    "    def emotion_encoder_training(self):\n",
    "        self.layers_to_freeze = self.casual_head_layers + self.lstm_layers\n",
    "        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n",
    "    def emotion_encoder_lstm_training(self):\n",
    "        self.layers_to_freeze = self.casual_head_layers + self.decoder_layers \n",
    "    def emotion_casual_head_training(self):\n",
    "        self.layers_to_freeze = self.encoder_layers + self.lstm_layers\n",
    "    def emotion_casual_lstm_head_training(self):\n",
    "        self.layers_to_freeze = self.encoder_layers\n",
    "        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n",
    "    def emotion_lstm_training(self):\n",
    "        self.layers_to_freeze = self.casual_head_layers + self.encoder_layers\n",
    "        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n",
    "        \n",
    "    def casual_head_training(self):\n",
    "        self.layers_to_freeze = self.emotion_head_layers + self.encoder_layers + self.lstm_layers\n",
    "\n",
    "    def full_training(self):\n",
    "        self.layers_to_freeze = []\n",
    "    def freeze(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(layer in name for layer in self.layers_to_freeze):\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        return model\n",
    "    def train(self, model, optimizer, train_dataset, valid_dataset, epochs, option=\"emotion_head\", batch_size=None):\n",
    "        self.optimizer = optimizer\n",
    "        if option == \"emotion_head\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_head_training()\n",
    "        elif option == \"emotion_encoder\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_encoder_training()\n",
    "        elif option == \"emotion_lstm\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_lstm_training()\n",
    "        elif option == \"emotion_encoder_lstm\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_encoder_lstm_training()\n",
    "        elif option == \"emotion_casual_head\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_casual_head_training()\n",
    "        elif option == \"emotion_casual_lstm_head\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_casual_lstm_head_training()\n",
    "        elif option == \"casual_head\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.casual_head_training()\n",
    "\n",
    "        if batch_size == None:\n",
    "            self.batch_size = self.batch_size_config\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        mkdir(self.save_checkpoint_dir + \"/\" + option)\n",
    "        self.fn_loss = nn.CrossEntropyLoss()\n",
    "        self.span_fn_loss = nn.MSELoss()\n",
    "\n",
    "        \n",
    "        print(\"device: \"+str(self.device))\n",
    "        print(\"batch_size: \"+str(self.batch_size))\n",
    "        print(\"gamma_focal_loss: \"+str(self.gamma_focal_loss))\n",
    "        print(\"alpha_focal_loss: \"+str(self.alpha_focal_loss))\n",
    "\n",
    "        self.early_stopper.reset()\n",
    "        self.model = self.freeze(model)\n",
    "        # if self.device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "        #     print(\"Training Parallel!\")\n",
    "        #     self.model = nn.DataParallel(self.model)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.train_dataloader = DataLoader(dataset=train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        self.valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        self.epochs = epochs\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self._train_epoch(epoch, option)\n",
    "            val_loss, emotion_correct, span_correct, emotion_label_list, emotion_pred_list = self._val_epoch(epoch, option)\n",
    "            \n",
    "            self.history[\"train loss\"].append(train_loss)\n",
    "            self.history[\"valid loss\"].append(val_loss)\n",
    "            self.history[\"emotion_correct\"].append(emotion_correct)\n",
    "            self.history[\"span_correct\"].append(span_correct)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.save_checkpoint(epoch, option)\n",
    "                print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f},  Valid loss {val_loss:.4f},  emotion_correct {emotion_correct*100:.4f}%,  span_correct {span_correct*100:.4f}%\")\n",
    "                print(classification_report(emotion_label_list, emotion_pred_list, target_names=self.target_names))\n",
    "            if self.early_stopper.early_stop(val_loss):\n",
    "                print(\"Early stop at epoch: \"+str(epoch) + \" with valid loss: \"+str(val_loss))\n",
    "                break\n",
    "\n",
    "        \n",
    "        write_json(self.save_history_dir + \"/\" + option + \"_history.json\", self.history) \n",
    "        \n",
    "    def _train_epoch(self, epoch, option):\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "        logger_message = f'Training epoch {epoch}/{self.epochs}'\n",
    "\n",
    "        progress_bar = tqdm(self.train_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        for batch, data in enumerate(progress_bar):\n",
    "            conversation_id = data[\"conversation_ID\"]\n",
    "            paragraph = data[\"paragraph\"].to(self.device)\n",
    "            utter_id = data[\"utterance_ID\"]\n",
    "            token_text_pool = data[\"token_text_pool\"].to(self.device)\n",
    "            token_speaker_pool = data[\"token_speaker_pool\"].to(self.device)\n",
    "            emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "            span_label = data[\"span_label\"].to(self.device)\n",
    "            # print(\"conversation_id:\"+str(conversation_id.shape))\n",
    "            # print(\"paragraph:\"+str(paragraph.shape))\n",
    "            # print(\"casual_pool:\"+str(casual_pool.shape))\n",
    "            # print(\"emotion_label:\"+str(emotion_label.shape))\n",
    "            # print(\"span_label:\"+str(span_label.shape))\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            emotion_pred, span_pred = self.model(paragraph, token_text_pool, token_speaker_pool)\n",
    "            # print(\"emotion_label : \"+str(emotion_label.shape))\n",
    "            # print(\"span_label : \"+str(span_label.shape))\n",
    "            # print(\"emotion_pred : \"+str(emotion_pred.shape))\n",
    "            # print(\"span_pred : \"+str(span_pred.shape))\n",
    "            \n",
    "            emotion_label = torch.argmax(emotion_label, dim=-1)\n",
    "            if option == \"emotion_head\":\n",
    "                loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "            elif option == \"emotion_lstm\":\n",
    "                loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "            elif option == \"emotion_encoder\":\n",
    "                loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "            elif option == \"emotion_encoder_lstm\":\n",
    "                loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "            elif option == \"emotion_casual_head\":\n",
    "                emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                loss = emotion_loss + span_loss\n",
    "            elif option == \"emotion_casual_lstm_head\":\n",
    "                emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                loss = emotion_loss + span_loss\n",
    "            elif option == \"casual_head\":\n",
    "                loss = self.fn_loss(span_pred, span_label)\n",
    "            else:\n",
    "                emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                loss = emotion_loss + span_loss\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return train_loss / len(self.train_dataloader)\n",
    "\n",
    "    def _val_epoch(self, epoch, option):\n",
    "        self.model.eval()\n",
    "        valid_loss = 0.0\n",
    "        emotion_correct=0\n",
    "        span_correct=0\n",
    "        emotion_pred_list = []\n",
    "        emotion_label_list = []\n",
    "        logger_message = f'Validation epoch {epoch}/{self.epochs}'\n",
    "        progress_bar = tqdm(self.valid_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(progress_bar):\n",
    "                conversation_id = data[\"conversation_ID\"]\n",
    "                paragraph = data[\"paragraph\"].to(self.device)\n",
    "                utter_id = data[\"utterance_ID\"]\n",
    "                token_text_pool = data[\"token_text_pool\"].to(self.device)\n",
    "                token_speaker_pool = data[\"token_speaker_pool\"].to(self.device)\n",
    "                emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "                span_label = data[\"span_label\"].to(self.device)\n",
    "                emotion_pred, span_pred = self.model(paragraph, token_text_pool, token_speaker_pool)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                emotion_label = torch.argmax(emotion_label, dim=-1)\n",
    "                if option == \"emotion_head\":\n",
    "                    loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                elif option == \"emotion_lstm\":\n",
    "                    loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                elif option == \"emotion_encoder\":\n",
    "                    loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                elif option == \"emotion_encoder_lstm\":\n",
    "                    loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                elif option == \"emotion_casual_head\":\n",
    "                    emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                    span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                    loss = emotion_loss + span_loss\n",
    "                elif option == \"emotion_casual_lstm_head\":\n",
    "                    emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                    span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                    loss = emotion_loss + span_loss\n",
    "                elif option == \"casual_head\":\n",
    "                    loss = self.fn_loss(span_pred, span_label)\n",
    "                else:\n",
    "                    emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                    span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                    loss = emotion_loss + span_loss\n",
    "\n",
    "                # emotion_label_ = torch.argmax(emotion_label, dim=-1)\n",
    "                emotion_pred_ = torch.argmax(emotion_pred, dim=-1)\n",
    "                emotion_pred_list.extend(emotion_pred_.cpu().tolist())\n",
    "                emotion_label_list.extend(emotion_label.cpu().tolist())\n",
    "                # print(\"emotion_label_: \"+ str(emotion_label_))\n",
    "                # print(\"emotion_pred_: \"+ str(emotion_pred_))\n",
    "                valid_loss += loss.item()\n",
    "                emotion_correct += (emotion_pred_ == emotion_label).sum().item()\n",
    "                span_correct += (span_pred == span_label).sum().item()\n",
    "        \n",
    "        return valid_loss / len(self.valid_dataloader), emotion_correct / len(self.valid_dataloader) / self.batch_size, span_correct / len(self.valid_dataloader) / self.batch_size / 16, emotion_label_list, emotion_pred_list\n",
    "        \n",
    "    def save_checkpoint(self, epoch, option):\n",
    "        checkpoint_path = os.path.join(self.save_checkpoint_dir, option + '/checkpoint_{}.pth'.format(epoch))\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(Prediction, self).__init__()\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.id2label = config[\"id2label\"]\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.submit_path = config[\"submit path\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "    def predict(self, model, raw_dataset, dataset):\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "        pred_list = []\n",
    "        \n",
    "        self.test_dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        logger_message = f'Predict '\n",
    "        progress_bar = tqdm(self.test_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(progress_bar):\n",
    "                conversation_id = data[\"conversation_ID\"]\n",
    "                paragraph = data[\"paragraph\"].to(self.device)\n",
    "                utter_id = data[\"utterance_ID\"]\n",
    "                token_text_pool = data[\"token_text_pool\"].to(self.device)\n",
    "                token_speaker_pool = data[\"token_speaker_pool\"].to(self.device)\n",
    "                emotion_pred, span_pred = self.model(paragraph, token_text_pool, token_speaker_pool)\n",
    "                \n",
    "                pred = self.convertpred2list(conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool)\n",
    "                pred_list.extend(pred)\n",
    "        submit = self.convertdict2submit(self.convertpred2dict(raw_dataset, pred_list))\n",
    "        self.save_json(self.submit_path, submit)\n",
    "    def convertpred2list(self, conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool):\n",
    "        pred_list = []\n",
    "        for i in range(len(emotion_pred)):\n",
    "            emotion = self.id2label[str(torch.argmax(emotion_pred[i]).item())]\n",
    "            emotion_string = str(utter_id[i].item()) + \"_\" + emotion\n",
    "            if emotion != \"neutral\":\n",
    "                # \n",
    "                span = torch.round(span_pred[i])\n",
    "                # print(\"span: \"+str(span))\n",
    "                # print(\"casual_span_pool: \"+str(casual_span_pool[i]))\n",
    "                for j in range(len(span)):\n",
    "                    if span[j] == 1:\n",
    "                        if casual_span_pool[i][j][0].item() == 0:\n",
    "                            break\n",
    "                        if casual_span_pool[i][j][1].item() == casual_span_pool[i][j][2].item():\n",
    "                            break\n",
    "                        span_string = str(int(casual_span_pool[i][j][0].item())) + \"_\" + str(int(casual_span_pool[i][j][1].item())) + \"_\" + str(int(casual_span_pool[i][j][2].item()))\n",
    "                        pred_dict = {}\n",
    "                        pred_dict[\"conversation_ID\"] = conversation_id[i].item()\n",
    "                        pred_dict[\"emotion-cause_pairs\"] = [emotion_string, span_string]\n",
    "                        # print(\"pred_dict: \"+str(pred_dict))\n",
    "                        pred_list.append(pred_dict)\n",
    "        return pred_list\n",
    "    def convertpred2dict(self, raw_dataset, pred_list):\n",
    "        dataset = self.convertlist2dict(raw_dataset)\n",
    "        # dict = \n",
    "        for i in range(len(pred_list)):\n",
    "            pred = pred_list[i]\n",
    "            conversation_id = pred[\"conversation_ID\"]\n",
    "            # print(\"conversation_id: \"+str(conversation_id))\n",
    "            emotion_cause_pairs = pred[\"emotion-cause_pairs\"]\n",
    "            # print(\"emotion_cause_pairs: \"+str(emotion_cause_pairs))\n",
    "            if \"emotion-cause_pairs\" in dataset[conversation_id]:\n",
    "                dataset[conversation_id][\"emotion-cause_pairs\"].append(emotion_cause_pairs)\n",
    "            else:\n",
    "                dataset[conversation_id][\"emotion-cause_pairs\"] = []\n",
    "        # print(dataset)\n",
    "        return dataset\n",
    "    def convertlist2dict(self, raw_dataset):\n",
    "        dataset = {}\n",
    "        for i in range(len(raw_dataset)):\n",
    "            conversation = raw_dataset[i]\n",
    "            conversation_ID = conversation[\"conversation_ID\"]\n",
    "            # if conversation_ID not in dataset.key\n",
    "            dataset[conversation_ID] = {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"conversation\": conversation['conversation']\n",
    "            }\n",
    "        return dataset\n",
    "    def convertdict2submit(self, raw_dataset):\n",
    "        dataset = []\n",
    "        for key in raw_dataset.keys():\n",
    "            # print(\"key: \"+str(key))\n",
    "            sample = raw_dataset[key]\n",
    "            # print(\"sample: \"+str(sample))\n",
    "            dataset.append(sample)\n",
    "        return dataset\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    def save_json(self, file_path, data):\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGG(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(TGG, self).__init__()\n",
    "        self.build(config_path)\n",
    "        self.lstm = nn.LSTM(self.text_max_length, self.hidden_size, self.num_layers, batch_first=True, dropout=0.2, bidirectional=True)\n",
    "        self.lstma_projection = nn.Linear(2*self.num_layers*self.hidden_size, self.embedding_dim)\n",
    "        self.cat_layer = nn.Linear(2*self.embedding_dim, self.embedding_dim)\n",
    "        self.emotion_fc = nn.Linear(self.embedding_dim, self.label_num)\n",
    "        self.casual_feedforward = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim , self.feedforward_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.feedforward_dim , 2 * self.num_text),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        # self.casual_fc = nn.Linear(self.embedding_dim, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "        self.feedforward_dim = config[\"feedforward_dim\"]\n",
    "        self.encoder_name = config[\"encoder_name\"]\n",
    "        self.decoder_name = config[\"decoder_name\"]\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.label_num = len(self.label2id)\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.paragraph_max_length = config[\"paragraph_max_length\"]\n",
    "        self.text_max_length = config[\"text_max_length\"]\n",
    "        self.left_padding = config[\"left_padding\"]\n",
    "        self.right_padding = config[\"right_padding\"]\n",
    "        self.num_text = 1 + self.left_padding + self.right_padding\n",
    "        self.device = config[\"device\"]\n",
    "        \n",
    "        main_part = AutoModel.from_pretrained(self.encoder_name)\n",
    "        self.embeddings = main_part.embeddings\n",
    "        self.encoder = main_part.encoder\n",
    "        self.pooler = main_part.pooler\n",
    "\n",
    "        main_part_2 = GPT2LMHeadModel.from_pretrained(self.decoder_name, output_hidden_states =True)\n",
    "        \n",
    "    def forward(self, paragraph, token_text_pool, token_speaker_pool):\n",
    "        batch_size = paragraph.size(0)\n",
    "        candidate_size = token_text_pool.size(1)\n",
    "        t = token_text_pool.size(2)\n",
    "        device = paragraph.get_device()\n",
    "\n",
    "        h0 = torch.randn((2 * self.num_layers, batch_size, self.hidden_size), device=device)\n",
    "        c0 = torch.randn((2 * self.num_layers, batch_size, self.hidden_size), device=device)\n",
    "        output, (hn, cn) = self.lstm(token_text_pool, (h0, c0))\n",
    "        hn = hn.permute(1,0,2)\n",
    "        hn = self.lstma_projection(hn.reshape(batch_size, -1))\n",
    "        # print(\"paragraph:\"+str(paragraph.shape))\n",
    "        \n",
    "        _paragraph = self.embeddings(paragraph)\n",
    "        _paragraph = self.encoder(_paragraph)['last_hidden_state']\n",
    "        _paragraph = self.pooler(_paragraph)\n",
    "        # print(\"_paragraph:\"+str(_paragraph.shape))\n",
    "\n",
    "        hidden_state = torch.cat([hn, _paragraph], dim=-1)\n",
    "\n",
    "        _x = self.cat_layer(hidden_state)\n",
    "        # print(\"_x:\"+str(_x.shape))\n",
    "        _e_category = self.emotion_fc(_x)\n",
    "        # print(\"_e_category:\"+str(_e_category.shape))\n",
    "\n",
    "        _x = self.casual_feedforward(_x)\n",
    "        # print(\"_x:\"+str(_x.shape))\n",
    "        _x = _x.reshape(batch_size, self.num_text, 2)\n",
    "        # print(\"_x:\"+str(_x.shape))\n",
    "\n",
    "        return self.softmax(_e_category), _x\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ConversationDataset(config_path, train_data)\n\u001b[1;32m      8\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m ConversationDataset(config_path, valid_data)\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTGG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m, in \u001b[0;36mTGG.__init__\u001b[0;34m(self, config_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config_path):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(TGG, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLSTM(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_max_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstma_projection \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim)\n",
      "Cell \u001b[0;32mIn[19], line 40\u001b[0m, in \u001b[0;36mTGG.build\u001b[0;34m(self, config_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m main_part\u001b[38;5;241m.\u001b[39mencoder\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;241m=\u001b[39m main_part\u001b[38;5;241m.\u001b[39mpooler\n\u001b[0;32m---> 40\u001b[0m main_part_2 \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/transformers/modeling_utils.py:3736\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mcan_generate() \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3735\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3736\u001b[0m         model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[43mGenerationConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3740\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3741\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3742\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3743\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3744\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3745\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3746\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3747\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3748\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3749\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3750\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   3751\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   3752\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneration config file not found, using a generation config created from the model config.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3753\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:741\u001b[0m, in \u001b[0;36mGenerationConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name, config_file_name, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m config_file_name\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    755\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/transformers/utils/hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m http_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1628\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/huggingface_hub/file_download.py:408\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:67\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     69\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/http/client.py:1348\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config_path = \"config.json\"\n",
    "train_data = read_json(\"../data/ECF 2.0/train/augment_train.json\")\n",
    "valid_data = read_json(\"../data/ECF 2.0/train/augment_valid.json\")\n",
    "# trial_data = read_json(\"../data/ECF 2.0/trial/trial.json\")\n",
    "# raw_trial_data = read_json(\"../data/ECF 2.0/trial/Subtask_1_trial.json\")\n",
    "\n",
    "train_dataset = ConversationDataset(config_path, train_data)\n",
    "valid_dataset = ConversationDataset(config_path, valid_data)\n",
    "model = TGG(config_path)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(\"\\\"\" + name + \"\\\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING emotion_casual_head MODEL\n",
      "device: cuda\n",
      "batch_size: 128\n",
      "gamma_focal_loss: 2\n",
      "alpha_focal_loss: [1, 5, 5, 1, 1, 3, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0/100: 100%|| 101/101 [01:28<00:00,  1.14it/s]\n",
      "Validation epoch 0/100: 100%|| 26/26 [00:24<00:00,  1.04it/s]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss 18.7939,  Valid loss 17.6089,  emotion_correct 17.1575%,  span_correct 45.4515%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.00      0.00      0.00       461\n",
      "        fear       0.19      0.49      0.28       461\n",
      "         joy       0.17      0.55      0.26       461\n",
      "     neutral       0.00      0.00      0.00       461\n",
      "     sadness       0.00      0.00      0.00       461\n",
      "    surprise       0.17      0.20      0.18       461\n",
      "\n",
      "    accuracy                           0.18      3227\n",
      "   macro avg       0.08      0.18      0.10      3227\n",
      "weighted avg       0.08      0.18      0.10      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/100: 100%|| 101/101 [01:42<00:00,  1.02s/it]\n",
      "Validation epoch 1/100: 100%|| 26/26 [00:26<00:00,  1.01s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train loss 18.1729,  Valid loss 17.5321,  emotion_correct 16.9772%,  span_correct 44.2815%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.19      0.16      0.17       461\n",
      "        fear       0.17      0.50      0.26       461\n",
      "         joy       0.00      0.00      0.00       461\n",
      "     neutral       0.17      0.43      0.24       461\n",
      "     sadness       0.00      0.00      0.00       461\n",
      "    surprise       0.19      0.14      0.16       461\n",
      "\n",
      "    accuracy                           0.18      3227\n",
      "   macro avg       0.10      0.18      0.12      3227\n",
      "weighted avg       0.10      0.18      0.12      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2/100: 100%|| 101/101 [01:48<00:00,  1.08s/it]\n",
      "Validation epoch 2/100: 100%|| 26/26 [00:26<00:00,  1.02s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train loss 18.0306,  Valid loss 17.3891,  emotion_correct 16.8570%,  span_correct 44.1669%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.16      0.45      0.24       461\n",
      "     disgust       0.00      0.00      0.00       461\n",
      "        fear       0.26      0.13      0.17       461\n",
      "         joy       0.17      0.61      0.27       461\n",
      "     neutral       0.00      0.00      0.00       461\n",
      "     sadness       0.14      0.03      0.04       461\n",
      "    surprise       0.00      0.00      0.00       461\n",
      "\n",
      "    accuracy                           0.17      3227\n",
      "   macro avg       0.11      0.17      0.10      3227\n",
      "weighted avg       0.11      0.17      0.10      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3/100: 100%|| 101/101 [01:45<00:00,  1.05s/it]\n",
      "Validation epoch 3/100: 100%|| 26/26 [00:26<00:00,  1.03s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train loss 17.9139,  Valid loss 17.2615,  emotion_correct 18.8101%,  span_correct 44.9669%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.18      0.49      0.26       461\n",
      "        fear       0.30      0.24      0.26       461\n",
      "         joy       0.19      0.15      0.17       461\n",
      "     neutral       0.13      0.06      0.08       461\n",
      "     sadness       0.00      0.00      0.00       461\n",
      "    surprise       0.19      0.42      0.26       461\n",
      "\n",
      "    accuracy                           0.19      3227\n",
      "   macro avg       0.14      0.19      0.15      3227\n",
      "weighted avg       0.14      0.19      0.15      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4/100: 100%|| 101/101 [01:46<00:00,  1.06s/it]\n",
      "Validation epoch 4/100: 100%|| 26/26 [00:26<00:00,  1.03s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss 17.8782,  Valid loss 17.2286,  emotion_correct 17.4279%,  span_correct 44.4242%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.18      0.43      0.25       461\n",
      "        fear       0.36      0.04      0.08       461\n",
      "         joy       0.22      0.08      0.12       461\n",
      "     neutral       0.16      0.29      0.20       461\n",
      "     sadness       0.14      0.03      0.04       461\n",
      "    surprise       0.18      0.39      0.25       461\n",
      "\n",
      "    accuracy                           0.18      3227\n",
      "   macro avg       0.18      0.18      0.14      3227\n",
      "weighted avg       0.18      0.18      0.14      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5/100: 100%|| 101/101 [01:46<00:00,  1.05s/it]\n",
      "Validation epoch 5/100: 100%|| 26/26 [00:26<00:00,  1.03s/it]\n",
      "Training epoch 6/100: 100%|| 101/101 [01:51<00:00,  1.10s/it]\n",
      "Validation epoch 6/100: 100%|| 26/26 [00:27<00:00,  1.05s/it]\n",
      "Training epoch 7/100: 100%|| 101/101 [01:51<00:00,  1.10s/it]\n",
      "Validation epoch 7/100: 100%|| 26/26 [00:26<00:00,  1.03s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train loss 17.7581,  Valid loss 17.0088,  emotion_correct 17.3077%,  span_correct 40.2344%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.18      0.01      0.02       461\n",
      "     disgust       0.16      0.82      0.27       461\n",
      "        fear       0.29      0.08      0.12       461\n",
      "         joy       0.00      0.00      0.00       461\n",
      "     neutral       0.17      0.06      0.09       461\n",
      "     sadness       0.17      0.01      0.02       461\n",
      "    surprise       0.23      0.27      0.25       461\n",
      "\n",
      "    accuracy                           0.18      3227\n",
      "   macro avg       0.17      0.18      0.11      3227\n",
      "weighted avg       0.17      0.18      0.11      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8/100: 100%|| 101/101 [01:46<00:00,  1.05s/it]\n",
      "Validation epoch 8/100: 100%|| 26/26 [00:26<00:00,  1.03s/it]\n",
      "Training epoch 9/100: 100%|| 101/101 [01:49<00:00,  1.09s/it]\n",
      "Validation epoch 9/100: 100%|| 26/26 [00:26<00:00,  1.02s/it]\n",
      "Training epoch 10/100: 100%|| 101/101 [01:49<00:00,  1.08s/it]\n",
      "Validation epoch 10/100: 100%|| 26/26 [00:26<00:00,  1.02s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train loss 17.5720,  Valid loss 16.8833,  emotion_correct 18.1791%,  span_correct 37.9301%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.23      0.22      0.22       461\n",
      "        fear       0.39      0.14      0.21       461\n",
      "         joy       0.17      0.78      0.28       461\n",
      "     neutral       0.50      0.01      0.01       461\n",
      "     sadness       0.00      0.00      0.00       461\n",
      "    surprise       0.15      0.16      0.16       461\n",
      "\n",
      "    accuracy                           0.19      3227\n",
      "   macro avg       0.21      0.19      0.13      3227\n",
      "weighted avg       0.21      0.19      0.13      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 11/100: 100%|| 101/101 [01:48<00:00,  1.08s/it]\n",
      "Validation epoch 11/100: 100%|| 26/26 [00:26<00:00,  1.03s/it]\n",
      "Training epoch 12/100: 100%|| 101/101 [01:49<00:00,  1.08s/it]\n",
      "Validation epoch 12/100: 100%|| 26/26 [00:26<00:00,  1.01s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train loss 17.3566,  Valid loss 16.7567,  emotion_correct 18.0288%,  span_correct 37.6446%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.18      0.37      0.25       461\n",
      "        fear       0.20      0.38      0.26       461\n",
      "         joy       0.26      0.10      0.14       461\n",
      "     neutral       0.17      0.34      0.23       461\n",
      "     sadness       0.16      0.11      0.13       461\n",
      "    surprise       0.00      0.00      0.00       461\n",
      "\n",
      "    accuracy                           0.19      3227\n",
      "   macro avg       0.14      0.19      0.14      3227\n",
      "weighted avg       0.14      0.19      0.14      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 13/100: 100%|| 101/101 [01:49<00:00,  1.09s/it]\n",
      "Validation epoch 13/100: 100%|| 26/26 [00:27<00:00,  1.04s/it]\n",
      "Training epoch 14/100: 100%|| 101/101 [01:51<00:00,  1.11s/it]\n",
      "Validation epoch 14/100: 100%|| 26/26 [00:27<00:00,  1.04s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train loss 17.3836,  Valid loss 16.5583,  emotion_correct 17.8486%,  span_correct 36.5610%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.20      0.29      0.24       461\n",
      "        fear       0.24      0.18      0.20       461\n",
      "         joy       0.17      0.63      0.27       461\n",
      "     neutral       0.00      0.00      0.00       461\n",
      "     sadness       0.17      0.19      0.18       461\n",
      "    surprise       0.00      0.00      0.00       461\n",
      "\n",
      "    accuracy                           0.18      3227\n",
      "   macro avg       0.11      0.18      0.13      3227\n",
      "weighted avg       0.11      0.18      0.13      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 15/100: 100%|| 101/101 [01:49<00:00,  1.09s/it]\n",
      "Validation epoch 15/100: 100%|| 26/26 [00:26<00:00,  1.03s/it]\n",
      "Training epoch 16/100: 100%|| 101/101 [01:49<00:00,  1.08s/it]\n",
      "Validation epoch 16/100: 100%|| 26/26 [00:26<00:00,  1.02s/it]\n",
      "Training epoch 17/100: 100%|| 101/101 [01:49<00:00,  1.08s/it]\n",
      "Validation epoch 17/100: 100%|| 26/26 [00:26<00:00,  1.02s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train loss 17.0521,  Valid loss 16.4660,  emotion_correct 18.1791%,  span_correct 37.1113%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.21      0.26      0.23       461\n",
      "        fear       0.24      0.23      0.24       461\n",
      "         joy       0.17      0.73      0.27       461\n",
      "     neutral       0.25      0.00      0.00       461\n",
      "     sadness       0.19      0.08      0.11       461\n",
      "    surprise       0.00      0.00      0.00       461\n",
      "\n",
      "    accuracy                           0.19      3227\n",
      "   macro avg       0.15      0.19      0.12      3227\n",
      "weighted avg       0.15      0.19      0.12      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 18/100: 100%|| 101/101 [01:48<00:00,  1.07s/it]\n",
      "Validation epoch 18/100: 100%|| 26/26 [00:26<00:00,  1.02s/it]\n",
      "Training epoch 19/100: 100%|| 101/101 [01:48<00:00,  1.08s/it]\n",
      "Validation epoch 19/100: 100%|| 26/26 [00:26<00:00,  1.02s/it]\n",
      "Training epoch 20/100:   8%|         | 8/101 [00:09<01:52,  1.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memotion_casual_head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 114\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, model, optimizer, train_dataset, valid_dataset, epochs, option, batch_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 114\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     val_loss, emotion_correct, span_correct, emotion_label_list, emotion_pred_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_val_epoch(epoch, option)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[6], line 186\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, epoch, option)\u001b[0m\n\u001b[1;32m    183\u001b[0m     span_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_fn_loss(span_pred, span_label)\n\u001b[1;32m    184\u001b[0m     loss \u001b[38;5;241m=\u001b[39m emotion_loss \u001b[38;5;241m+\u001b[39m span_loss\n\u001b[0;32m--> 186\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_head\", batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_lstm\", batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_head\", batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_encoder\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_encoder_lstm\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_dataset = ConversationDataset(config_path, trial_data, option=\"trial\")\n",
    "predict = Prediction(config_path)\n",
    "predict.predict(model, raw_trial_data, trial_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
