{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install scikit-learn\n",
    "# !pip install focal-loss-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxility Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "# Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video, read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Model\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer, BertModel, BertTokenizer, BertGenerationDecoder, GPT2Config, GPT2Model, GPT2LMHeadModel\n",
    "\n",
    "# Training parameter\n",
    "from torch.optim import Adam\n",
    "# Training process\n",
    "from tqdm import tqdm\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import sys\n",
    "from focal_loss.focal_loss import FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "def write_json(path, data):\n",
    "    with open(path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "def mkdir(path):\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(path)\n",
    "        print(\"The new directory checkpoint is created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, config_path, data, option=\"train\"):\n",
    "        super(ConversationDataset, self).__init__()\n",
    "        self.build(config_path)\n",
    "        self.data = data\n",
    "        self.option = option\n",
    "    def build(self, config_path):\n",
    "        config = read_json(config_path)\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.id2label = config[\"id2label\"]\n",
    "        self.tokenizer_name = config[\"tokenizer_name\"]\n",
    "        self.padding = config[\"padding\"]\n",
    "        self.paragraph_max_length = config[\"paragraph_max_length\"]\n",
    "        self.text_max_length = config[\"text_max_length\"]\n",
    "        self.left_padding = config[\"left_padding\"]\n",
    "        self.right_padding = config[\"right_padding\"]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.text_pool_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        # self.text_pool_tokenizer.add_special_tokens({'pad_token': 'eos_token_id'})\n",
    "        self.text_pool_tokenizer.pad_token = self.text_pool_tokenizer.eos_token\n",
    "    def span(self, text, subtext, ss=0):\n",
    "        if len(subtext) == 0:\n",
    "            return []\n",
    "        for i in range(ss, len(text) - len(subtext)):\n",
    "            for j in range(len(subtext)):\n",
    "                if text[i+j] != subtext[j]:\n",
    "                    match = 0\n",
    "                    break\n",
    "                else:\n",
    "                    match += 1\n",
    "                    if match == len(subtext):\n",
    "                        return [i, i+len(subtext)]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        conversation_ID = self.data[index][\"conversation_ID\"]\n",
    "        paragraph = self.tokenizer(self.data[index][\"paragraph\"], padding=self.padding, max_length=self.paragraph_max_length, return_tensors=\"pt\")\n",
    "        paragraph = torch.squeeze(paragraph['input_ids'])\n",
    "        utterance_ID = self.data[index][\"utterance_ID\"]\n",
    "        text_pool = self.data[index][\"text_pool\"]\n",
    "        speaker_pool = self.data[index][\"speaker_pool\"]\n",
    "        token_text_pool = []\n",
    "        token_speaker_pool = []\n",
    "\n",
    "        for i in range(len(text_pool)):\n",
    "            text = self.tokenizer.encode(text_pool[i], padding=self.padding, max_length=self.text_max_length)\n",
    "            speaker = self.tokenizer(speaker_pool[i], padding=self.padding, max_length=8)[\"input_ids\"]\n",
    "            token_text_pool.append(text)\n",
    "            token_speaker_pool.append(speaker)\n",
    "\n",
    "        if len(text_pool) < self.left_padding+self.right_padding+1:\n",
    "            for i in range(len(text_pool), self.left_padding+self.right_padding+1):\n",
    "                text = self.tokenizer.encode(\"\", padding=self.padding, max_length=self.text_max_length)\n",
    "                speaker = self.tokenizer(\"\", padding=self.padding, max_length=8)[\"input_ids\"]\n",
    "                token_text_pool.insert(0, text)\n",
    "                token_speaker_pool.insert(0, speaker)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.option == \"train\":\n",
    "            label = self.label2id[self.data[index][\"emotion\"][\"property\"]]\n",
    "            emotion_label = [0] * len(self.id2label)\n",
    "            emotion_label[label] = 1\n",
    "            emotion_label = torch.FloatTensor(emotion_label)\n",
    "            casual_pool = self.data[index][\"emotion\"][\"casual\"]\n",
    "            token_casual = []\n",
    "\n",
    "\n",
    "            list = []\n",
    "         \n",
    "            for i in range(len(casual_pool)):\n",
    "                casual = [casual_pool[i][\"start\"], casual_pool[i][\"end\"]]\n",
    "                token_casual.append(casual)\n",
    "                token = self.tokenizer.encode(casual_pool[i][\"casual_text\"], add_special_tokens=False)\n",
    "                list.extend(self.span(paragraph.tolist(), token))\n",
    "            # print(\" token_casual: \"+str(len(token_casual)))\n",
    "\n",
    "            for i in range(len(casual_pool), self.left_padding+self.right_padding+1):\n",
    "                casual = [0, 0]\n",
    "                token_casual.insert(0, casual)\n",
    "            print(casual_pool)\n",
    "            print(list)\n",
    "            paragraph_label = [0] * self.paragraph_max_length\n",
    "            for i in list:\n",
    "                paragraph_label[i] = 1\n",
    "\n",
    "\n",
    "            return {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"paragraph\": paragraph,\n",
    "                \"paragraph_label\": torch.FloatTensor(paragraph_label),\n",
    "                \"utterance_ID\": utterance_ID,\n",
    "                \"token_text_pool\": torch.FloatTensor(token_text_pool),\n",
    "                \"token_speaker_pool\": torch.FloatTensor(token_speaker_pool),\n",
    "                \"emotion_label\": emotion_label,\n",
    "                \"span_label\": torch.FloatTensor(token_casual)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"paragraph\": paragraph,\n",
    "                \"utterance_ID\": utterance_ID,\n",
    "                \"token_text_pool\": torch.FloatTensor(token_text_pool),\n",
    "                \"token_speaker_pool\": torch.FloatTensor(token_speaker_pool),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1045, 2572, 26363]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.encode(\"I am Duc\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.layers_to_freeze = []\n",
    "        self.history = {\n",
    "            \"train loss\": [],\n",
    "            \"valid loss\": [],\n",
    "            \"emotion_correct\": [],\n",
    "            \"span_correct\": []\n",
    "        }\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        print(config_path)\n",
    "        config = read_json(config_path)\n",
    "        self.patience = config['patience']\n",
    "        self.min_delta = config['min_delta']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.device = config['device']\n",
    "        self.save_checkpoint_dir = config['output_dir'] + \"/checkpoint\"\n",
    "        self.save_history_dir = config['output_dir'] + \"/history\"\n",
    "        self.emotion_head_layers = config['emotion_head_layers']\n",
    "        self.casual_head_layers = config['casual_head_layers']\n",
    "        self.decoder_layers = config['decoder_layers']\n",
    "        self.encoder_layers = config['encoder_layers']\n",
    "        self.lstm_layers = config['lstm_layers']\n",
    "        self.alpha_focal_loss = config['alpha_focal_loss']\n",
    "        self.gamma_focal_loss = config['gamma_focal_loss']\n",
    "        self.label2id = config['label2id']\n",
    "        self.target_names = self.label2id.keys()\n",
    "        self.early_stopper = EarlyStopper(self.patience, self.min_delta)\n",
    "\n",
    "        mkdir(self.save_history_dir)\n",
    "\n",
    "    def emotion_head_training(self):\n",
    "        self.layers_to_freeze = self.casual_head_layers + self.encoder_layers + self.lstm_layers\n",
    "        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n",
    "    def emotion_encoder_training(self):\n",
    "        self.layers_to_freeze = self.casual_head_layers + self.lstm_layers\n",
    "        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n",
    "    def emotion_encoder_lstm_training(self):\n",
    "        self.layers_to_freeze = self.casual_head_layers + self.decoder_layers \n",
    "    def emotion_casual_head_training(self):\n",
    "        self.layers_to_freeze = self.encoder_layers + self.lstm_layers\n",
    "    def emotion_casual_lstm_head_training(self):\n",
    "        self.layers_to_freeze = self.encoder_layers\n",
    "        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n",
    "    def emotion_lstm_training(self):\n",
    "        self.layers_to_freeze = self.casual_head_layers + self.encoder_layers\n",
    "        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n",
    "        \n",
    "    def casual_head_training(self):\n",
    "        self.layers_to_freeze = self.emotion_head_layers + self.encoder_layers + self.lstm_layers\n",
    "\n",
    "    def full_training(self):\n",
    "        self.layers_to_freeze = []\n",
    "    def freeze(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(layer in name for layer in self.layers_to_freeze):\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        return model\n",
    "    def train(self, model, optimizer, train_dataset, valid_dataset, epochs, option=\"emotion_head\", batch_size=None):\n",
    "        self.optimizer = optimizer\n",
    "        if option == \"emotion_head\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_head_training()\n",
    "        elif option == \"emotion_encoder\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_encoder_training()\n",
    "        elif option == \"emotion_lstm\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_lstm_training()\n",
    "        elif option == \"emotion_encoder_lstm\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_encoder_lstm_training()\n",
    "        elif option == \"emotion_casual_head\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_casual_head_training()\n",
    "        elif option == \"emotion_casual_lstm_head\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_casual_lstm_head_training()\n",
    "        elif option == \"casual_head\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.casual_head_training()\n",
    "\n",
    "        if batch_size == None:\n",
    "            self.batch_size = self.batch_size_config\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        mkdir(self.save_checkpoint_dir + \"/\" + option)\n",
    "        self.fn_loss = nn.CrossEntropyLoss()\n",
    "        self.span_fn_loss = nn.MSELoss()\n",
    "\n",
    "        \n",
    "        print(\"device: \"+str(self.device))\n",
    "        print(\"batch_size: \"+str(self.batch_size))\n",
    "        print(\"gamma_focal_loss: \"+str(self.gamma_focal_loss))\n",
    "        print(\"alpha_focal_loss: \"+str(self.alpha_focal_loss))\n",
    "\n",
    "        self.early_stopper.reset()\n",
    "        self.model = self.freeze(model)\n",
    "        # if self.device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "        #     print(\"Training Parallel!\")\n",
    "        #     self.model = nn.DataParallel(self.model)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.train_dataloader = DataLoader(dataset=train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        self.valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        self.epochs = epochs\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self._train_epoch(epoch, option)\n",
    "            val_loss, emotion_correct, span_correct, emotion_label_list, emotion_pred_list = self._val_epoch(epoch, option)\n",
    "            \n",
    "            self.history[\"train loss\"].append(train_loss)\n",
    "            self.history[\"valid loss\"].append(val_loss)\n",
    "            self.history[\"emotion_correct\"].append(emotion_correct)\n",
    "            self.history[\"span_correct\"].append(span_correct)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.save_checkpoint(epoch, option)\n",
    "                print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f},  Valid loss {val_loss:.4f},  emotion_correct {emotion_correct*100:.4f}%,  span_correct {span_correct*100:.4f}%\")\n",
    "                print(classification_report(emotion_label_list, emotion_pred_list, target_names=self.target_names))\n",
    "            if self.early_stopper.early_stop(val_loss):\n",
    "                print(\"Early stop at epoch: \"+str(epoch) + \" with valid loss: \"+str(val_loss))\n",
    "                break\n",
    "\n",
    "        \n",
    "        write_json(self.save_history_dir + \"/\" + option + \"_history.json\", self.history) \n",
    "        \n",
    "    def _train_epoch(self, epoch, option):\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "        logger_message = f'Training epoch {epoch}/{self.epochs}'\n",
    "\n",
    "        progress_bar = tqdm(self.train_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        for batch, data in enumerate(progress_bar):\n",
    "            conversation_id = data[\"conversation_ID\"]\n",
    "            paragraph = data[\"paragraph\"].to(self.device)\n",
    "            utter_id = data[\"utterance_ID\"]\n",
    "            token_text_pool = data[\"token_text_pool\"].to(self.device)\n",
    "            token_speaker_pool = data[\"token_speaker_pool\"].to(self.device)\n",
    "            emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "            span_label = data[\"span_label\"].to(self.device)\n",
    "            # print(\"conversation_id:\"+str(conversation_id.shape))\n",
    "            # print(\"paragraph:\"+str(paragraph.shape))\n",
    "            # print(\"casual_pool:\"+str(casual_pool.shape))\n",
    "            # print(\"emotion_label:\"+str(emotion_label.shape))\n",
    "            # print(\"span_label:\"+str(span_label.shape))\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            emotion_pred, span_pred = self.model(paragraph, token_text_pool, token_speaker_pool)\n",
    "            # print(\"emotion_label : \"+str(emotion_label.shape))\n",
    "            # print(\"span_label : \"+str(span_label.shape))\n",
    "            # print(\"emotion_pred : \"+str(emotion_pred.shape))\n",
    "            # print(\"span_pred : \"+str(span_pred.shape))\n",
    "            \n",
    "            emotion_label = torch.argmax(emotion_label, dim=-1)\n",
    "            if option == \"emotion_head\":\n",
    "                loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "            elif option == \"emotion_lstm\":\n",
    "                loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "            elif option == \"emotion_encoder\":\n",
    "                loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "            elif option == \"emotion_encoder_lstm\":\n",
    "                loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "            elif option == \"emotion_casual_head\":\n",
    "                emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                loss = emotion_loss + span_loss\n",
    "            elif option == \"emotion_casual_lstm_head\":\n",
    "                emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                loss = emotion_loss + span_loss\n",
    "            elif option == \"casual_head\":\n",
    "                loss = self.fn_loss(span_pred, span_label)\n",
    "            else:\n",
    "                emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                loss = emotion_loss + span_loss\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return train_loss / len(self.train_dataloader)\n",
    "\n",
    "    def _val_epoch(self, epoch, option):\n",
    "        self.model.eval()\n",
    "        valid_loss = 0.0\n",
    "        emotion_correct=0\n",
    "        span_correct=0\n",
    "        emotion_pred_list = []\n",
    "        emotion_label_list = []\n",
    "        logger_message = f'Validation epoch {epoch}/{self.epochs}'\n",
    "        progress_bar = tqdm(self.valid_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(progress_bar):\n",
    "                conversation_id = data[\"conversation_ID\"]\n",
    "                paragraph = data[\"paragraph\"].to(self.device)\n",
    "                utter_id = data[\"utterance_ID\"]\n",
    "                token_text_pool = data[\"token_text_pool\"].to(self.device)\n",
    "                token_speaker_pool = data[\"token_speaker_pool\"].to(self.device)\n",
    "                emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "                span_label = data[\"span_label\"].to(self.device)\n",
    "                emotion_pred, span_pred = self.model(paragraph, token_text_pool, token_speaker_pool)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                emotion_label = torch.argmax(emotion_label, dim=-1)\n",
    "                if option == \"emotion_head\":\n",
    "                    loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                elif option == \"emotion_lstm\":\n",
    "                    loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                elif option == \"emotion_encoder\":\n",
    "                    loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                elif option == \"emotion_encoder_lstm\":\n",
    "                    loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                elif option == \"emotion_casual_head\":\n",
    "                    emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                    span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                    loss = emotion_loss + span_loss\n",
    "                elif option == \"emotion_casual_lstm_head\":\n",
    "                    emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                    span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                    loss = emotion_loss + span_loss\n",
    "                elif option == \"casual_head\":\n",
    "                    loss = self.fn_loss(span_pred, span_label)\n",
    "                else:\n",
    "                    emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                    span_loss = self.span_fn_loss(span_pred, span_label)\n",
    "                    loss = emotion_loss + span_loss\n",
    "\n",
    "                # emotion_label_ = torch.argmax(emotion_label, dim=-1)\n",
    "                emotion_pred_ = torch.argmax(emotion_pred, dim=-1)\n",
    "                emotion_pred_list.extend(emotion_pred_.cpu().tolist())\n",
    "                emotion_label_list.extend(emotion_label.cpu().tolist())\n",
    "                # print(\"emotion_label_: \"+ str(emotion_label_))\n",
    "                # print(\"emotion_pred_: \"+ str(emotion_pred_))\n",
    "                valid_loss += loss.item()\n",
    "                emotion_correct += (emotion_pred_ == emotion_label).sum().item()\n",
    "                span_correct += (span_pred == span_label).sum().item()\n",
    "        \n",
    "        return valid_loss / len(self.valid_dataloader), emotion_correct / len(self.valid_dataloader) / self.batch_size, span_correct / len(self.valid_dataloader) / self.batch_size / 16, emotion_label_list, emotion_pred_list\n",
    "        \n",
    "    def save_checkpoint(self, epoch, option):\n",
    "        checkpoint_path = os.path.join(self.save_checkpoint_dir, option + '/checkpoint_{}.pth'.format(epoch))\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(Prediction, self).__init__()\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.id2label = config[\"id2label\"]\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.submit_path = config[\"submit path\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "    def predict(self, model, raw_dataset, dataset):\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "        pred_list = []\n",
    "        \n",
    "        self.test_dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        logger_message = f'Predict '\n",
    "        progress_bar = tqdm(self.test_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(progress_bar):\n",
    "                conversation_id = data[\"conversation_ID\"]\n",
    "                paragraph = data[\"paragraph\"].to(self.device)\n",
    "                utter_id = data[\"utterance_ID\"]\n",
    "                token_text_pool = data[\"token_text_pool\"].to(self.device)\n",
    "                token_speaker_pool = data[\"token_speaker_pool\"].to(self.device)\n",
    "                emotion_pred, span_pred = self.model(paragraph, token_text_pool, token_speaker_pool)\n",
    "                \n",
    "                pred = self.convertpred2list(conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool)\n",
    "                pred_list.extend(pred)\n",
    "        submit = self.convertdict2submit(self.convertpred2dict(raw_dataset, pred_list))\n",
    "        self.save_json(self.submit_path, submit)\n",
    "    def convertpred2list(self, conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool):\n",
    "        pred_list = []\n",
    "        for i in range(len(emotion_pred)):\n",
    "            emotion = self.id2label[str(torch.argmax(emotion_pred[i]).item())]\n",
    "            emotion_string = str(utter_id[i].item()) + \"_\" + emotion\n",
    "            if emotion != \"neutral\":\n",
    "                # \n",
    "                span = torch.round(span_pred[i])\n",
    "                # print(\"span: \"+str(span))\n",
    "                # print(\"casual_span_pool: \"+str(casual_span_pool[i]))\n",
    "                for j in range(len(span)):\n",
    "                    if span[j] == 1:\n",
    "                        if casual_span_pool[i][j][0].item() == 0:\n",
    "                            break\n",
    "                        if casual_span_pool[i][j][1].item() == casual_span_pool[i][j][2].item():\n",
    "                            break\n",
    "                        span_string = str(int(casual_span_pool[i][j][0].item())) + \"_\" + str(int(casual_span_pool[i][j][1].item())) + \"_\" + str(int(casual_span_pool[i][j][2].item()))\n",
    "                        pred_dict = {}\n",
    "                        pred_dict[\"conversation_ID\"] = conversation_id[i].item()\n",
    "                        pred_dict[\"emotion-cause_pairs\"] = [emotion_string, span_string]\n",
    "                        # print(\"pred_dict: \"+str(pred_dict))\n",
    "                        pred_list.append(pred_dict)\n",
    "        return pred_list\n",
    "    def convertpred2dict(self, raw_dataset, pred_list):\n",
    "        dataset = self.convertlist2dict(raw_dataset)\n",
    "        # dict = \n",
    "        for i in range(len(pred_list)):\n",
    "            pred = pred_list[i]\n",
    "            conversation_id = pred[\"conversation_ID\"]\n",
    "            # print(\"conversation_id: \"+str(conversation_id))\n",
    "            emotion_cause_pairs = pred[\"emotion-cause_pairs\"]\n",
    "            # print(\"emotion_cause_pairs: \"+str(emotion_cause_pairs))\n",
    "            if \"emotion-cause_pairs\" in dataset[conversation_id]:\n",
    "                dataset[conversation_id][\"emotion-cause_pairs\"].append(emotion_cause_pairs)\n",
    "            else:\n",
    "                dataset[conversation_id][\"emotion-cause_pairs\"] = []\n",
    "        # print(dataset)\n",
    "        return dataset\n",
    "    def convertlist2dict(self, raw_dataset):\n",
    "        dataset = {}\n",
    "        for i in range(len(raw_dataset)):\n",
    "            conversation = raw_dataset[i]\n",
    "            conversation_ID = conversation[\"conversation_ID\"]\n",
    "            # if conversation_ID not in dataset.key\n",
    "            dataset[conversation_ID] = {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"conversation\": conversation['conversation']\n",
    "            }\n",
    "        return dataset\n",
    "    def convertdict2submit(self, raw_dataset):\n",
    "        dataset = []\n",
    "        for key in raw_dataset.keys():\n",
    "            # print(\"key: \"+str(key))\n",
    "            sample = raw_dataset[key]\n",
    "            # print(\"sample: \"+str(sample))\n",
    "            dataset.append(sample)\n",
    "        return dataset\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    def save_json(self, file_path, data):\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGG(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(TGG, self).__init__()\n",
    "        self.build(config_path)\n",
    "        self.lstm = nn.LSTM(self.text_max_length, self.hidden_size, self.num_layers, batch_first=True, dropout=0.2, bidirectional=True)\n",
    "        self.lstma_projection = nn.Linear(2*self.num_layers*self.hidden_size, self.embedding_dim)\n",
    "        self.cat_layer = nn.Linear(2*self.embedding_dim, self.embedding_dim)\n",
    "        self.emotion_fc = nn.Linear(self.embedding_dim, self.label_num)\n",
    "        self.casual_feedforward = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim , self.feedforward_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.feedforward_dim , 2 * self.num_text),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        # self.casual_fc = nn.Linear(self.embedding_dim, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "        self.feedforward_dim = config[\"feedforward_dim\"]\n",
    "        self.encoder_name = config[\"encoder_name\"]\n",
    "        self.decoder_name = config[\"decoder_name\"]\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.label_num = len(self.label2id)\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.paragraph_max_length = config[\"paragraph_max_length\"]\n",
    "        self.text_max_length = config[\"text_max_length\"]\n",
    "        self.left_padding = config[\"left_padding\"]\n",
    "        self.right_padding = config[\"right_padding\"]\n",
    "        self.num_text = 1 + self.left_padding + self.right_padding\n",
    "        self.device = config[\"device\"]\n",
    "        \n",
    "        main_part = AutoModel.from_pretrained(self.encoder_name)\n",
    "        self.embeddings = main_part.embeddings\n",
    "        self.encoder = main_part.encoder\n",
    "        self.pooler = main_part.pooler\n",
    "\n",
    "        main_part_2 = GPT2LMHeadModel.from_pretrained(self.decoder_name, output_hidden_states =True)\n",
    "        \n",
    "    def forward(self, paragraph, token_text_pool, token_speaker_pool):\n",
    "        batch_size = paragraph.size(0)\n",
    "        candidate_size = token_text_pool.size(1)\n",
    "        t = token_text_pool.size(2)\n",
    "        device = paragraph.get_device()\n",
    "\n",
    "        h0 = torch.randn((2 * self.num_layers, batch_size, self.hidden_size), device=device)\n",
    "        c0 = torch.randn((2 * self.num_layers, batch_size, self.hidden_size), device=device)\n",
    "        output, (hn, cn) = self.lstm(token_text_pool, (h0, c0))\n",
    "        hn = hn.permute(1,0,2)\n",
    "        hn = self.lstma_projection(hn.reshape(batch_size, -1))\n",
    "        # print(\"paragraph:\"+str(paragraph.shape))\n",
    "        \n",
    "        _paragraph = self.embeddings(paragraph)\n",
    "        _paragraph = self.encoder(_paragraph)['last_hidden_state']\n",
    "        _paragraph = self.pooler(_paragraph)\n",
    "        # print(\"_paragraph:\"+str(_paragraph.shape))\n",
    "\n",
    "        hidden_state = torch.cat([hn, _paragraph], dim=-1)\n",
    "\n",
    "        _x = self.cat_layer(hidden_state)\n",
    "        # print(\"_x:\"+str(_x.shape))\n",
    "        _e_category = self.emotion_fc(_x)\n",
    "        # print(\"_e_category:\"+str(_e_category.shape))\n",
    "\n",
    "        _x = self.casual_feedforward(_x)\n",
    "        # print(\"_x:\"+str(_x.shape))\n",
    "        _x = _x.reshape(batch_size, self.num_text, 2)\n",
    "        # print(\"_x:\"+str(_x.shape))\n",
    "\n",
    "        return self.softmax(_e_category), _x\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config.json\"\n",
    "train_data = read_json(\"../data/ECF 2.0/train/augment_train.json\")\n",
    "valid_data = read_json(\"../data/ECF 2.0/train/augment_valid.json\")\n",
    "# trial_data = read_json(\"../data/ECF 2.0/trial/trial.json\")\n",
    "# raw_trial_data = read_json(\"../data/ECF 2.0/trial/Subtask_1_trial.json\")\n",
    "\n",
    "train_dataset = ConversationDataset(config_path, train_data)\n",
    "valid_dataset = ConversationDataset(config_path, valid_data)\n",
    "model = TGG(config_path)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(\"\\\"\" + name + \"\\\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING emotion_casual_head MODEL\n",
      "device: cuda\n",
      "batch_size: 128\n",
      "gamma_focal_loss: 2\n",
      "alpha_focal_loss: [1, 5, 5, 1, 1, 3, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0/100: 100%|██████████| 101/101 [01:28<00:00,  1.14it/s]\n",
      "Validation epoch 0/100: 100%|██████████| 26/26 [00:24<00:00,  1.04it/s]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss 18.7939,  Valid loss 17.6089,  emotion_correct 17.1575%,  span_correct 45.4515%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.00      0.00      0.00       461\n",
      "        fear       0.19      0.49      0.28       461\n",
      "         joy       0.17      0.55      0.26       461\n",
      "     neutral       0.00      0.00      0.00       461\n",
      "     sadness       0.00      0.00      0.00       461\n",
      "    surprise       0.17      0.20      0.18       461\n",
      "\n",
      "    accuracy                           0.18      3227\n",
      "   macro avg       0.08      0.18      0.10      3227\n",
      "weighted avg       0.08      0.18      0.10      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/100: 100%|██████████| 101/101 [01:42<00:00,  1.02s/it]\n",
      "Validation epoch 1/100: 100%|██████████| 26/26 [00:26<00:00,  1.01s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train loss 18.1729,  Valid loss 17.5321,  emotion_correct 16.9772%,  span_correct 44.2815%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.19      0.16      0.17       461\n",
      "        fear       0.17      0.50      0.26       461\n",
      "         joy       0.00      0.00      0.00       461\n",
      "     neutral       0.17      0.43      0.24       461\n",
      "     sadness       0.00      0.00      0.00       461\n",
      "    surprise       0.19      0.14      0.16       461\n",
      "\n",
      "    accuracy                           0.18      3227\n",
      "   macro avg       0.10      0.18      0.12      3227\n",
      "weighted avg       0.10      0.18      0.12      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2/100: 100%|██████████| 101/101 [01:48<00:00,  1.08s/it]\n",
      "Validation epoch 2/100: 100%|██████████| 26/26 [00:26<00:00,  1.02s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train loss 18.0306,  Valid loss 17.3891,  emotion_correct 16.8570%,  span_correct 44.1669%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.16      0.45      0.24       461\n",
      "     disgust       0.00      0.00      0.00       461\n",
      "        fear       0.26      0.13      0.17       461\n",
      "         joy       0.17      0.61      0.27       461\n",
      "     neutral       0.00      0.00      0.00       461\n",
      "     sadness       0.14      0.03      0.04       461\n",
      "    surprise       0.00      0.00      0.00       461\n",
      "\n",
      "    accuracy                           0.17      3227\n",
      "   macro avg       0.11      0.17      0.10      3227\n",
      "weighted avg       0.11      0.17      0.10      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3/100: 100%|██████████| 101/101 [01:45<00:00,  1.05s/it]\n",
      "Validation epoch 3/100: 100%|██████████| 26/26 [00:26<00:00,  1.03s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train loss 17.9139,  Valid loss 17.2615,  emotion_correct 18.8101%,  span_correct 44.9669%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.18      0.49      0.26       461\n",
      "        fear       0.30      0.24      0.26       461\n",
      "         joy       0.19      0.15      0.17       461\n",
      "     neutral       0.13      0.06      0.08       461\n",
      "     sadness       0.00      0.00      0.00       461\n",
      "    surprise       0.19      0.42      0.26       461\n",
      "\n",
      "    accuracy                           0.19      3227\n",
      "   macro avg       0.14      0.19      0.15      3227\n",
      "weighted avg       0.14      0.19      0.15      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4/100: 100%|██████████| 101/101 [01:46<00:00,  1.06s/it]\n",
      "Validation epoch 4/100: 100%|██████████| 26/26 [00:26<00:00,  1.03s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss 17.8782,  Valid loss 17.2286,  emotion_correct 17.4279%,  span_correct 44.4242%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.18      0.43      0.25       461\n",
      "        fear       0.36      0.04      0.08       461\n",
      "         joy       0.22      0.08      0.12       461\n",
      "     neutral       0.16      0.29      0.20       461\n",
      "     sadness       0.14      0.03      0.04       461\n",
      "    surprise       0.18      0.39      0.25       461\n",
      "\n",
      "    accuracy                           0.18      3227\n",
      "   macro avg       0.18      0.18      0.14      3227\n",
      "weighted avg       0.18      0.18      0.14      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5/100: 100%|██████████| 101/101 [01:46<00:00,  1.05s/it]\n",
      "Validation epoch 5/100: 100%|██████████| 26/26 [00:26<00:00,  1.03s/it]\n",
      "Training epoch 6/100: 100%|██████████| 101/101 [01:51<00:00,  1.10s/it]\n",
      "Validation epoch 6/100: 100%|██████████| 26/26 [00:27<00:00,  1.05s/it]\n",
      "Training epoch 7/100: 100%|██████████| 101/101 [01:51<00:00,  1.10s/it]\n",
      "Validation epoch 7/100: 100%|██████████| 26/26 [00:26<00:00,  1.03s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train loss 17.7581,  Valid loss 17.0088,  emotion_correct 17.3077%,  span_correct 40.2344%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.18      0.01      0.02       461\n",
      "     disgust       0.16      0.82      0.27       461\n",
      "        fear       0.29      0.08      0.12       461\n",
      "         joy       0.00      0.00      0.00       461\n",
      "     neutral       0.17      0.06      0.09       461\n",
      "     sadness       0.17      0.01      0.02       461\n",
      "    surprise       0.23      0.27      0.25       461\n",
      "\n",
      "    accuracy                           0.18      3227\n",
      "   macro avg       0.17      0.18      0.11      3227\n",
      "weighted avg       0.17      0.18      0.11      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8/100: 100%|██████████| 101/101 [01:46<00:00,  1.05s/it]\n",
      "Validation epoch 8/100: 100%|██████████| 26/26 [00:26<00:00,  1.03s/it]\n",
      "Training epoch 9/100: 100%|██████████| 101/101 [01:49<00:00,  1.09s/it]\n",
      "Validation epoch 9/100: 100%|██████████| 26/26 [00:26<00:00,  1.02s/it]\n",
      "Training epoch 10/100: 100%|██████████| 101/101 [01:49<00:00,  1.08s/it]\n",
      "Validation epoch 10/100: 100%|██████████| 26/26 [00:26<00:00,  1.02s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train loss 17.5720,  Valid loss 16.8833,  emotion_correct 18.1791%,  span_correct 37.9301%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.23      0.22      0.22       461\n",
      "        fear       0.39      0.14      0.21       461\n",
      "         joy       0.17      0.78      0.28       461\n",
      "     neutral       0.50      0.01      0.01       461\n",
      "     sadness       0.00      0.00      0.00       461\n",
      "    surprise       0.15      0.16      0.16       461\n",
      "\n",
      "    accuracy                           0.19      3227\n",
      "   macro avg       0.21      0.19      0.13      3227\n",
      "weighted avg       0.21      0.19      0.13      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 11/100: 100%|██████████| 101/101 [01:48<00:00,  1.08s/it]\n",
      "Validation epoch 11/100: 100%|██████████| 26/26 [00:26<00:00,  1.03s/it]\n",
      "Training epoch 12/100: 100%|██████████| 101/101 [01:49<00:00,  1.08s/it]\n",
      "Validation epoch 12/100: 100%|██████████| 26/26 [00:26<00:00,  1.01s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train loss 17.3566,  Valid loss 16.7567,  emotion_correct 18.0288%,  span_correct 37.6446%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.18      0.37      0.25       461\n",
      "        fear       0.20      0.38      0.26       461\n",
      "         joy       0.26      0.10      0.14       461\n",
      "     neutral       0.17      0.34      0.23       461\n",
      "     sadness       0.16      0.11      0.13       461\n",
      "    surprise       0.00      0.00      0.00       461\n",
      "\n",
      "    accuracy                           0.19      3227\n",
      "   macro avg       0.14      0.19      0.14      3227\n",
      "weighted avg       0.14      0.19      0.14      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 13/100: 100%|██████████| 101/101 [01:49<00:00,  1.09s/it]\n",
      "Validation epoch 13/100: 100%|██████████| 26/26 [00:27<00:00,  1.04s/it]\n",
      "Training epoch 14/100: 100%|██████████| 101/101 [01:51<00:00,  1.11s/it]\n",
      "Validation epoch 14/100: 100%|██████████| 26/26 [00:27<00:00,  1.04s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train loss 17.3836,  Valid loss 16.5583,  emotion_correct 17.8486%,  span_correct 36.5610%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.20      0.29      0.24       461\n",
      "        fear       0.24      0.18      0.20       461\n",
      "         joy       0.17      0.63      0.27       461\n",
      "     neutral       0.00      0.00      0.00       461\n",
      "     sadness       0.17      0.19      0.18       461\n",
      "    surprise       0.00      0.00      0.00       461\n",
      "\n",
      "    accuracy                           0.18      3227\n",
      "   macro avg       0.11      0.18      0.13      3227\n",
      "weighted avg       0.11      0.18      0.13      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 15/100: 100%|██████████| 101/101 [01:49<00:00,  1.09s/it]\n",
      "Validation epoch 15/100: 100%|██████████| 26/26 [00:26<00:00,  1.03s/it]\n",
      "Training epoch 16/100: 100%|██████████| 101/101 [01:49<00:00,  1.08s/it]\n",
      "Validation epoch 16/100: 100%|██████████| 26/26 [00:26<00:00,  1.02s/it]\n",
      "Training epoch 17/100: 100%|██████████| 101/101 [01:49<00:00,  1.08s/it]\n",
      "Validation epoch 17/100: 100%|██████████| 26/26 [00:26<00:00,  1.02s/it]\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train loss 17.0521,  Valid loss 16.4660,  emotion_correct 18.1791%,  span_correct 37.1113%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       461\n",
      "     disgust       0.21      0.26      0.23       461\n",
      "        fear       0.24      0.23      0.24       461\n",
      "         joy       0.17      0.73      0.27       461\n",
      "     neutral       0.25      0.00      0.00       461\n",
      "     sadness       0.19      0.08      0.11       461\n",
      "    surprise       0.00      0.00      0.00       461\n",
      "\n",
      "    accuracy                           0.19      3227\n",
      "   macro avg       0.15      0.19      0.12      3227\n",
      "weighted avg       0.15      0.19      0.12      3227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 18/100: 100%|██████████| 101/101 [01:48<00:00,  1.07s/it]\n",
      "Validation epoch 18/100: 100%|██████████| 26/26 [00:26<00:00,  1.02s/it]\n",
      "Training epoch 19/100: 100%|██████████| 101/101 [01:48<00:00,  1.08s/it]\n",
      "Validation epoch 19/100: 100%|██████████| 26/26 [00:26<00:00,  1.02s/it]\n",
      "Training epoch 20/100:   8%|▊         | 8/101 [00:09<01:52,  1.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memotion_casual_head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 114\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, model, optimizer, train_dataset, valid_dataset, epochs, option, batch_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 114\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     val_loss, emotion_correct, span_correct, emotion_label_list, emotion_pred_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_val_epoch(epoch, option)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[6], line 186\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, epoch, option)\u001b[0m\n\u001b[1;32m    183\u001b[0m     span_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_fn_loss(span_pred, span_label)\n\u001b[1;32m    184\u001b[0m     loss \u001b[38;5;241m=\u001b[39m emotion_loss \u001b[38;5;241m+\u001b[39m span_loss\n\u001b[0;32m--> 186\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_casual_head\", batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_lstm\", batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_head\", batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_encoder\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_encoder_lstm\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_dataset = ConversationDataset(config_path, trial_data, option=\"trial\")\n",
    "predict = Prediction(config_path)\n",
    "predict.predict(model, raw_trial_data, trial_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
