{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LIBRARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s24gb1/anaconda3/envs/ga_aug_std/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "# Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video, read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Model\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer, BertModel, BertTokenizer, BertGenerationDecoder\n",
    "\n",
    "# Training parameter\n",
    "from torch.optim import Adam\n",
    "# Training process\n",
    "from tqdm import tqdm\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxility class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, config_path, data, option=\"train\"):\n",
    "        super(ConversationDataset, self).__init__()\n",
    "        self.build(config_path)\n",
    "        self.data = data\n",
    "        self.option = option\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.id2label = config[\"id2label\"]\n",
    "        self.tokenizer_name = config[\"tokenizer_name\"]\n",
    "        self.padding = config[\"padding\"]\n",
    "        self.max_length = config[\"max_length\"]\n",
    "        self.candidate_num = config[\"candidate num\"]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        conversation_ID = self.data[index][\"conversation_ID\"]\n",
    "        paragraph = self.tokenizer(self.data[index][\"paragraph\"], padding=self.padding, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        paragraph = torch.squeeze(paragraph['input_ids'])\n",
    "        utterance_ID = self.data[index][\"utterance_ID\"]\n",
    "\n",
    "        casual_pool = []\n",
    "        for i in range(len(self.data[index][\"casual_pool\"])):\n",
    "            token = self.tokenizer(self.data[index][\"casual_pool\"][i], padding=self.padding, max_length=self.max_length)\n",
    "            casual_pool.append(token['input_ids'])\n",
    "        \n",
    "        if self.option == \"train\":\n",
    "            span_label = self.data[index][\"span_label\"].copy()\n",
    "            emotion_label = self.label2id[self.data[index][\"emotion\"]]\n",
    "\n",
    "        casual_span_pool = self.data[index][\"casual_span_pool\"].copy()\n",
    "        for i in range(self.candidate_num-len(casual_pool)):\n",
    "            token = self.tokenizer(\"\", padding=self.padding, max_length=self.max_length)\n",
    "            casual_pool.append(token['input_ids'])\n",
    "            casual_span_pool.append([0,0,0])\n",
    "            if self.option == \"train\":\n",
    "                span_label.append(0)\n",
    "        casual_pool = torch.LongTensor(casual_pool)  \n",
    "        casual_span_pool = torch.FloatTensor(casual_span_pool)\n",
    "\n",
    "\n",
    "        if self.option == \"train\":\n",
    "            span_label = torch.FloatTensor(span_label)\n",
    "            return {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"paragraph\": paragraph,\n",
    "                \"utterance_ID\": utterance_ID,\n",
    "                \"casual_pool\": casual_pool,\n",
    "                \"casual_span_pool\": casual_span_pool,\n",
    "                \"emotion_label\": emotion_label,\n",
    "                \"span_label\": span_label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"paragraph\": paragraph,\n",
    "                \"utterance_ID\": utterance_ID,\n",
    "                \"casual_pool\": casual_pool,\n",
    "                \"casual_span_pool\": casual_span_pool\n",
    "            }\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(Evaluation, self).__init__()\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.id2label = config[\"id2label\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.candidate_num = config[\"candidate_num\"]\n",
    "    def forward(self, conversation_id, utter_id, emotion_pred, span_pred, emotion_label, span_label):\n",
    "        score_emo = []\n",
    "        score_list = []\n",
    "        score_list_1 = []\n",
    "        emotion_pred = torch.argmax(emotion_pred, dim=1)\n",
    "        conversation_id = conversation_id.unsqueeze(1)\n",
    "        conversation_id = conversation_id.expand(self.batch_size, self.candidate_num)\n",
    "        utter_id = utter_id.unsqueeze(1)\n",
    "        utter_id = utter_id.expand(self.batch_size, self.candidate_num)\n",
    "        emotion_pred = emotion_pred.unsqueeze(1)\n",
    "        emotion_pred = emotion_pred.expand(self.batch_size, self.candidate_num)\n",
    "        for i in range(len(conversation_id)):\n",
    "            conv_id, emo_utt_id, cau_utt_id, span_start_id, span_end_id, emotion_category = conversation_id[i], utter_id[i], span_pred[i][0], span_pred[i][1], span_pred[i][2], self.id2label(emotion_pred)\n",
    "            conv_id_label, emo_utt_id_label, cau_utt_id_label, span_start_id_label, span_end_id_label, emotion_category_label = conversation_id[i], utter_id[i], span_label[i][0], span_label[i][1], span_label[i][2], self.id2label(emotion_label)\n",
    "            \n",
    "            span_pair_dict = [conv_id_label, emo_utt_id_label, cau_utt_id_label, span_start_id_label, span_end_id_label, emotion_category_label]\n",
    "            pred_pairs = [conv_id, emo_utt_id, cau_utt_id, span_start_id, span_end_id, emotion_category]\n",
    "            \n",
    "            emotion = self.cal_emotion(span_pair_dict, pred_pairs)\n",
    "            emocate = self.cal_prf_span_pair_emocate(span_pair_dict, pred_pairs)\n",
    "            emocate_proportional = self.cal_prf_span_pair_emocate_proportional(span_pair_dict, pred_pairs)\n",
    "            score_emo.append(emotion)\n",
    "            score_list.append(emocate)\n",
    "            score_list_1.append(emocate_proportional)\n",
    "        \n",
    "        score_emo = np.array(score_emo)\n",
    "        score_list = np.array(score_list)\n",
    "        score_list_1 = np.array(score_list_1)\n",
    "        return {\n",
    "            \"emotion_acc\": score_emo.mean(),\n",
    "            \"weighted_strict_precision\": score_list[:, 0].mean(),\n",
    "            \"weighted_strict_recall\": score_list[:, 1].mean(),\n",
    "            \"weighted_strict_f1\": score_list[:, 2].mean(),\n",
    "            \"strict_precision\": score_list[:, 3].mean(),\n",
    "            \"strict_recall\": score_list[:, 4].mean(),\n",
    "            \"strict_f1\": score_list[:, 5].mean(),\n",
    "            \"weighted_Proportional_precision\": score_list_1[:, 0].mean(),\n",
    "            \"weighted_Proportional_recall\": score_list_1[:, 1].mean(),\n",
    "            \"weighted_Proportional_f1\": score_list_1[:, 2].mean(),\n",
    "            \"Proportional_precision\": score_list_1[:, 3].mean(),\n",
    "            \"Proportional_recall\": score_list_1[:, 4].mean(),\n",
    "            \"Proportional_f1\": score_list_1[:, 5].mean()\n",
    "        }\n",
    "\n",
    "            \n",
    "    '''\n",
    "    Strict Match: emotion_utt and cause_utt are the same, and the cause spans completely match.\n",
    "    Fuzzy Match: emotion_utt and cause_utt are the same, and the cause spans overlap\n",
    "    '''\n",
    "    def judge_cause_span_pair_emocate(self, pred_span_pair, true_spans_pos_dict, span_mode='fuzzy'): # strict/fuzzy\n",
    "        d_id, emo_id, cau_id, start_cur, end_cur, emo = pred_span_pair\n",
    "        cur_key = 'dia{}_emoutt{}_causeutt{}'.format(d_id, emo_id, cau_id)\n",
    "        if cur_key in true_spans_pos_dict:\n",
    "            if span_mode == 'strict':\n",
    "                if [start_cur, end_cur, emo] in true_spans_pos_dict[cur_key]:\n",
    "                    true_spans_pos_dict[cur_key].remove([start_cur, end_cur, emo])\n",
    "                    return True\n",
    "            else:\n",
    "                for t_start, t_end, emo_y in true_spans_pos_dict[cur_key]:\n",
    "                    if emo == emo_y and not(end_cur<=t_start or start_cur>=t_end):\n",
    "                        true_spans_pos_dict[cur_key].remove([t_start, t_end, emo_y]) \n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def cal_prf_span_pair_emocate(self, span_pair_dict, pred_pairs, span_mode='strict'): \n",
    "        conf_mat = np.zeros([7,7])\n",
    "        for p in pred_pairs: # [conv_id, emo_utt_id, cau_utt_id, span_start_id, span_end_id, emotion_category]\n",
    "            if self.judge_cause_span_pair_emocate(p, span_pair_dict, span_mode=span_mode):\n",
    "                conf_mat[p[5]][p[5]] += 1\n",
    "            else:\n",
    "                conf_mat[0][p[5]] += 1\n",
    "        for k, v in span_pair_dict.items():\n",
    "            for p in v:\n",
    "                conf_mat[p[2]][0] += 1\n",
    "        p = np.diagonal(conf_mat / np.reshape(np.sum(conf_mat, axis = 0)+(1e-8), [1,7]))\n",
    "        r = np.diagonal(conf_mat / np.reshape(np.sum(conf_mat, axis = 1)+(1e-8), [7,1]))\n",
    "        f = 2*p*r/(p+r+(1e-8))\n",
    "        weight0 = np.sum(conf_mat, axis = 1)\n",
    "        weight = weight0[1:] / np.sum(weight0[1:])\n",
    "        w_avg_p = np.sum(p[1:] * weight)\n",
    "        w_avg_r = np.sum(r[1:] * weight)\n",
    "        w_avg_f1 = np.sum(f[1:] * weight)\n",
    "        \n",
    "        micro_acc = np.sum(np.diagonal(conf_mat)[1:])\n",
    "        micro_p = micro_acc / (sum(np.sum(conf_mat, axis = 0)[1:])+(1e-8))\n",
    "        micro_r = micro_acc / (sum(np.sum(conf_mat, axis = 1)[1:])+(1e-8))\n",
    "        micro_f1 = 2*micro_p*micro_r/(micro_p+micro_r+1e-8)\n",
    "        \n",
    "        return [w_avg_p, w_avg_r, w_avg_f1, micro_p, micro_r, micro_f1]\n",
    "\n",
    "\n",
    "    def get_match_scores(self, pred_span, true_spans):\n",
    "        match_id, match_gold_length, match_length, match_score = 0, 0, 0, 0\n",
    "        p_start, p_end, p_emo = pred_span\n",
    "        for ii, (t_start, t_end, t_emo) in enumerate(true_spans):\n",
    "            if p_emo == t_emo and not (p_end<=t_start or p_start>=t_end):\n",
    "                cur_match_length = min(p_end, t_end) - max(p_start, t_start)\n",
    "                cur_gold_length = t_end - t_start\n",
    "                cur_match_score = cur_match_length / float(cur_gold_length)\n",
    "                if cur_match_score > match_score:\n",
    "                    match_id = ii\n",
    "                    match_gold_length = cur_gold_length\n",
    "                    match_length = cur_match_length\n",
    "                    match_score = cur_match_score\n",
    "                if (cur_match_score == match_score) and (cur_match_score > 0):\n",
    "                    if cur_match_length > match_length:\n",
    "                        match_id = ii\n",
    "                        match_gold_length = cur_gold_length\n",
    "                        match_length = cur_match_length\n",
    "                        match_score = cur_match_score\n",
    "        return match_id, match_gold_length, match_length, match_score\n",
    "\n",
    "\n",
    "    '''\n",
    "    Proportional Match (span): Each predicted span is compared with all golden spans, and determine which golden span it matches based on the overlap ratio (match score). Then the Precision, Recall, and F1 are calculated based on the overlapping tokens.\n",
    "    '''\n",
    "    def cal_prf_span_pair_emocate_proportional(self, true_span_pair_dict, pred_span_pair_dict): # 'dia{}_emoutt{}_causeutt{}': [[span_start_id, span_end_id, emotion_category], ...]\n",
    "        prf_mat = np.zeros([7,5]) # row: emotion category; col: correct_num, true_num, pred_num, matched_true_span_num, true_span_num\n",
    "        true_span_pair_dict_copy = copy.deepcopy(true_span_pair_dict)\n",
    "        for k, v in pred_span_pair_dict.items():\n",
    "            for pred_span in v:\n",
    "                prf_mat[pred_span[2]][2] += pred_span[1] - pred_span[0]\n",
    "                if k in true_span_pair_dict:\n",
    "                    true_spans = true_span_pair_dict[k]\n",
    "                    match_id, match_gold_length, match_length, match_score = self.get_match_scores(pred_span, true_spans)\n",
    "                    if match_length > 0:\n",
    "                        prf_mat[pred_span[2]][0] += match_length\n",
    "                        prf_mat[pred_span[2]][1] += match_gold_length # Multiple predicted spans may match the same golden span.\n",
    "                        prf_mat[pred_span[2]][3] += 1\n",
    "                        if true_spans[match_id] in true_span_pair_dict_copy[k]:\n",
    "                            true_span_pair_dict_copy[k].remove(true_spans[match_id])\n",
    "        \n",
    "        for k, v in true_span_pair_dict_copy.items():\n",
    "            for true_span in v:\n",
    "                prf_mat[true_span[2]][1] += true_span[1] - true_span[0]\n",
    "                prf_mat[true_span[2]][3] += 1\n",
    "        for k, v in true_span_pair_dict.items():\n",
    "            for true_span in v:\n",
    "                prf_mat[true_span[2]][4] += 1\n",
    "        \n",
    "        p_scores = prf_mat[1:,0] / (prf_mat[1:,2]+(1e-8))\n",
    "        r_scores = prf_mat[1:,0] / (prf_mat[1:,1]+(1e-8))\n",
    "        f1_scores = 2*p_scores*r_scores/(p_scores+r_scores+(1e-8))\n",
    "        weight = prf_mat[1:,4] / sum(prf_mat[1:,4]) # Calculate the weight based on the actual number of golden spans.\n",
    "        w_avg_p = sum(p_scores*weight)\n",
    "        w_avg_r = sum(r_scores*weight)\n",
    "        w_avg_f1 = sum(f1_scores*weight)\n",
    "\n",
    "        total_correct = sum(prf_mat[1:,0])\n",
    "        micro_p = total_correct / (sum(prf_mat[1:,2])+(1e-8))\n",
    "        micro_r = total_correct / (sum(prf_mat[1:,1])+(1e-8))\n",
    "        micro_f1 = 2*micro_p*micro_r/(micro_p+micro_r+1e-8)\n",
    "\n",
    "        return [w_avg_p, w_avg_r, w_avg_f1, micro_p, micro_r, micro_f1]\n",
    "    def cal_emotion(self, true_span_pair_dict, pred_span_pair_dict):\n",
    "        return true_span_pair_dict[0] == pred_span_pair_dict[0]\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSpanHead(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(CasualSpanHead, self).__init__()\n",
    "        self.build(config_path)\n",
    "        \n",
    "        self.hidden_projection = nn.Linear(self.token_size, 1)\n",
    "        self.embedding_projection = nn.Linear(self.embedding_dim, 1)\n",
    "        \n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.candidate_num = config[\"candidate_num\"]\n",
    "        self.token_size = config[\"token_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "        self.feedforward_size = config[\"feedforward_size\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.casual_span_encoder_name = config[\"casual_span_encoder_name\"]\n",
    "        main_part = AutoModel.from_pretrained(self.casual_span_encoder_name)\n",
    "        self.casual_span_encoder = main_part.encoder\n",
    "        self.casual_span_pooler = main_part.pooler\n",
    "    def forward(self, x, candidate):\n",
    "        _x = x.unsqueeze(1)\n",
    "        _x = _x.expand(self.batch_size, self.candidate_num, self.token_size, self.embedding_dim)\n",
    "        \n",
    "        _candidate = candidate.reshape(self.candidate_num, self.batch_size, self.token_size, self.embedding_dim)\n",
    "        for i in range(self.candidate_num):\n",
    "            _candidate[i] = self.casual_span_encoder(_candidate[i])\n",
    "            _candidate[i] = self.casual_span_pooler(_candidate[i])\n",
    "        _candidate = _candidate.reshape(self.batch_size, self.candidate_num, self.token_size, self.embedding_dim)\n",
    "        \n",
    "        _similarity = F.cosine_similarity(_x, _candidate, dim=1)\n",
    "        _similarity = self.embedding_projection(_similarity).squeeze(-1)\n",
    "        _similarity = self.hidden_projection(_similarity).squeeze(-1)\n",
    "        return _similarity\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "\n",
    "class EmotionHead(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(EmotionHead, self).__init__()\n",
    "        self.build(config_path)\n",
    "        \n",
    "        self.emotion_fc = nn.Linear(self.embedding_dim, self.label_num)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.candidate_num = config[\"candidate_num\"]\n",
    "        self.token_size = config[\"token_size\"]\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.label_num = len(self.label2id)\n",
    "    def forward(self, x):\n",
    "        return self.emotion_fc(x)\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    \n",
    "        \n",
    "class TGG(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(TGG, self).__init__()\n",
    "        self.emotion_head = EmotionHead(config_path)\n",
    "        self.casual_span_head = CasualSpanHead(config_path)\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.candidate_num = config[\"candidate_num\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "        self.token_size = config[\"token_size\"]\n",
    "        self.main_part_name = config[\"main_part_name\"]\n",
    "        \n",
    "        main_part = AutoModel.from_pretrained(self.main_part_name)\n",
    "        self.embedding = main_part.embedding\n",
    "        self.encoder = main_part.encoder\n",
    "        self.pooler = main_part.pooler\n",
    "        \n",
    "    def forward(self, x, candidate):\n",
    "        _x = self.embedding(x)\n",
    "        \n",
    "        _candidate = candidate.unsqueeze(-1)\n",
    "        _candidate = _candidate.expand(self.batch_size, self.candidate_num, self.token_size, self.embedding_dim)\n",
    "        _candidate = _candidate.reshape(self.candidate_num, self.batch_size, self.token_size, self.embedding_dim)\n",
    "        for i in range(self.candidate_num):\n",
    "            _candidate[i] = self.embedding(_candidate[i])\n",
    "        _candidate = _candidate.reshape(self.batch_size, self.candidate_num, self.token_size, self.embedding_dim)\n",
    "        \n",
    "        _x = self.encoder(_x)\n",
    "        _x = self.pooler(_x)\n",
    "        _e_category = self.emotion_head(_x)\n",
    "        _casual_span = self.casual_span_head(_x, _candidate)\n",
    "        return _e_category, _casual_span\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionHead(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(EmotionHead, self).__init__()\n",
    "        self.build(config_path)\n",
    "        \n",
    "        self.emotion_fc = nn.Linear(self.embedding_dim, self.label_num)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.candidate_num = config[\"candidate_num\"]\n",
    "        self.token_size = config[\"token_size\"]\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.label_num = len(self.label2id)\n",
    "    def forward(self, x):\n",
    "        return self.emotion_fc(x)\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    \n",
    "        \n",
    "class TGG(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(TGG, self).__init__()\n",
    "        self.build(config_path)\n",
    "        self.emotion_head = EmotionHead(config_path)\n",
    "        self.casual_feedforward = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(self.hidden_size, self.embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.candidate_num = config[\"candidate_num\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "        self.token_size = config[\"token_size\"]\n",
    "        self.main_part_name = config[\"main_part_name\"]\n",
    "        # self.batch_size = config[\"batch_size\"]\n",
    "        self.device = config[\"device\"]\n",
    "        \n",
    "        main_part = AutoModel.from_pretrained(self.main_part_name)\n",
    "        self.embeddings = main_part.embeddings\n",
    "        self.encoder = main_part.encoder\n",
    "        self.pooler = main_part.pooler\n",
    "        \n",
    "    def forward(self, x, candidate):\n",
    "        self.batch_size = x.size(0)\n",
    "        # print(\"x: \"+str(x.shape))\n",
    "        _x = self.embeddings(x)\n",
    "        _x = self.encoder(_x)['last_hidden_state']\n",
    "        _x = self.pooler(_x)\n",
    "        _e_category = self.emotion_head(_x)\n",
    "\n",
    "\n",
    "        _candidate = torch.zeros((self.candidate_num, self.batch_size, self.embedding_dim), device=self.device)\n",
    "        cand = candidate.reshape(self.candidate_num, self.batch_size, self.token_size)\n",
    "        for i in range(self.candidate_num):\n",
    "            _candidate[i] = self.pooler(self.encoder(self.embeddings(cand[i]))['last_hidden_state'])\n",
    "        _candidate = _candidate.reshape(self.batch_size, self.candidate_num, self.embedding_dim)\n",
    "        \n",
    "        # print(_candidate)\n",
    "        _candidate = self.casual_feedforward(_candidate)\n",
    "        _x = self.casual_feedforward(_x)\n",
    "        _x = _x.unsqueeze(1)\n",
    "        _x = _x.expand(self.batch_size, self.candidate_num, self.embedding_dim)\n",
    "        _similarity = F.cosine_similarity(_x, _candidate, dim=2)\n",
    "        \n",
    "        return _e_category, _similarity\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.layers_to_freeze = []\n",
    "        self.history = {\n",
    "            \"train loss\": [],\n",
    "            \"valid loss\": [],\n",
    "            \"valid metrics\": []\n",
    "        }\n",
    "        self.evaluator = Evaluation(config_path)\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        print(config_path)\n",
    "        config = self.read_json(config_path)\n",
    "        self.patience = config['patience']\n",
    "        self.min_delta = config['min_delta']\n",
    "        self.fn_loss_name = config['loss_fn_name']\n",
    "        self.batch_size_config = config['batch_size']\n",
    "        self.device = config['device']\n",
    "        self.save_checkpoint_dir = config['save_checkpoint_dir']\n",
    "        self.emotion_head_layers = config['emotion_head_layers']\n",
    "        self.casual_emotion_layers = config['casual_emotion_layers']\n",
    "        self.main_layers = config['main_layers']\n",
    "        # if \"CrossEntropyLoss\" == self.fn_loss_name:\n",
    "        self.fn_loss = nn.CrossEntropyLoss()\n",
    "        self.early_stopper = EarlyStopper(self.patience, self.min_delta)\n",
    "    def emotion_head_training(self):\n",
    "        self.layers_to_freeze = self.casual_emotion_layers + self.main_layers\n",
    "        \n",
    "    def casual_emotion_training(self):\n",
    "        self.layers_to_freeze = self.emotion_head_layers + self.main_layers\n",
    "\n",
    "    def full_training(self):\n",
    "        self.layers_to_freeze = []\n",
    "    def freeze(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(layer in name for layer in self.layers_to_freeze):\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        return model\n",
    "    def train(self, model, optimizer, train_dataset, valid_dataset, epochs, option=\"full\", batch_size=None):\n",
    "        self.optimizer = optimizer\n",
    "        if option == \"emotion\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_head_training()\n",
    "        elif option == \"casual_emotion\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.casual_emotion_training()\n",
    "        else:\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.full_training()\n",
    "\n",
    "        if batch_size != None:\n",
    "            self.batch_size = batch_size\n",
    "        else:\n",
    "            self.batch_size = self.batch_size_config\n",
    "\n",
    "\n",
    "        self.early_stopper.reset()\n",
    "        self.model = self.freeze(model)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.train_dataloader = DataLoader(dataset=train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        self.epochs = epochs\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self._train_epoch(epoch)\n",
    "            val_loss = self._val_epoch(epoch)\n",
    "            \n",
    "            self.history[\"train loss\"].append(train_loss)\n",
    "            self.history[\"valid loss\"].append(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.save_checkpoint(epoch)\n",
    "                print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f},  Valid loss {val_loss:.4f}\")\n",
    "            if self.early_stopper.early_stop(val_loss):\n",
    "                print(\"Early stop at epoch: \"+str(epoch) + \" with valid loss: \"+str(val_loss))\n",
    "                break\n",
    "            \n",
    "    def _train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "        logger_message = f'Training epoch {epoch}/{self.epochs}'\n",
    "\n",
    "        progress_bar = tqdm(self.train_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        for batch, data in enumerate(progress_bar):\n",
    "            conversation_id = data[\"conversation_ID\"]\n",
    "            paragraph = data[\"paragraph\"].to(self.device)\n",
    "            utter_id = data[\"utterance_ID\"]\n",
    "            casual_pool = data[\"casual_pool\"].to(self.device)\n",
    "            casual_span_pool = data[\"casual_span_pool\"]\n",
    "            emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "            span_label = data[\"span_label\"].to(self.device)\n",
    "            # print(\"conversation_id:\"+str(conversation_id.shape))\n",
    "            # print(\"paragraph:\"+str(paragraph.shape))\n",
    "            # print(\"casual_pool:\"+str(casual_pool.shape))\n",
    "            # print(\"emotion_label:\"+str(emotion_label.shape))\n",
    "            # print(\"span_label:\"+str(span_label.shape))\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            emotion_pred, span_pred = self.model(paragraph, casual_pool)\n",
    "            \n",
    "            emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "            span_loss = self.fn_loss(span_pred, span_label)\n",
    "            loss = emotion_loss + span_loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return train_loss / len(self.train_dataloader)\n",
    "\n",
    "    def _val_epoch(self, epoch):\n",
    "        self.model.eval()\n",
    "        valid_loss = 0.0\n",
    "        valid_metrics = {}\n",
    "        logger_message = f'Validation epoch {epoch}/{self.epochs}'\n",
    "        progress_bar = tqdm(self.valid_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(progress_bar):\n",
    "                conversation_id = data[\"conversation_ID\"]\n",
    "                paragraph = data[\"paragraph\"].to(self.device)\n",
    "                utter_id = data[\"utterance_ID\"]\n",
    "                casual_pool = data[\"casual_pool\"].to(self.device)\n",
    "                casual_span_pool = data[\"casual_span_pool\"]\n",
    "                emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "                span_label = data[\"span_label\"].to(self.device)\n",
    "                emotion_pred, span_pred = self.model(paragraph, casual_pool)\n",
    "                \n",
    "                emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                span_loss = self.fn_loss(span_pred, span_label)\n",
    "                loss = emotion_loss + span_loss\n",
    "                valid_loss += loss.item()\n",
    "                \n",
    "        return valid_loss / len(self.valid_dataloader)\n",
    "\n",
    "    def recalculation(self, evaluate_score, valid_metrics, option=\"sum\"):\n",
    "        if option==\"sum\":\n",
    "            for key in evaluate_score.keys():\n",
    "                if key not in valid_metrics.keys():\n",
    "                    valid_metrics[key]=[]\n",
    "                valid_metrics[key].append(evaluate_score[key])\n",
    "            return valid_metrics\n",
    "        else:\n",
    "            for key in evaluate_score.keys():\n",
    "                valid_metrics[key] = sum(valid_metrics[key]) / len(valid_metrics[key])\n",
    "            return valid_metrics\n",
    "\n",
    "    def save_checkpoint(self, epoch):\n",
    "        checkpoint_path = os.path.join(self.save_checkpoint_dir, 'checkpoint_{}.pth'.format(epoch))\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }, checkpoint_path)\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(Prediction, self).__init__()\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.id2label = config[\"id2label\"]\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.submit_path = config[\"submit path\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "    def predict(self, model, raw_dataset, dataset):\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "        pred_list = []\n",
    "        \n",
    "        self.test_dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        logger_message = f'Predict '\n",
    "        progress_bar = tqdm(self.test_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(progress_bar):\n",
    "                conversation_id = data[\"conversation_ID\"]\n",
    "                paragraph = data[\"paragraph\"].to(self.device)\n",
    "                utter_id = data[\"utterance_ID\"]\n",
    "                casual_pool = data[\"casual_pool\"].to(self.device)\n",
    "                casual_span_pool = data[\"casual_span_pool\"]\n",
    "                # emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "                # span_label = data[\"span_label\"].to(self.device)\n",
    "                emotion_pred, span_pred = self.model(paragraph, casual_pool)\n",
    "                \n",
    "                pred = self.convertpred2list(conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool)\n",
    "                pred_list.extend(pred)\n",
    "        submit = self.convertdict2submit(self.convertpred2dict(raw_dataset, pred_list))\n",
    "        self.save_json(self.submit_path, submit)\n",
    "    def convertpred2list(self, conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool):\n",
    "        pred_list = []\n",
    "        for i in range(len(emotion_pred)):\n",
    "            emotion = self.id2label[str(torch.argmax(emotion_pred[i]).item())]\n",
    "            emotion_string = str(utter_id[i].item()) + \"_\" + emotion\n",
    "            if emotion != \"neutral\":\n",
    "                # \n",
    "                span = torch.round(span_pred[i])\n",
    "                # print(\"span: \"+str(span))\n",
    "                # print(\"casual_span_pool: \"+str(casual_span_pool[i]))\n",
    "                for j in range(len(span)):\n",
    "                    if span[j] == 1:\n",
    "                        if casual_span_pool[i][j][0].item() == 0:\n",
    "                            break\n",
    "                        if casual_span_pool[i][j][1].item() == casual_span_pool[i][j][2].item():\n",
    "                            break\n",
    "                        span_string = str(int(casual_span_pool[i][j][0].item())) + \"_\" + str(int(casual_span_pool[i][j][1].item())) + \"_\" + str(int(casual_span_pool[i][j][2].item()))\n",
    "                        pred_dict = {}\n",
    "                        pred_dict[\"conversation_ID\"] = conversation_id[i].item()\n",
    "                        pred_dict[\"emotion-cause_pairs\"] = [emotion_string, span_string]\n",
    "                        # print(\"pred_dict: \"+str(pred_dict))\n",
    "                        pred_list.append(pred_dict)\n",
    "        return pred_list\n",
    "    def convertpred2dict(self, raw_dataset, pred_list):\n",
    "        dataset = self.convertlist2dict(raw_dataset)\n",
    "        # dict = \n",
    "        for i in range(len(pred_list)):\n",
    "            pred = pred_list[i]\n",
    "            conversation_id = pred[\"conversation_ID\"]\n",
    "            # print(\"conversation_id: \"+str(conversation_id))\n",
    "            emotion_cause_pairs = pred[\"emotion-cause_pairs\"]\n",
    "            # print(\"emotion_cause_pairs: \"+str(emotion_cause_pairs))\n",
    "            if \"emotion-cause_pairs\" in dataset[conversation_id]:\n",
    "                dataset[conversation_id][\"emotion-cause_pairs\"].append(emotion_cause_pairs)\n",
    "            else:\n",
    "                dataset[conversation_id][\"emotion-cause_pairs\"] = []\n",
    "        # print(dataset)\n",
    "        return dataset\n",
    "    def convertlist2dict(self, raw_dataset):\n",
    "        dataset = {}\n",
    "        for i in range(len(raw_dataset)):\n",
    "            conversation = raw_dataset[i]\n",
    "            conversation_ID = conversation[\"conversation_ID\"]\n",
    "            # if conversation_ID not in dataset.key\n",
    "            dataset[conversation_ID] = {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"conversation\": conversation['conversation']\n",
    "            }\n",
    "        return dataset\n",
    "    def convertdict2submit(self, raw_dataset):\n",
    "        dataset = []\n",
    "        for key in raw_dataset.keys():\n",
    "            # print(\"key: \"+str(key))\n",
    "            sample = raw_dataset[key]\n",
    "            # print(\"sample: \"+str(sample))\n",
    "            dataset.append(sample)\n",
    "        return dataset\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    def save_json(self, file_path, data):\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config.json\"\n",
    "dataset = read_json(\"data/ECF 2.0/train/train.json\")\n",
    "trial_data = read_json(\"data/ECF 2.0/trial/trial.json\")\n",
    "raw_trial_data = read_json(\"data/ECF 2.0/trial/Subtask_1_trial.json\")\n",
    "\n",
    "n_train = int(0.8 * len(dataset))\n",
    "train_data, valid_data = dataset[:n_train], dataset[n_train:]\n",
    "train_dataset = ConversationDataset(config_path, train_data)\n",
    "valid_dataset = ConversationDataset(config_path, valid_data)\n",
    "model = TGG(config_path)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(\"\\\"\" + name + \"\\\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\n",
      "TRAINING emotion MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0/100: 100%|██████████| 85/85 [07:49<00:00,  5.52s/it]\n",
      "Validation epoch 0/100: 100%|██████████| 22/22 [02:01<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss 2.6259,  Valid loss 9.4572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/100: 100%|██████████| 85/85 [08:47<00:00,  6.20s/it]\n",
      "Validation epoch 1/100: 100%|██████████| 22/22 [02:14<00:00,  6.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train loss 2.5920,  Valid loss 9.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2/100: 100%|██████████| 85/85 [09:11<00:00,  6.49s/it]\n",
      "Validation epoch 2/100: 100%|██████████| 22/22 [02:10<00:00,  5.91s/it]\n",
      "Training epoch 3/100: 100%|██████████| 85/85 [08:56<00:00,  6.31s/it]\n",
      "Validation epoch 3/100: 100%|██████████| 22/22 [02:08<00:00,  5.84s/it]\n",
      "Training epoch 4/100: 100%|██████████| 85/85 [09:13<00:00,  6.51s/it]\n",
      "Validation epoch 4/100: 100%|██████████| 22/22 [02:16<00:00,  6.23s/it]\n",
      "Training epoch 5/100: 100%|██████████| 85/85 [09:17<00:00,  6.56s/it]\n",
      "Validation epoch 5/100: 100%|██████████| 22/22 [02:13<00:00,  6.05s/it]\n",
      "Training epoch 6/100: 100%|██████████| 85/85 [09:09<00:00,  6.47s/it]\n",
      "Validation epoch 6/100: 100%|██████████| 22/22 [02:12<00:00,  6.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop at epoch: 6 with valid loss: 9.535531130704014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict : 100%|██████████| 45/45 [00:18<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config_path)\n",
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion\", batch_size=128)\n",
    "trial_dataset = ConversationDataset(config_path, trial_data, option=\"trial\")\n",
    "predict = Prediction(config_path)\n",
    "predict.predict(model, raw_trial_data, trial_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING casual_emotion MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0/100: 100%|██████████| 85/85 [08:59<00:00,  6.35s/it]\n",
      "Validation epoch 0/100: 100%|██████████| 22/22 [02:11<00:00,  5.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss 2.5378,  Valid loss 9.4816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/100: 100%|██████████| 85/85 [09:00<00:00,  6.36s/it]\n",
      "Validation epoch 1/100: 100%|██████████| 22/22 [02:11<00:00,  5.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train loss 2.5345,  Valid loss 9.3875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2/100: 100%|██████████| 85/85 [09:01<00:00,  6.38s/it]\n",
      "Validation epoch 2/100: 100%|██████████| 22/22 [02:12<00:00,  6.01s/it]\n",
      "Training epoch 3/100: 100%|██████████| 85/85 [09:07<00:00,  6.44s/it]\n",
      "Validation epoch 3/100: 100%|██████████| 22/22 [02:08<00:00,  5.86s/it]\n",
      "Training epoch 4/100: 100%|██████████| 85/85 [09:02<00:00,  6.38s/it]\n",
      "Validation epoch 4/100: 100%|██████████| 22/22 [02:11<00:00,  5.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss 2.5296,  Valid loss 9.3270\n",
      "Early stop at epoch: 4 with valid loss: 9.327030485326594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict : 100%|██████████| 45/45 [00:17<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"casual_emotion\", batch_size=128)\n",
    "trial_dataset = ConversationDataset(config_path, trial_data, option=\"trial\")\n",
    "predict = Prediction(config_path)\n",
    "predict.predict(model, raw_trial_data, trial_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING full MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0/100: 100%|██████████| 1347/1347 [23:17<00:00,  1.04s/it]\n",
      "Validation epoch 0/100: 100%|██████████| 337/337 [02:03<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss 2.7653,  Valid loss 11.4463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/100: 100%|██████████| 1347/1347 [22:27<00:00,  1.00s/it]\n",
      "Validation epoch 1/100: 100%|██████████| 337/337 [02:02<00:00,  2.74it/s]\n",
      "Training epoch 2/100: 100%|██████████| 1347/1347 [22:30<00:00,  1.00s/it]\n",
      "Validation epoch 2/100: 100%|██████████| 337/337 [02:02<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train loss 2.6326,  Valid loss 7.4694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3/100: 100%|██████████| 1347/1347 [22:30<00:00,  1.00s/it]\n",
      "Validation epoch 3/100: 100%|██████████| 337/337 [02:02<00:00,  2.75it/s]\n",
      "Training epoch 4/100: 100%|██████████| 1347/1347 [22:28<00:00,  1.00s/it]\n",
      "Validation epoch 4/100: 100%|██████████| 337/337 [02:01<00:00,  2.77it/s]\n",
      "Training epoch 5/100: 100%|██████████| 1347/1347 [22:20<00:00,  1.00it/s]\n",
      "Validation epoch 5/100: 100%|██████████| 337/337 [02:00<00:00,  2.79it/s]\n",
      "Training epoch 6/100: 100%|██████████| 1347/1347 [22:14<00:00,  1.01it/s]\n",
      "Validation epoch 6/100: 100%|██████████| 337/337 [02:00<00:00,  2.79it/s]\n",
      "Training epoch 7/100: 100%|██████████| 1347/1347 [22:12<00:00,  1.01it/s]\n",
      "Validation epoch 7/100: 100%|██████████| 337/337 [02:00<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop at epoch: 7 with valid loss: 11.001746106925987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict : 100%|██████████| 45/45 [00:15<00:00,  2.89it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"full\")\n",
    "trial_dataset = ConversationDataset(config_path, trial_data, option=\"trial\")\n",
    "predict = Prediction(config_path)\n",
    "predict.predict(model, raw_trial_data, trial_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ga_aug_std",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
