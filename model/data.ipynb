{"cells":[{"cell_type":"code","source":["pip install textattack"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LC2PzmCq-o9O","executionInfo":{"status":"ok","timestamp":1705470823441,"user_tz":-420,"elapsed":52939,"user":{"displayName":"Duc Le Ngoc","userId":"10890292381411576176"}},"outputId":"c91f6362-2110-486b-ae0e-de2958931794"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textattack\n","  Downloading textattack-0.3.9-py3-none-any.whl (436 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bert-score>=0.3.5 (from textattack)\n","  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.6.2)\n","Collecting flair (from textattack)\n","  Downloading flair-0.13.1-py3-none-any.whl (388 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.13.1)\n","Collecting language-tool-python (from textattack)\n","  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n","Collecting lemminflect (from textattack)\n","  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lru-dict (from textattack)\n","  Downloading lru_dict-1.3.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Collecting datasets>=2.4.0 (from textattack)\n","  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.23.5)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.5.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.11.4)\n","Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.1.0+cu121)\n","Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (4.35.2)\n","Collecting terminaltables (from textattack)\n","  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.66.1)\n","Collecting word2number (from textattack)\n","  Downloading word2number-1.1.zip (9.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting num2words (from textattack)\n","  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (10.1.0)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.7.1)\n","Collecting pinyin>=0.4.0 (from textattack)\n","  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n","Collecting OpenHowNet (from textattack)\n","  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n","Collecting pycld2 (from textattack)\n","  Downloading pycld2-0.41.tar.gz (41.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting click<8.1.0 (from textattack)\n","  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (2.31.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (23.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.6)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.4.0->textattack)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.4.1)\n","Collecting multiprocess (from datasets>=2.4.0->textattack)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.20.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (6.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2023.3.post1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.4.1)\n","Collecting boto3>=1.20.27 (from flair->textattack)\n","  Downloading boto3-1.34.20-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bpemb>=0.3.2 (from flair->textattack)\n","  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n","Collecting conllu>=4.0 (from flair->textattack)\n","  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n","Collecting deprecated>=1.2.13 (from flair->textattack)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Collecting ftfy>=6.1.0 (from flair->textattack)\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.6.6)\n","Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.3.2)\n","Collecting janome>=0.4.2 (from flair->textattack)\n","  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langdetect>=1.0.9 (from flair->textattack)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.9.4)\n","Collecting mpld3>=0.3 (from flair->textattack)\n","  Downloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pptree>=3.1 (from flair->textattack)\n","  Downloading pptree-3.1.tar.gz (3.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytorch-revgrad>=0.2.0 (from flair->textattack)\n","  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.2)\n","Collecting segtok>=1.5.11 (from flair->textattack)\n","  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n","Collecting sqlitedict>=2.0.0 (from flair->textattack)\n","  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.9.0)\n","Collecting transformer-smaller-training-vocab>=0.2.3 (from flair->textattack)\n","  Downloading transformer_smaller_training_vocab-0.3.3-py3-none-any.whl (14 kB)\n","Collecting urllib3<2.0.0,>=1.0.0 (from flair->textattack)\n","  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wikipedia-api>=0.5.7 (from flair->textattack)\n","  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n","Collecting semver<4.0.0,>=3.0.0 (from flair->textattack)\n","  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.3.2)\n","Collecting docopt>=0.6.2 (from num2words->textattack)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting anytree (from OpenHowNet->textattack)\n","  Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (67.7.2)\n","Collecting botocore<1.35.0,>=1.34.20 (from boto3>=1.20.27->flair->textattack)\n","  Downloading botocore-1.34.20-py3-none-any.whl (11.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair->textattack)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair->textattack)\n","  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece (from bpemb>=0.3.2->flair->textattack)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair->textattack) (1.14.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (4.0.3)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair->textattack) (0.2.13)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (1.16.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (4.11.2)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair->textattack) (6.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.47.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2023.11.17)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair->textattack) (3.2.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (3.20.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n","Collecting accelerate>=0.20.3 (from transformers>=4.30.0->textattack)\n","  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers>=4.30.0->textattack) (5.9.5)\n","Building wheels for collected packages: pinyin, pycld2, word2number, docopt, langdetect, pptree, sqlitedict\n","  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630476 sha256=7e6a7121ea89a2f62107ab7bb8debe65c2860f3a0b84222d916dc8db5a111aa7\n","  Stored in directory: /root/.cache/pip/wheels/33/38/af/616fc6f154aa5bae65a1da12b22d79943434269f0468ff9b3f\n","  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycld2: filename=pycld2-0.41-cp310-cp310-linux_x86_64.whl size=9904071 sha256=e8f8b56ae28de91d564c5f03de5e01436504b78212de193d6a56a5d068dd272e\n","  Stored in directory: /root/.cache/pip/wheels/be/81/31/240c89c845e008a93d98542325270007de595bfd356eb0b06c\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=612cc87a849fd24112eba670e05410238412119b23a7f2e26b6515ca5736163e\n","  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=932c8ed8687429b466a765b6a4b332225d66b65476280a353cc2cbd014b18924\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=8e01df9aec6befe8872b5ba33e0af8ad73ca5a075bdda750849d2c2a01d23a17\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=cef9de036e6cb6107cdc128d921fb98927cf89d49a349382541f34554b14a401\n","  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=8e47c35a842327a2247be62d779974e8e8e80329d08768dbcf2cf57ae24d858c\n","  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n","Successfully built pinyin pycld2 word2number docopt langdetect pptree sqlitedict\n","Installing collected packages: word2number, sqlitedict, sentencepiece, pycld2, pptree, pinyin, janome, docopt, urllib3, terminaltables, semver, segtok, num2words, lru-dict, lemminflect, langdetect, jmespath, ftfy, dill, deprecated, conllu, click, anytree, multiprocess, botocore, wikipedia-api, s3transfer, pytorch-revgrad, OpenHowNet, mpld3, language-tool-python, bpemb, datasets, boto3, accelerate, bert-score, transformer-smaller-training-vocab, flair, textattack\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.7\n","    Uninstalling urllib3-2.0.7:\n","      Successfully uninstalled urllib3-2.0.7\n","  Attempting uninstall: click\n","    Found existing installation: click 8.1.7\n","    Uninstalling click-8.1.7:\n","      Successfully uninstalled click-8.1.7\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed OpenHowNet-2.0 accelerate-0.26.1 anytree-2.12.1 bert-score-0.3.13 boto3-1.34.20 botocore-1.34.20 bpemb-0.3.4 click-8.0.4 conllu-4.5.3 datasets-2.16.1 deprecated-1.2.14 dill-0.3.7 docopt-0.6.2 flair-0.13.1 ftfy-6.1.3 janome-0.5.0 jmespath-1.0.1 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.3.0 mpld3-0.5.10 multiprocess-0.70.15 num2words-0.5.13 pinyin-0.4.0 pptree-3.1 pycld2-0.41 pytorch-revgrad-0.2.0 s3transfer-0.10.0 segtok-1.5.11 semver-3.0.2 sentencepiece-0.1.99 sqlitedict-2.1.0 terminaltables-3.1.10 textattack-0.3.9 transformer-smaller-training-vocab-0.3.3 urllib3-1.26.18 wikipedia-api-0.6.0 word2number-1.1\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bx_qi8oE-fSW","executionInfo":{"status":"ok","timestamp":1705470838879,"user_tz":-420,"elapsed":15441,"user":{"displayName":"Duc Le Ngoc","userId":"10890292381411576176"}},"outputId":"49050678-64c1-41ff-a292-691cf4080989"},"outputs":[{"output_type":"stream","name":"stderr","text":["textattack: Updating TextAttack package dependencies.\n","textattack: Downloading NLTK required packages.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package omw to /root/nltk_data...\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import json\n","import string\n","import random\n","from textattack.augmentation import WordNetAugmenter"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7qo0c5s-iOa","executionInfo":{"status":"ok","timestamp":1705470764414,"user_tz":-420,"elapsed":26413,"user":{"displayName":"Duc Le Ngoc","userId":"10890292381411576176"}},"outputId":"e83fad6e-f03e-44ff-996d-36190dd606ac"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"kqn28u6J-fSX","executionInfo":{"status":"ok","timestamp":1705470735401,"user_tz":-420,"elapsed":2,"user":{"displayName":"Duc Le Ngoc","userId":"10890292381411576176"}}},"outputs":[],"source":["def read_json(path):\n","    with open(path, 'r') as json_file:\n","        data = json.load(json_file)\n","    return data\n","def write_json(path, data):\n","    with open(path, 'w') as file:\n","        # Write the Python data structure as JSON to the file\n","        json.dump(data, file, indent=2)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ROYffTTm-fSX","executionInfo":{"status":"ok","timestamp":1705470859655,"user_tz":-420,"elapsed":2913,"user":{"displayName":"Duc Le Ngoc","userId":"10890292381411576176"}},"outputId":"b7100373-8511-4efe-8ced-914a4d2e4a20"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}],"source":["config_path = \"/content/drive/MyDrive/NAACL/model/config.json\"\n","config = read_json(config_path)\n","left_padding = config[\"left_padding\"]\n","right_padding = config[\"right_padding\"]\n","\n","punctuation_error = [\n","        \"%\"\n","    ]\n","wordnet_aug = WordNetAugmenter()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Ii1Hrcyb-fSX","executionInfo":{"status":"ok","timestamp":1705470859655,"user_tz":-420,"elapsed":1,"user":{"displayName":"Duc Le Ngoc","userId":"10890292381411576176"}}},"outputs":[],"source":["def remove_punctuation(text):\n","    # Create a string of all punctuation characters\n","    punctuation_chars = string.punctuation\n","\n","    # Remove punctuation at the beginning and end of the string\n","    cleaned_text = text.strip(punctuation_chars).strip().strip(punctuation_chars).strip().strip(punctuation_chars).strip()\n","\n","    return cleaned_text\n","def span(text, sub_text, s_ind=0):\n","    sub_text = remove_punctuation(sub_text)\n","    new_text = text.split()\n","    if text == sub_text:\n","        ss = s_ind\n","        se = s_ind+len(new_text)\n","        return ss, se, text\n","    new_sub_text = sub_text\n","    new_sub_text = new_sub_text.split()\n","    check = True\n","    for i in range(len(new_text)):\n","        temp = i\n","        for j in range(len(new_sub_text)):\n","            if new_sub_text[j] != new_text[i + j]:\n","                if j == len(new_sub_text)-1:\n","                    for punctuation in punctuation_error:\n","\n","                        if new_sub_text[j]+punctuation == new_text[i + j]:\n","                            # print(new_sub_text)\n","                            new_sub_text[j] = new_sub_text[j]+punctuation\n","                            ss = i\n","                            se = i+j\n","                            print(new_sub_text)\n","                            return s_ind+ss, s_ind+se+1, ' '.join(new_sub_text)\n","                check = False\n","                break\n","            else:\n","                temp = i + j\n","                check = True\n","        if check == True:\n","            ss = i\n","            se = temp\n","            return s_ind+ss, s_ind+se+1, sub_text\n","\n","def convertraw_to_train(data):\n","    new_data = []\n","    max_len = 0\n","    for i in range(len(data)):\n","        conversation_ID = data[i]['conversation_ID']\n","        conversation = data[i]['conversation']\n","\n","        utterance_ID_pool = []\n","        text_pool = []\n","        speaker_pool = []\n","        emotion_pool = {}\n","        for j in range(len(conversation)):\n","            utterance_ID_pool.append(conversation[j]['utterance_ID'])\n","            text_pool.append(conversation[j]['text'])\n","            if max_len < len(conversation[j]['text'].split()):\n","                max_len = len(conversation[j]['text'].split())\n","            speaker_pool.append(conversation[j]['speaker'])\n","            emotion_pool[j+1] = {}\n","            emotion_pool[j+1][\"property\"] = {}\n","            emotion_pool[j+1][\"casual\"] = {}\n","            if \"emotion\" in conversation[j].keys():\n","                emotion_pool[j+1][\"property\"] = conversation[j]['emotion']\n","\n","\n","        if \"emotion-cause_pairs\" in data[i].keys():\n","            cause_pairs_pool = []\n","            cause_pairs = data[i]['emotion-cause_pairs']\n","            for j in range(len(cause_pairs)):\n","                emotion_utter = cause_pairs[j][0]\n","                casual_utter = cause_pairs[j][1]\n","                utter, emotion = emotion_utter.split(\"_\")\n","                casual_utter, casual_text = casual_utter.split(\"_\")\n","                # if conversation_ID == 1049:\n","                #     print(\"cause_pairs[j][0]:\" +str(cause_pairs[j][0]))\n","                #     print(\"cause_pairs[j][1]:\" +str(cause_pairs[j][1]))\n","                #     print(\"utter:\" +str(utter))\n","                #     print(\"casual_utter:\" +str(casual_utter))\n","                #     print(\"int(utter) - left_padding :\" +str(int(utter) - left_padding))\n","                #     print(\"int(utter) + right_padding :\" +str(int(utter) + right_padding))\n","                if int(utter) - left_padding > int(casual_utter):\n","                    continue\n","                if int(utter) + right_padding < int(casual_utter):\n","                    continue\n","                start, end, _ = span(text_pool[int(casual_utter)-1], casual_text)\n","                emotion_pool[int(utter)][\"casual\"][int(casual_utter)] = {\n","                    \"casual_text\": casual_text,\n","                    \"start\": start,\n","                    \"end\": end\n","                }\n","\n","\n","        for j in range(len(conversation)):\n","            start_ids = max(1, j-left_padding+1)\n","            end_ids = min(j+right_padding+2, len(conversation)+1)\n","            for k in range(start_ids, end_ids):\n","                if k not in emotion_pool[j+1][\"casual\"].keys():\n","                    emotion_pool[j+1][\"casual\"][k] = {\n","                        \"casual_text\": \"\",\n","                        \"start\": 0,\n","                        \"end\": 0\n","                    }\n","\n","            emotion_pool[j+1][\"casual\"] = dict(sorted(emotion_pool[j+1][\"casual\"].items()))\n","\n","            pool = []\n","            for k in emotion_pool[j+1][\"casual\"].keys():\n","                pool.append({\n","                    \"utter_ID\": k,\n","                    \"casual_text\": emotion_pool[j+1][\"casual\"][k][\"casual_text\"],\n","                    \"start\": emotion_pool[j+1][\"casual\"][k][\"start\"],\n","                    \"end\": emotion_pool[j+1][\"casual\"][k][\"end\"]\n","                })\n","            emotion_pool[j+1][\"casual\"] = pool\n","\n","            start_ids = max(0, j-left_padding)\n","            end_ids = min(j+right_padding+1, len(conversation))\n","            paragraph = \" \".join(text_pool[start_ids: end_ids])\n","\n","            new_data.append({\n","                \"conversation_ID\" : conversation_ID,\n","                \"utterance_ID\" : utterance_ID_pool[j],\n","                \"paragraph\": paragraph,\n","                \"emotion\": emotion_pool[j+1],\n","                \"text_pool\": text_pool[start_ids: end_ids],\n","                \"speaker_pool\": speaker_pool[start_ids: end_ids],\n","            })\n","    print(max_len)\n","    return new_data\n","\n","def count(data):\n","    dictionary = {}\n","\n","    for item in data:\n","        # print( dictionary.keys())\n","        emotion = item['emotion']['property']\n","        if emotion in dictionary.keys():\n","            # print(emotion)\n","            dictionary[emotion] +=1\n","        else:\n","            # print(emotion)\n","            dictionary[emotion] =1\n","    return dictionary\n","\n","def count_dict(data):\n","    dictionary = {}\n","\n","    for item in data.keys():\n","        dictionary[item] = len(data[item])\n","    return dictionary\n","\n","def count_dis(data):\n","    dict_dis = {}\n","    num = 0\n","    for i in range(len(data)):\n","        emotion_casual_pairs = data[i]['emotion-cause_pairs']\n","        for j in range(len(emotion_casual_pairs)):\n","            emotion_utter = emotion_casual_pairs[j][0]\n","            casual_utter = emotion_casual_pairs[j][1]\n","            utter, emotion = emotion_utter.split(\"_\")\n","            casual_utter, casual_text = casual_utter.split(\"_\")\n","            utter = int(utter)\n","            casual_utter = int(casual_utter)\n","            num += 1\n","            if utter - casual_utter not in dict_dis.keys():\n","                dict_dis[utter - casual_utter] = 1\n","            else:\n","                dict_dis[utter - casual_utter] += 1\n","\n","    dict_dis = dict(sorted(dict_dis.items(), key=lambda item: item[1]))\n","    dict_r = {}\n","    for j in dict_dis.keys():\n","        dict_r[j] = dict_dis[j]/num\n","\n","    return dict_dis, dict_r\n","def augment_data(data):\n","    dict = {}\n","    for item in data:\n","        property = item['emotion']['property']\n","        if property not in dict:\n","            dict[property] = [item]\n","        else:\n","            dict[property].append(item)\n","\n","    num_elements_to_choose = len(dict['neutral']) // 3\n","\n","    # Randomly choose one-third of the elements\n","    dict['neutral'] = random.sample(dict['neutral'], num_elements_to_choose)\n","\n","    count_appear = count_dict(dict)\n","    max_appear = max(count_appear.values())\n","\n","    num_augment = {}\n","    for emotion in count_appear.keys():\n","        num_augment[emotion] = max_appear - count_appear[emotion]\n","    print(\"num_augment : \"+str(num_augment))\n","\n","    augment_dict = {\n","        \"joy\": [],\n","        \"neutral\": []\n","    }\n","\n","    for emotion in dict.keys():\n","        for i in range(num_augment[emotion]):\n","            index = i % len(dict[emotion])\n","            sample = augment_sample(dict[emotion][index])\n","            if emotion not in augment_dict:\n","                augment_dict[emotion]=[sample]\n","            else:\n","                augment_dict[emotion].append(sample)\n","            # print(dict[emotion][index])\n","            # print(sample)\n","            # print(\"====\")\n","\n","    train_dict = {}\n","    test_dict = {}\n","    train_data = []\n","    test_data = []\n","    print(dict.keys())\n","    print(augment_dict.keys())\n","    for emotion in dict.keys():\n","        print(emotion)\n","        list = dict[emotion] + augment_dict[emotion]\n","        n_train = int(0.8*len(list))\n","        train_dict[emotion] = list[:n_train]\n","        test_dict[emotion] = list[n_train:]\n","        train_data.extend(train_dict[emotion])\n","        test_data.extend(test_dict[emotion])\n","    # new_data = neutral_data[0:int(len(neutral_data)/3)] + other_data\n","\n","\n","\n","    random.shuffle(train_data)\n","    random.shuffle(test_data)\n","    return train_data, test_data\n","def augment_sample(sample):\n","    emotion = sample[\"emotion\"]\n","    text_pool = sample[\"text_pool\"]\n","\n","    for i in range(len(text_pool)):\n","        if emotion['casual'][i][\"casual_text\"] == \"\":\n","            text_pool[i] = augment_text(text_pool[i])[0]\n","            # print(augment_text(text_pool[i]))\n","\n","    paragraph = \"\"\n","    for i in range(len(text_pool)):\n","        # print(new_text_pool[i])\n","        paragraph += \" \" + text_pool[i]\n","\n","    new_sample = {\n","        'conversation_ID': sample['conversation_ID'],\n","        'utterance_ID': sample['utterance_ID'],\n","        'paragraph': paragraph,\n","        'emotion':\n","        {\n","            'property': sample['emotion']['property'],\n","            'casual': sample['emotion']['casual']\n","        },\n","        'text_pool': text_pool,\n","        'speaker_pool': sample['speaker_pool']\n","    }\n","    return new_sample\n","def augment_text(text):\n","    return wordnet_aug.augment(text)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aM3AG7bE-fSY","executionInfo":{"status":"ok","timestamp":1705470901745,"user_tz":-420,"elapsed":1043,"user":{"displayName":"Duc Le Ngoc","userId":"10890292381411576176"}},"outputId":"b03ecf70-2c4d-487b-8c07-35ad62ff6c09"},"outputs":[{"output_type":"stream","name":"stdout","text":["78\n"]}],"source":["data = read_json(\"/content/drive/MyDrive/HaruLab/Dataset/ECF 2.0/train/Subtask_1_train.json\")\n","# count_dis(data)\n","new_data= convertraw_to_train(data)\n","# write_json(\"../data/ECF 2.0/train/train.json\", new_data)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOgbB1en-fSY","executionInfo":{"status":"ok","timestamp":1705471224005,"user_tz":-420,"elapsed":320029,"user":{"displayName":"Duc Le Ngoc","userId":"10890292381411576176"}},"outputId":"fdc19cc4-afe8-44c6-cf64-64bcc878f9ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["num_augment : {'neutral': 325, 'surprise': 461, 'anger': 686, 'sadness': 1154, 'joy': 0, 'disgust': 1887, 'fear': 1928}\n","dict_keys(['neutral', 'surprise', 'anger', 'sadness', 'joy', 'disgust', 'fear'])\n","dict_keys(['joy', 'neutral', 'surprise', 'anger', 'sadness', 'disgust', 'fear'])\n","neutral\n","surprise\n","anger\n","sadness\n","joy\n","disgust\n","fear\n"]}],"source":["train_data, test_data = augment_data(new_data)\n","write_json(\"/content/drive/MyDrive/HaruLab/Dataset/ECF 2.0/train/augment_train.json\", train_data)\n","write_json(\"/content/drive/MyDrive/HaruLab/Dataset/ECF 2.0/train/augment_valid.json\", test_data)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ms3EtPnI-fSY","executionInfo":{"status":"ok","timestamp":1705471224006,"user_tz":-420,"elapsed":4,"user":{"displayName":"Duc Le Ngoc","userId":"10890292381411576176"}},"outputId":"c8cc3e72-7322-4de6-eb99-fcd97edda91a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'neutral': 1840,\n"," 'disgust': 1840,\n"," 'fear': 1840,\n"," 'joy': 1840,\n"," 'anger': 1840,\n"," 'surprise': 1840,\n"," 'sadness': 1840}"]},"metadata":{},"execution_count":11}],"source":["count(train_data)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDu60Ny5-fSY","executionInfo":{"status":"ok","timestamp":1705471224006,"user_tz":-420,"elapsed":3,"user":{"displayName":"Duc Le Ngoc","userId":"10890292381411576176"}},"outputId":"9b71436c-f837-40c6-880e-b20cc18a9225"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'joy': 461,\n"," 'neutral': 461,\n"," 'surprise': 461,\n"," 'sadness': 461,\n"," 'fear': 461,\n"," 'disgust': 461,\n"," 'anger': 461}"]},"metadata":{},"execution_count":12}],"source":["count(test_data)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lkdN6yZH-fSY","executionInfo":{"status":"ok","timestamp":1705471224904,"user_tz":-420,"elapsed":900,"user":{"displayName":"Duc Le Ngoc","userId":"10890292381411576176"}},"outputId":"b8b57a46-153a-432f-991a-5cc4ddb02960"},"outputs":[{"output_type":"stream","name":"stdout","text":["63\n"]}],"source":["data = read_json(\"/content/drive/MyDrive/HaruLab/Dataset/ECF 2.0/test/Subtask_1_test.json\")\n","new_data = convertraw_to_train(data)\n","write_json(\"/content/drive/MyDrive/HaruLab/Dataset/ECF 2.0/test/test.json\", new_data)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}