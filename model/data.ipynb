{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_padding = 6\n",
    "right_padding = 2\n",
    "\n",
    "punctuation_error = [\n",
    "        \"%\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    with open(path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "def write_json(path, data):\n",
    "    with open(path, 'w') as file:\n",
    "        # Write the Python data structure as JSON to the file\n",
    "        json.dump(data, file, indent=2)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Create a string of all punctuation characters\n",
    "    punctuation_chars = string.punctuation\n",
    "\n",
    "    # Remove punctuation at the beginning and end of the string\n",
    "    cleaned_text = text.strip(punctuation_chars).strip().strip(punctuation_chars).strip().strip(punctuation_chars).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "def span(text, sub_text, s_ind=0):\n",
    "    sub_text = remove_punctuation(sub_text)\n",
    "    new_text = text.split()\n",
    "    if text == sub_text:\n",
    "        ss = s_ind\n",
    "        se = s_ind+len(new_text)\n",
    "        return ss, se, text\n",
    "    new_sub_text = sub_text\n",
    "    new_sub_text = new_sub_text.split()\n",
    "    check = True\n",
    "    for i in range(len(new_text)):\n",
    "        temp = i\n",
    "        for j in range(len(new_sub_text)):\n",
    "            if new_sub_text[j] != new_text[i + j]:\n",
    "                if j == len(new_sub_text)-1:\n",
    "                    for punctuation in punctuation_error:\n",
    "\n",
    "                        if new_sub_text[j]+punctuation == new_text[i + j]:\n",
    "                            # print(new_sub_text)\n",
    "                            new_sub_text[j] = new_sub_text[j]+punctuation \n",
    "                            ss = i\n",
    "                            se = i+j\n",
    "                            print(new_sub_text)\n",
    "                            return s_ind+ss, s_ind+se+1, ' '.join(new_sub_text)\n",
    "                check = False\n",
    "                break\n",
    "            else:\n",
    "                temp = i + j\n",
    "                check = True\n",
    "        if check == True:\n",
    "            ss = i\n",
    "            se = temp\n",
    "            return s_ind+ss, s_ind+se+1, sub_text\n",
    "\n",
    "def convertraw_to_train(data):\n",
    "    new_data = []\n",
    "    max_len = 0\n",
    "    for i in range(len(data)):\n",
    "        conversation_ID = data[i]['conversation_ID']\n",
    "        conversation = data[i]['conversation']\n",
    "\n",
    "        utterance_ID_pool = []\n",
    "        text_pool = []\n",
    "        speaker_pool = []\n",
    "        emotion_pool = {}\n",
    "        for j in range(len(conversation)):\n",
    "            utterance_ID_pool.append(conversation[j]['utterance_ID'])\n",
    "            text_pool.append(conversation[j]['text'])\n",
    "            if max_len < len(conversation[j]['text'].split()):\n",
    "                max_len = len(conversation[j]['text'].split())\n",
    "            speaker_pool.append(conversation[j]['speaker'])\n",
    "            emotion_pool[j+1] = {}\n",
    "            emotion_pool[j+1][\"property\"] = {}\n",
    "            emotion_pool[j+1][\"casual\"] = {}\n",
    "            if \"emotion\" in conversation[j].keys():\n",
    "                emotion_pool[j+1][\"property\"] = conversation[j]['emotion']\n",
    "\n",
    "        \n",
    "        if \"emotion-cause_pairs\" in data[i].keys():\n",
    "            cause_pairs_pool = []\n",
    "            cause_pairs = data[i]['emotion-cause_pairs']\n",
    "            for j in range(len(cause_pairs)):\n",
    "                emotion_utter = cause_pairs[j][0]\n",
    "                casual_utter = cause_pairs[j][1]\n",
    "                utter, emotion = emotion_utter.split(\"_\")\n",
    "                casual_utter, casual_text = casual_utter.split(\"_\")\n",
    "                start, end, _ = span(text_pool[int(casual_utter)-1], casual_text)\n",
    "                emotion_pool[int(utter)][\"casual\"][int(casual_utter)] = {\n",
    "                    \"casual_text\": casual_text,\n",
    "                    \"start\": start,\n",
    "                    \"end\": end\n",
    "                }\n",
    "\n",
    "\n",
    "        for j in range(len(conversation)):\n",
    "            start_ids = max(0, j-left_padding)\n",
    "            end_ids = min(j+right_padding+1, len(conversation))\n",
    "            for k in range(start_ids, end_ids):\n",
    "                if k not in emotion_pool[j+1][\"casual\"].keys():\n",
    "                    emotion_pool[j+1][\"casual\"][k] = {\n",
    "                        \"casual_text\": \"\",\n",
    "                        \"start\": 0,\n",
    "                        \"end\": 0\n",
    "                    }\n",
    "\n",
    "            emotion_pool[j+1][\"casual\"] = dict(sorted(emotion_pool[j+1][\"casual\"].items()))\n",
    "            start_ids = max(0, j-left_padding)\n",
    "            end_ids = min(j+right_padding+1, len(conversation))\n",
    "            paragraph = \" \".join(text_pool[start_ids: end_ids])\n",
    "            \n",
    "            new_data.append({\n",
    "                \"conversation_ID\" : conversation_ID,\n",
    "                \"utterance_ID\" : utterance_ID_pool[j],\n",
    "                \"paragraph\": paragraph,\n",
    "                \"emotion\": emotion_pool[j+1],\n",
    "                \"text_pool\": text_pool[start_ids: end_ids],\n",
    "                \"speaker_pool\": speaker_pool[start_ids: end_ids],\n",
    "            })\n",
    "    print(max_len)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "data = read_json(\"..\\data\\ECF 2.0\\\\train\\Subtask_1_train.json\")\n",
    "new_data= convertraw_to_train(data)\n",
    "write_json(\"..\\data\\ECF 2.0\\\\train\\\\train.json\", new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "data = read_json(\"..\\data\\ECF 2.0\\\\trial\\Subtask_1_trial.json\")\n",
    "new_data= convertraw_to_train(data)\n",
    "write_json(\"..\\data\\ECF 2.0\\\\trial\\\\trial.json\", new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
