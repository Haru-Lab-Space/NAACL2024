{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random\n",
    "from textattack.augmentation import WordNetAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    with open(path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "def write_json(path, data):\n",
    "    with open(path, 'w') as file:\n",
    "        # Write the Python data structure as JSON to the file\n",
    "        json.dump(data, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "config_path = \"config.json\"\n",
    "config = read_json(config_path)\n",
    "left_padding = config[\"left_padding\"]\n",
    "right_padding = config[\"right_padding\"]\n",
    "\n",
    "punctuation_error = [\n",
    "        \"%\"\n",
    "    ]\n",
    "wordnet_aug = WordNetAugmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Create a string of all punctuation characters\n",
    "    punctuation_chars = string.punctuation\n",
    "\n",
    "    # Remove punctuation at the beginning and end of the string\n",
    "    cleaned_text = text.strip(punctuation_chars).strip().strip(punctuation_chars).strip().strip(punctuation_chars).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "def span(text, sub_text, s_ind=0):\n",
    "    sub_text = remove_punctuation(sub_text)\n",
    "    new_text = text.split()\n",
    "    if text == sub_text:\n",
    "        ss = s_ind\n",
    "        se = s_ind+len(new_text)\n",
    "        return ss, se, text\n",
    "    new_sub_text = sub_text\n",
    "    new_sub_text = new_sub_text.split()\n",
    "    check = True\n",
    "    for i in range(len(new_text)):\n",
    "        temp = i\n",
    "        for j in range(len(new_sub_text)):\n",
    "            if new_sub_text[j] != new_text[i + j]:\n",
    "                if j == len(new_sub_text)-1:\n",
    "                    for punctuation in punctuation_error:\n",
    "\n",
    "                        if new_sub_text[j]+punctuation == new_text[i + j]:\n",
    "                            # print(new_sub_text)\n",
    "                            new_sub_text[j] = new_sub_text[j]+punctuation \n",
    "                            ss = i\n",
    "                            se = i+j\n",
    "                            print(new_sub_text)\n",
    "                            return s_ind+ss, s_ind+se+1, ' '.join(new_sub_text)\n",
    "                check = False\n",
    "                break\n",
    "            else:\n",
    "                temp = i + j\n",
    "                check = True\n",
    "        if check == True:\n",
    "            ss = i\n",
    "            se = temp\n",
    "            return s_ind+ss, s_ind+se+1, sub_text\n",
    "\n",
    "def convertraw_to_train(data):\n",
    "    new_data = []\n",
    "    max_len = 0\n",
    "    for i in range(len(data)):\n",
    "        conversation_ID = data[i]['conversation_ID']\n",
    "        conversation = data[i]['conversation']\n",
    "\n",
    "        utterance_ID_pool = []\n",
    "        text_pool = []\n",
    "        speaker_pool = []\n",
    "        emotion_pool = {}\n",
    "        for j in range(len(conversation)):\n",
    "            utterance_ID_pool.append(conversation[j]['utterance_ID'])\n",
    "            text_pool.append(conversation[j]['text'])\n",
    "            if max_len < len(conversation[j]['text'].split()):\n",
    "                max_len = len(conversation[j]['text'].split())\n",
    "            speaker_pool.append(conversation[j]['speaker'])\n",
    "            emotion_pool[j+1] = {}\n",
    "            emotion_pool[j+1][\"property\"] = {}\n",
    "            emotion_pool[j+1][\"casual\"] = {}\n",
    "            if \"emotion\" in conversation[j].keys():\n",
    "                emotion_pool[j+1][\"property\"] = conversation[j]['emotion']\n",
    "\n",
    "        \n",
    "        if \"emotion-cause_pairs\" in data[i].keys():\n",
    "            cause_pairs_pool = []\n",
    "            cause_pairs = data[i]['emotion-cause_pairs']\n",
    "            for j in range(len(cause_pairs)):\n",
    "                emotion_utter = cause_pairs[j][0]\n",
    "                casual_utter = cause_pairs[j][1]\n",
    "                utter, emotion = emotion_utter.split(\"_\")\n",
    "                casual_utter, casual_text = casual_utter.split(\"_\")\n",
    "                # if conversation_ID == 1049:\n",
    "                #     print(\"cause_pairs[j][0]:\" +str(cause_pairs[j][0]))\n",
    "                #     print(\"cause_pairs[j][1]:\" +str(cause_pairs[j][1]))\n",
    "                #     print(\"utter:\" +str(utter))\n",
    "                #     print(\"casual_utter:\" +str(casual_utter))\n",
    "                #     print(\"int(utter) - left_padding :\" +str(int(utter) - left_padding))\n",
    "                #     print(\"int(utter) + right_padding :\" +str(int(utter) + right_padding))\n",
    "                if int(utter) - left_padding > int(casual_utter):\n",
    "                    continue\n",
    "                if int(utter) + right_padding < int(casual_utter):\n",
    "                    continue\n",
    "                start, end, _ = span(text_pool[int(casual_utter)-1], casual_text)\n",
    "                emotion_pool[int(utter)][\"casual\"][int(casual_utter)] = {\n",
    "                    \"casual_text\": casual_text,\n",
    "                    \"start\": start,\n",
    "                    \"end\": end\n",
    "                }\n",
    "\n",
    "\n",
    "        for j in range(len(conversation)):\n",
    "            start_ids = max(1, j-left_padding+1)\n",
    "            end_ids = min(j+right_padding+2, len(conversation)+1)\n",
    "            for k in range(start_ids, end_ids):\n",
    "                if k not in emotion_pool[j+1][\"casual\"].keys():\n",
    "                    emotion_pool[j+1][\"casual\"][k] = {\n",
    "                        \"casual_text\": \"\",\n",
    "                        \"start\": 0,\n",
    "                        \"end\": 0\n",
    "                    }\n",
    "\n",
    "            emotion_pool[j+1][\"casual\"] = dict(sorted(emotion_pool[j+1][\"casual\"].items()))\n",
    "\n",
    "            pool = []\n",
    "            for k in emotion_pool[j+1][\"casual\"].keys():\n",
    "                pool.append({\n",
    "                    \"utter_ID\": k,\n",
    "                    \"casual_text\": emotion_pool[j+1][\"casual\"][k][\"casual_text\"],\n",
    "                    \"start\": emotion_pool[j+1][\"casual\"][k][\"start\"],\n",
    "                    \"end\": emotion_pool[j+1][\"casual\"][k][\"end\"]\n",
    "                })\n",
    "            emotion_pool[j+1][\"casual\"] = pool\n",
    "\n",
    "            start_ids = max(0, j-left_padding)\n",
    "            end_ids = min(j+right_padding+1, len(conversation))\n",
    "            paragraph = \" \".join(text_pool[start_ids: end_ids])\n",
    "            \n",
    "            new_data.append({\n",
    "                \"conversation_ID\" : conversation_ID,\n",
    "                \"utterance_ID\" : utterance_ID_pool[j],\n",
    "                \"paragraph\": paragraph,\n",
    "                \"emotion\": emotion_pool[j+1],\n",
    "                \"text_pool\": text_pool[start_ids: end_ids],\n",
    "                \"speaker_pool\": speaker_pool[start_ids: end_ids],\n",
    "            })\n",
    "    print(max_len)\n",
    "    return new_data\n",
    "\n",
    "def count(data):\n",
    "    dictionary = {}\n",
    "\n",
    "    for item in data:\n",
    "        # print( dictionary.keys())\n",
    "        emotion = item['emotion']['property']\n",
    "        if emotion in dictionary.keys():\n",
    "            # print(emotion)\n",
    "            dictionary[emotion] +=1\n",
    "        else:\n",
    "            # print(emotion)\n",
    "            dictionary[emotion] =1\n",
    "    return dictionary\n",
    "\n",
    "def count_dict(data):\n",
    "    dictionary = {}\n",
    "\n",
    "    for item in data.keys():\n",
    "        dictionary[item] = len(data[item])\n",
    "    return dictionary\n",
    "\n",
    "def count_dis(data):\n",
    "    dict_dis = {}\n",
    "    num = 0\n",
    "    for i in range(len(data)):\n",
    "        emotion_casual_pairs = data[i]['emotion-cause_pairs']\n",
    "        for j in range(len(emotion_casual_pairs)):\n",
    "            emotion_utter = emotion_casual_pairs[j][0]\n",
    "            casual_utter = emotion_casual_pairs[j][1]\n",
    "            utter, emotion = emotion_utter.split(\"_\")\n",
    "            casual_utter, casual_text = casual_utter.split(\"_\")\n",
    "            utter = int(utter)\n",
    "            casual_utter = int(casual_utter)\n",
    "            num += 1\n",
    "            if utter - casual_utter not in dict_dis.keys():\n",
    "                dict_dis[utter - casual_utter] = 1\n",
    "            else:\n",
    "                dict_dis[utter - casual_utter] += 1\n",
    "\n",
    "    dict_dis = dict(sorted(dict_dis.items(), key=lambda item: item[1]))\n",
    "    dict_r = {}\n",
    "    for j in dict_dis.keys():\n",
    "        dict_r[j] = dict_dis[j]/num\n",
    "    \n",
    "    return dict_dis, dict_r\n",
    "def augment_data(data):\n",
    "    dict = {}\n",
    "    for item in data:\n",
    "        property = item['emotion']['property']\n",
    "        if property not in dict:\n",
    "            dict[property] = [item]\n",
    "        else:\n",
    "            dict[property].append(item)\n",
    "\n",
    "    num_elements_to_choose = len(dict['neutral']) // 3\n",
    "\n",
    "    # Randomly choose one-third of the elements\n",
    "    dict['neutral'] = random.sample(dict['neutral'], num_elements_to_choose)\n",
    "\n",
    "    count_appear = count_dict(dict)\n",
    "    max_appear = max(count_appear.values())\n",
    "\n",
    "    num_augment = {}\n",
    "    for emotion in count_appear.keys():\n",
    "        num_augment[emotion] = max_appear - count_appear[emotion] \n",
    "    print(\"num_augment : \"+str(num_augment))\n",
    "\n",
    "    augment_dict = {\n",
    "        \"joy\": [],\n",
    "        \"neutral\": []\n",
    "    }\n",
    "    \n",
    "    for emotion in dict.keys():\n",
    "        for i in range(num_augment[emotion]):\n",
    "            index = i % len(dict[emotion])\n",
    "            sample = augment_sample(dict[emotion][index])\n",
    "            if emotion not in augment_dict:\n",
    "                augment_dict[emotion]=[sample]\n",
    "            else:\n",
    "                augment_dict[emotion].append(sample)\n",
    "            # print(dict[emotion][index])\n",
    "            # print(sample)\n",
    "            # print(\"====\")\n",
    "\n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    print(dict.keys())\n",
    "    print(augment_dict.keys())\n",
    "    for emotion in dict.keys():\n",
    "        print(emotion)\n",
    "        list = dict[emotion] + augment_dict[emotion]\n",
    "        n_train = int(0.8*len(list))\n",
    "        train_dict[emotion] = list[:n_train]\n",
    "        test_dict[emotion] = list[n_train:]\n",
    "        train_data.extend(train_dict[emotion])\n",
    "        test_data.extend(test_dict[emotion])\n",
    "    # new_data = neutral_data[0:int(len(neutral_data)/3)] + other_data\n",
    "\n",
    "\n",
    "\n",
    "    # random.shuffle(train_data)\n",
    "    # random.shuffle(test_data)\n",
    "    return train_data + test_data\n",
    "def augment_sample(sample):\n",
    "    emotion = sample[\"emotion\"]\n",
    "    text_pool = sample[\"text_pool\"]\n",
    "\n",
    "    for i in range(len(text_pool)):\n",
    "        if emotion['casual'][i][\"casual_text\"] == \"\":\n",
    "            text_pool[i] = augment_text(text_pool[i])[0]\n",
    "            # print(augment_text(text_pool[i]))\n",
    "\n",
    "    gpt_paragraph = \"\"\n",
    "    paragraph = \"\"\n",
    "    for i in range(len(text_pool)):\n",
    "        # print(new_text_pool[i])\n",
    "        paragraph += \" \" + text_pool[i]\n",
    "        gpt_paragraph += \" \\\" \" + text_pool[i] + \" \\\"\"\n",
    "\n",
    "    new_sample = {\n",
    "        'conversation_ID': sample['conversation_ID'],\n",
    "        'utterance_ID': sample['utterance_ID'],\n",
    "        'paragraph': paragraph,\n",
    "        'gpt_paragraph':  gpt_paragraph,\n",
    "        'emotion': \n",
    "        {\n",
    "            'property': sample['emotion']['property'],\n",
    "            'casual': sample['emotion']['casual']\n",
    "        },\n",
    "        'text_pool': text_pool,\n",
    "        'speaker_pool': sample['speaker_pool']\n",
    "    }\n",
    "    return new_sample\n",
    "def augment_text(text):\n",
    "    return wordnet_aug.augment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "data = read_json(\"../data/ECF 2.0/train/Subtask_1_train.json\")\n",
    "# count_dis(data)\n",
    "new_data= convertraw_to_train(data)\n",
    "# write_json(\"../data/ECF 2.0/train/train.json\", new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_augment : {'neutral': 265, 'surprise': 334, 'anger': 576, 'sadness': 961, 'joy': 0, 'disgust': 1512, 'fear': 1533}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m n_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(new_data))\n\u001b[0;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43maugment_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_data \u001b[38;5;241m=\u001b[39m new_data[n_train:]\n\u001b[1;32m      4\u001b[0m write_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/ECF 2.0/train/augment_train.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_data)\n",
      "Cell \u001b[0;32mIn[4], line 207\u001b[0m, in \u001b[0;36maugment_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_augment[emotion]):\n\u001b[1;32m    206\u001b[0m     index \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mdict\u001b[39m[emotion])\n\u001b[0;32m--> 207\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43maugment_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43memotion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m emotion \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m augment_dict:\n\u001b[1;32m    209\u001b[0m         augment_dict[emotion]\u001b[38;5;241m=\u001b[39m[sample]\n",
      "Cell \u001b[0;32mIn[4], line 243\u001b[0m, in \u001b[0;36maugment_sample\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(text_pool)):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m emotion[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasual\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcasual_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 243\u001b[0m         text_pool[i] \u001b[38;5;241m=\u001b[39m \u001b[43maugment_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_pool\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# print(augment_text(text_pool[i]))\u001b[39;00m\n\u001b[1;32m    246\u001b[0m gpt_paragraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 268\u001b[0m, in \u001b[0;36maugment_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maugment_text\u001b[39m(text):\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwordnet_aug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/textattack/augmentation/augmenter.py:125\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    122\u001b[0m words_swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodified_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m words_swapped \u001b[38;5;241m<\u001b[39m num_words_to_swap:\n\u001b[0;32m--> 125\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transformation_constraints\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Get rid of transformations we already have\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    131\u001b[0m         t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transformed_texts \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_transformed_texts\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/textattack/transformations/transformation.py:57\u001b[0m, in \u001b[0;36mTransformation.__call__\u001b[0;34m(self, current_text, pre_transformation_constraints, indices_to_modify, shifted_idxs, return_indices)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_indices:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices_to_modify\n\u001b[0;32m---> 57\u001b[0m transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_to_modify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m transformed_texts:\n\u001b[1;32m     59\u001b[0m     text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_transformation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/textattack/transformations/word_swaps/word_swap.py:51\u001b[0m, in \u001b[0;36mWordSwap._get_transformations\u001b[0;34m(self, current_text, indices_to_modify)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;241m==\u001b[39m word_to_replace:\n\u001b[1;32m     50\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m         transformed_texts_idx\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcurrent_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_word_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     52\u001b[0m     transformed_texts\u001b[38;5;241m.\u001b[39mextend(transformed_texts_idx)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformed_texts\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/textattack/shared/attacked_text.py:360\u001b[0m, in \u001b[0;36mAttackedText.replace_word_at_index\u001b[0;34m(self, index, new_word)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_word, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace_word_at_index requires ``str`` new_word, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_word)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m     )\n\u001b[0;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_words_at_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_word\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/textattack/shared/attacked_text.py:351\u001b[0m, in \u001b[0;36mAttackedText.replace_words_at_indices\u001b[0;34m(self, indices, new_words)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot assign word at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    350\u001b[0m     words[i] \u001b[38;5;241m=\u001b[39m new_word\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_new_attacked_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/textattack/shared/attacked_text.py:424\u001b[0m, in \u001b[0;36mAttackedText.generate_new_attacked_text\u001b[0;34m(self, new_words)\u001b[0m\n\u001b[1;32m    422\u001b[0m adv_words \u001b[38;5;241m=\u001b[39m words_from_text(adv_word_seq)\n\u001b[1;32m    423\u001b[0m adv_num_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(adv_words)\n\u001b[0;32m--> 424\u001b[0m num_words_diff \u001b[38;5;241m=\u001b[39m adv_num_words \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mwords_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_word\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Track indices on insertions and deletions.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_words_diff \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;66;03m# Re-calculated modified indices. If words are inserted or deleted,\u001b[39;00m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m# they could change.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion/lib/python3.8/site-packages/textattack/shared/utils/strings.py:39\u001b[0m, in \u001b[0;36mwords_from_text\u001b[0;34m(s, words_to_ignore)\u001b[0m\n\u001b[1;32m     37\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(seg_list)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(s\u001b[38;5;241m.\u001b[39msplit())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_train = int(0.8*len(new_data))\n",
    "train_data = augment_data(new_data[:n_train])\n",
    "test_data = new_data[n_train:]\n",
    "write_json(\"../data/ECF 2.0/train/augment_train.json\", train_data)\n",
    "write_json(\"../data/ECF 2.0/train/augment_valid.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'surprise': 1840,\n",
       " 'neutral': 1840,\n",
       " 'joy': 1840,\n",
       " 'sadness': 1840,\n",
       " 'fear': 1840,\n",
       " 'anger': 1840,\n",
       " 'disgust': 1840}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 461,\n",
       " 'fear': 461,\n",
       " 'neutral': 461,\n",
       " 'disgust': 461,\n",
       " 'surprise': 461,\n",
       " 'sadness': 461,\n",
       " 'joy': 461}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "data = read_json(\"../data/ECF 2.0/test/Subtask_1_test.json\")\n",
    "new_data = convertraw_to_train(data)\n",
    "write_json(\"../data/ECF 2.0/test/test.json\", new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
