{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxility Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "# Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video, read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Model\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer, BertModel, BertTokenizer, BertGenerationDecoder, RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Training parameter\n",
    "from torch.optim import Adam\n",
    "# Training process\n",
    "from tqdm import tqdm\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import sys\n",
    "from sklearn.metrics import classification_report\n",
    "from focal_loss.focal_loss import FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "def write_json(path, data):\n",
    "    with open(path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, config_path, data, option=\"train\"):\n",
    "        super(ConversationDataset, self).__init__()\n",
    "        self.build(config_path)\n",
    "        self.data = data\n",
    "        self.option = option\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.id2label = config[\"id2label\"]\n",
    "        self.tokenizer_name = config[\"tokenizer_name\"]\n",
    "        self.padding = config[\"padding\"]\n",
    "        self.max_length = config[\"max_length\"]\n",
    "        self.candidate_num = config[\"candidate num\"]\n",
    "        if self.tokenizer_name == 'bert-base-uncased':\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        elif self.tokenizer_name == 'roberta-base':\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained(self.tokenizer_name)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        conversation_ID = self.data[index][\"conversation_ID\"]\n",
    "        # print(self.data[index])\n",
    "        paragraph = self.tokenizer(self.data[index][\"paragraph\"], padding=self.padding, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        paragraph = torch.squeeze(paragraph['input_ids'])\n",
    "        main_text = self.tokenizer(self.data[index][\"main_text\"], padding=self.padding, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        main_text = torch.squeeze(main_text['input_ids'])\n",
    "        utterance_ID = self.data[index][\"utterance_ID\"]\n",
    "\n",
    "        casual_pool = torch.squeeze(self.tokenizer(self.data[index][\"casual_pool\"], padding=self.padding, max_length=self.max_length, return_tensors=\"pt\")['input_ids'])\n",
    "        # print(\"casual_pool: \"+str(casual_pool))\n",
    "        casual_span_pool = torch.FloatTensor(self.data[index][\"casual_span_pool\"])\n",
    "\n",
    "        if self.option == \"train\":\n",
    "            label = self.label2id[self.data[index][\"emotion\"]]\n",
    "            emotion_label = [0] * len(self.id2label)\n",
    "            emotion_label[label] = 1\n",
    "            # print(emotion_label)\n",
    "            emotion_label = torch.FloatTensor(emotion_label)\n",
    "            span_label = torch.FloatTensor([self.data[index][\"span_label\"]])\n",
    "            return {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"paragraph\": paragraph,\n",
    "                \"main_text\": main_text,\n",
    "                \"utterance_ID\": utterance_ID,\n",
    "                \"casual_pool\": casual_pool,\n",
    "                \"casual_span_pool\": casual_span_pool,\n",
    "                \"emotion_label\": emotion_label,\n",
    "                \"span_label\": span_label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"paragraph\": paragraph,\n",
    "                \"main_text\": main_text,\n",
    "                \"utterance_ID\": utterance_ID,\n",
    "                \"casual_pool\": casual_pool,\n",
    "                \"casual_span_pool\": casual_span_pool\n",
    "            }\n",
    "    \n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.layers_to_freeze = []\n",
    "        self.history = {\n",
    "            \"train loss\": [],\n",
    "            \"valid loss\": [],\n",
    "            \"emotion_correct\": [],\n",
    "            \"span_correct\": []\n",
    "        }\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        print(config_path)\n",
    "        config = read_json(config_path)\n",
    "        self.patience = config['patience']\n",
    "        self.min_delta = config['min_delta']\n",
    "        self.fn_loss_name = config['loss_fn_name']\n",
    "        self.batch_size_config = config['batch_size']\n",
    "        self.device = config['device']\n",
    "        self.save_checkpoint_dir = config['save_checkpoint_dir']\n",
    "        self.save_history_dir = config['save_history_dir']\n",
    "        self.emotion_head_layers = config['emotion_head_layers']\n",
    "        self.casual_emotion_layers = config['casual_emotion_layers']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.main_layers = config['main_layers']\n",
    "        self.label2id = config['label2id']\n",
    "        self.alpha_focal_loss = config['alpha_focal_loss']\n",
    "        self.gamma_focal_loss = config['gamma_focal_loss']\n",
    "        self.target_names = self.label2id.keys()\n",
    "        self.early_stopper = EarlyStopper(self.patience, self.min_delta)\n",
    "    def emotion_head_training(self):\n",
    "        self.layers_to_freeze = self.casual_emotion_layers + self.main_layers\n",
    "        # self.fn_loss = nn.CrossEntropyLoss()\n",
    "        self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n",
    "    def emotion_main_training(self):\n",
    "        self.layers_to_freeze = self.casual_emotion_layers\n",
    "        # self.fn_loss = nn.CrossEntropyLoss()\n",
    "        self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss))\n",
    "        \n",
    "    def casual_emotion_training(self):\n",
    "        self.layers_to_freeze = self.emotion_head_layers + self.main_layers\n",
    "        self.fn_loss = nn.MSELoss()\n",
    "\n",
    "    def full_training(self):\n",
    "        self.layers_to_freeze = []\n",
    "    def freeze(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(layer in name for layer in self.layers_to_freeze):\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        return model\n",
    "    def train(self, model, optimizer, train_dataset, valid_dataset, epochs, option=\"emotion_head\", batch_size=None):\n",
    "        self.optimizer = optimizer\n",
    "        if option == \"emotion_head\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_head_training()\n",
    "        elif option == \"emotion_main\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_main_training()\n",
    "        elif option == \"casual_head\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.casual_emotion_training()\n",
    "\n",
    "        if batch_size == None:\n",
    "            self.batch_size = self.batch_size_config\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        print(\"device: \"+str(self.device))\n",
    "        print(\"batch_size: \"+str(self.batch_size))\n",
    "        print(\"gamma_focal_loss: \"+str(self.gamma_focal_loss))\n",
    "        print(\"alpha_focal_loss: \"+str(self.alpha_focal_loss))\n",
    "\n",
    "\n",
    "        self.early_stopper.reset()\n",
    "        self.model = self.freeze(model)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.train_dataloader = DataLoader(dataset=train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        self.valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "        best_val_loss = np.inf\n",
    "        self.epochs = epochs\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self._train_epoch(epoch, option)\n",
    "            val_loss, emotion_correct, span_correct, emotion_label_list, emotion_pred_list = self._val_epoch(epoch, option)\n",
    "            \n",
    "            self.history[\"train loss\"].append(train_loss)\n",
    "            self.history[\"valid loss\"].append(val_loss)\n",
    "            self.history[\"emotion_correct\"].append(emotion_correct)\n",
    "            self.history[\"span_correct\"].append(span_correct)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.save_checkpoint(epoch, option)\n",
    "                print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f},  Valid loss {val_loss:.4f},  emotion_correct {emotion_correct*100:.4f}%,  span_correct {span_correct*100:.4f}%\")\n",
    "            \n",
    "                print(classification_report(emotion_label_list, emotion_pred_list, target_names=self.target_names))\n",
    "            if self.early_stopper.early_stop(val_loss):\n",
    "                print(\"Early stop at epoch: \"+str(epoch) + \" with valid loss: \"+str(val_loss))\n",
    "                break\n",
    "        write_json(self.save_history_dir + \"/\" + option + \"_history.json\", self.history) \n",
    "    def _train_epoch(self, epoch, option):\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "        logger_message = f'Training epoch {epoch}/{self.epochs}'\n",
    "\n",
    "        progress_bar = tqdm(self.train_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        for batch, data in enumerate(progress_bar):\n",
    "            # conversation_id = data[\"conversation_ID\"]\n",
    "            paragraph = data[\"paragraph\"].to(self.device)\n",
    "            main_text = data[\"main_text\"].to(self.device)\n",
    "            # utter_id = data[\"utterance_ID\"]\n",
    "            casual_pool = data[\"casual_pool\"].to(self.device)\n",
    "            # casual_span_pool = data[\"casual_span_pool\"]\n",
    "            emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "            span_label = data[\"span_label\"].to(self.device)\n",
    "            # print(\"conversation_id:\"+str(conversation_id.shape))\n",
    "            # print(\"paragraph:\"+str(paragraph.shape))\n",
    "            # print(\"casual_pool:\"+str(casual_pool.shape))\n",
    "            # print(\"emotion_label:\"+str(emotion_label.shape))\n",
    "            # print(\"span_label:\"+str(span_label.shape))\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            emotion_pred, span_pred = self.model(main_text, paragraph, casual_pool)\n",
    "            \n",
    "            emotion_label = torch.argmax(emotion_label, dim=-1)\n",
    "            if option == \"emotion_head\":\n",
    "                # print(emotion_pred)\n",
    "                # print(emotion_pred.shape)\n",
    "                loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "\n",
    "            elif option == \"emotion_main\":\n",
    "                loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "            elif option == \"casual_head\":\n",
    "                loss = self.fn_loss(span_pred, span_label)\n",
    "            else:\n",
    "                emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                span_loss = self.fn_loss(span_pred, span_label)\n",
    "                loss = emotion_loss + span_loss\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return train_loss / len(self.train_dataloader)\n",
    "\n",
    "    def _val_epoch(self, epoch, option):\n",
    "        self.model.eval()\n",
    "        valid_loss = 0.0\n",
    "        emotion_correct=0\n",
    "        span_correct=0\n",
    "        emotion_pred_list = []\n",
    "        emotion_label_list = []\n",
    "        logger_message = f'Validation epoch {epoch}/{self.epochs}'\n",
    "        progress_bar = tqdm(self.valid_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(progress_bar):\n",
    "                # conversation_id = data[\"conversation_ID\"]\n",
    "                paragraph = data[\"paragraph\"].to(self.device)\n",
    "                main_text = data[\"main_text\"].to(self.device)\n",
    "                # utter_id = data[\"utterance_ID\"]\n",
    "                casual_pool = data[\"casual_pool\"].to(self.device)\n",
    "                # casual_span_pool = data[\"casual_span_pool\"]\n",
    "                emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "                span_label = data[\"span_label\"].to(self.device)\n",
    "                emotion_pred, span_pred = self.model(main_text, paragraph, casual_pool)\n",
    "                \n",
    "                \n",
    "                emotion_label = torch.argmax(emotion_label, dim=-1)\n",
    "                if option == \"emotion_head\":\n",
    "                    loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                elif option == \"emotion_main\":\n",
    "                    loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                elif option == \"casual_head\":\n",
    "                    loss = self.fn_loss(span_pred, span_label)\n",
    "                else:\n",
    "                    emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                    span_loss = self.fn_loss(span_pred, span_label)\n",
    "                    loss = emotion_loss + span_loss\n",
    "\n",
    "                # emotion_label_ = torch.argmax(emotion_label, dim=-1)\n",
    "                emotion_pred_ = torch.argmax(emotion_pred, dim=-1)\n",
    "                emotion_pred_list.extend(emotion_pred_.cpu().tolist())\n",
    "                emotion_label_list.extend(emotion_label.cpu().tolist())\n",
    "                # print(\"emotion_label_: \"+ str(emotion_label_))\n",
    "                # print(\"emotion_pred_: \"+ str(emotion_pred_))\n",
    "                valid_loss += loss.item()\n",
    "                emotion_correct += (emotion_pred_ == emotion_label).sum().item()\n",
    "                span_correct += (span_pred == span_label).sum().item()\n",
    "        \n",
    "        return valid_loss / len(self.valid_dataloader), emotion_correct / len(self.valid_dataloader) / self.batch_size, span_correct / len(self.valid_dataloader) / self.batch_size, emotion_label_list, emotion_pred_list\n",
    "        \n",
    "    def save_checkpoint(self, epoch, option):\n",
    "        checkpoint_path = os.path.join(self.save_checkpoint_dir, option + '/checkpoint_{}.pth'.format(epoch))\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(Prediction, self).__init__()\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.id2label = config[\"id2label\"]\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.submit_path = config[\"submit path\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.top_k = config[\"top_k\"]\n",
    "        \n",
    "    def predict(self, model, raw_dataset, dataset):\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "        pred_list = []\n",
    "        \n",
    "        self.test_dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        logger_message = f'Predict '\n",
    "        progress_bar = tqdm(self.test_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(progress_bar):\n",
    "                # print(\"conversation_ID \"+str(data[\"conversation_ID\"].shape))\n",
    "                # print(\"paragraph \"+str(data[\"paragraph\"].shape))\n",
    "                # print(\"utterance_ID \"+str(data[\"utterance_ID\"].shape))\n",
    "                # print(\"casual_pool \"+str(data[\"casual_pool\"].shape))\n",
    "                # print(\"casual_span_pool \"+str(data[\"casual_span_pool\"].shape))\n",
    "                conversation_id = data[\"conversation_ID\"]\n",
    "                paragraph = data[\"paragraph\"].to(self.device)\n",
    "                main_text = data[\"main_text\"].to(self.device)\n",
    "                utter_id = data[\"utterance_ID\"]\n",
    "                casual_pool = data[\"casual_pool\"].to(self.device)\n",
    "                casual_span_pool = data[\"casual_span_pool\"]\n",
    "                # emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "                # span_label = data[\"span_label\"].to(self.device)\n",
    "                emotion_pred, span_pred = self.model(main_text, paragraph, casual_pool)\n",
    "                \n",
    "                pred = self.convertpred2list(conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool)\n",
    "                pred_list.extend(pred)\n",
    "        \n",
    "        submit = self.convertpred2dict(raw_dataset, pred_list)\n",
    "        # self.save_json(\"data/predict/convertpred2dict.json\", submit)\n",
    "        submit = self.get_top(submit, self.top_k)\n",
    "        # self.save_json(\"data/predict/get_top.json\", submit)\n",
    "        submit = self.convertdict2submit(submit)\n",
    "        # print(submit)\n",
    "        self.save_json(self.submit_path, submit)\n",
    "    def convertpred2list(self, conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool):\n",
    "        pred_list = []\n",
    "        for i in range(len(emotion_pred)):\n",
    "            emotion = self.id2label[str(torch.argmax(emotion_pred[i]).item())]\n",
    "            emotion_string = str(utter_id[i].item()) + \"_\" + emotion\n",
    "            if emotion != \"neutral\":\n",
    "                # \n",
    "                span = span_pred[i].item()\n",
    "                # print(\"casual_span_pool: \"+str(casual_span_pool[i]))\n",
    "                # if span >= self.predict_threshold:\n",
    "                if casual_span_pool[i][0].item() == 0:\n",
    "                    break\n",
    "                if casual_span_pool[i][1].item() == casual_span_pool[i][2].item():\n",
    "                    break\n",
    "                span_string = str(int(casual_span_pool[i][0].item())) + \"_\" + str(int(casual_span_pool[i][1].item())) + \"_\" + str(int(casual_span_pool[i][2].item()))\n",
    "                pred_dict = {}\n",
    "                pred_dict[\"conversation_ID\"] = conversation_id[i].item()\n",
    "                pred_dict[\"emotion-cause_pairs\"] = [emotion_string, span_string]\n",
    "                pred_dict[\"logits\"] = span\n",
    "                # print(\"pred_dict: \"+str(pred_dict))\n",
    "                pred_list.append(pred_dict)\n",
    "        # print(\"list: \"+str(pred_list))\n",
    "        return pred_list\n",
    "    def convertpred2dict(self, raw_dataset, pred_list):\n",
    "        \n",
    "        dataset = self.convertlist2dict(raw_dataset)\n",
    "        # print(dataset)\n",
    "        # dict = \n",
    "        for i in range(len(pred_list)):\n",
    "            pred = pred_list[i]\n",
    "            # print(\"pred: \"+str(pred))\n",
    "            conversation_id = pred[\"conversation_ID\"]\n",
    "            # print(\"conversation_id: \"+str(conversation_id))\n",
    "            emotion_cause_pairs = pred[\"emotion-cause_pairs\"]\n",
    "            # print(\"emotion_cause_pairs: \"+str(emotion_cause_pairs))\n",
    "            if \"emotion-cause_pairs\" in dataset[conversation_id]:\n",
    "                dataset[conversation_id][\"emotion-cause_pairs\"].append(emotion_cause_pairs)\n",
    "                dataset[conversation_id][\"logits\"].append(pred[\"logits\"])\n",
    "                # print(pred[\"logits\"])\n",
    "            else:\n",
    "                dataset[conversation_id][\"emotion-cause_pairs\"] = []\n",
    "        # print(dataset)\n",
    "        \n",
    "        return dataset\n",
    "    def get_top(self, raw_dataset, top_k):\n",
    "        for conversation_ID in raw_dataset.keys():\n",
    "            # print(\"i : \"+str(conversation_ID))\n",
    "            # print(\"raw_dataset : \"+str(raw_dataset[conversation_ID]))\n",
    "            # print(\"raw_dataset[i]['logits'] : \"+str(raw_dataset[conversation_ID]['logits']))\n",
    "            logits_list = raw_dataset[conversation_ID]['logits']\n",
    "            cause_pairs_list = raw_dataset[conversation_ID]['emotion-cause_pairs']\n",
    "            indices_of_topk = sorted(range(len(logits_list)), key=lambda i: logits_list[i], reverse=True)[:top_k]\n",
    "            new_cause_pairs_list = []\n",
    "            for index in indices_of_topk:\n",
    "                new_cause_pairs_list.append(cause_pairs_list[index])\n",
    "            raw_dataset[conversation_ID]['emotion-cause_pairs'] = new_cause_pairs_list\n",
    "        return raw_dataset\n",
    "    \n",
    "    def convertlist2dict(self, raw_dataset):\n",
    "        dataset = {}\n",
    "        for i in range(len(raw_dataset)):\n",
    "            conversation = raw_dataset[i]\n",
    "            conversation_ID = conversation[\"conversation_ID\"]\n",
    "            # if conversation_ID not in dataset.key\n",
    "            dataset[conversation_ID] = {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"conversation\": conversation['conversation'],\n",
    "                \"emotion-cause_pairs\": [],\n",
    "                \"logits\": []\n",
    "            }\n",
    "        return dataset\n",
    "    def convertdict2submit(self, raw_dataset):\n",
    "        dataset = []\n",
    "        for key in raw_dataset.keys():\n",
    "            # print(\"key: \"+str(key))\n",
    "            sample = raw_dataset[key]\n",
    "            sample = {\n",
    "                \"conversation_ID\": raw_dataset[key]['conversation_ID'],\n",
    "                \"conversation\": raw_dataset[key]['conversation'],\n",
    "                \"emotion-cause_pairs\": raw_dataset[key]['emotion-cause_pairs']\n",
    "            }\n",
    "            # print(\"sample: \"+str(sample))\n",
    "            dataset.append(sample)\n",
    "        return dataset\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    def save_json(self, file_path, data):\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGG(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(TGG, self).__init__()\n",
    "        self.build(config_path)\n",
    "        self.emotion_fc = nn.Linear(self.embedding_dim, self.label_num)\n",
    "        self.casual_feedforward = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        # self.emotion_feedforward = nn.Sequential(\n",
    "        #     nn.Linear(self.embedding_dim, self.hidden_size),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(self.hidden_size, self.embedding_dim),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.2)\n",
    "        # )\n",
    "        self.cat_layer = nn.Linear(2*self.embedding_dim, self.embedding_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "        self.token_size = config[\"token_size\"]\n",
    "        self.main_part_name = config[\"main_part_name\"]\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.label_num = len(self.label2id)\n",
    "        self.device = config[\"device\"]\n",
    "        if self.main_part_name == 'bert-base-uncased':\n",
    "        \n",
    "            main_part = AutoModel.from_pretrained(self.main_part_name)\n",
    "            self.embeddings = main_part.embeddings\n",
    "            self.encoder = main_part.encoder\n",
    "            self.pooler = main_part.pooler\n",
    "        elif self.main_part_name == 'roberta-base':    \n",
    "            main_part = RobertaModel.from_pretrained(self.main_part_name)\n",
    "            self.embeddings = main_part.embeddings\n",
    "            self.encoder = main_part.encoder\n",
    "            self.pooler = main_part.pooler\n",
    "        \n",
    "    def forward(self, main_text, paragraph, candidate):\n",
    "        # batch_size = paragraph.size(0)\n",
    "        # print(\"x: \"+str(x.shape))\n",
    "        # print(\"candidate: \"+str(candidate.shape))\n",
    "        _paragraph = self.embeddings(paragraph)\n",
    "        _paragraph = self.encoder(_paragraph)['last_hidden_state']\n",
    "        _paragraph = self.pooler(_paragraph)\n",
    "        # _main_text= self.embeddings(main_text)\n",
    "        # _main_text = self.encoder(_main_text)['last_hidden_state']\n",
    "        # _main_text = self.pooler(_main_text)\n",
    "\n",
    "        # _x = self.cat_layer(torch.cat([_paragraph, _main_text], dim =-1))\n",
    "        # _e_category = self.emotion_fc(_x)\n",
    "        _e_category = self.emotion_fc(_paragraph)\n",
    "\n",
    "        # print(\"_e_category: \"+str(_e_category.shape))\n",
    "        \n",
    "        _candidate = self.embeddings(candidate)\n",
    "        _candidate = self.encoder(_candidate)['last_hidden_state']\n",
    "        _candidate = self.pooler(_candidate)\n",
    "        # print(\"_candidate: \"+str(_candidate.shape))\n",
    "\n",
    "\n",
    "        _candidate = self.casual_feedforward(_candidate)\n",
    "        # print(\"_candidate: \"+str(_candidate.shape))\n",
    "\n",
    "        _similarity = torch.einsum('ij,ij->i', _paragraph, _candidate)\n",
    "        \n",
    "        # print(\"_similarity: \"+str(_similarity.shape))\n",
    "        return self.softmax(_e_category), torch.sigmoid(_similarity)\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TGG(nn.Module):\n",
    "#     def __init__(self, config_path):\n",
    "#         super(TGG, self).__init__()\n",
    "#         self.build(config_path)\n",
    "#         self.emotion_fc = nn.Linear(self.embedding_dim, self.label_num)\n",
    "#         self.casual_feedforward = nn.Sequential(\n",
    "#             nn.Linear(self.embedding_dim, self.hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(self.hidden_size, self.embedding_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2)\n",
    "#         )\n",
    "#         self.cat_layer = nn.Linear(3*self.embedding_dim, self.embedding_dim)\n",
    "#         self.rnn = nn.LSTM(input_size=self.embedding_dim, hidden_size =self.embedding_dim, num_layers = 2, batch_first=True, bidirectional=True)\n",
    "#         self.softmax = nn.Softmax(dim=-1)\n",
    "#     def build(self, config_path):\n",
    "#         config = self.read_json(config_path)\n",
    "#         self.hidden_size = config[\"hidden_size\"]\n",
    "#         self.embedding_dim = config[\"embedding_dim\"]\n",
    "#         self.token_size = config[\"token_size\"]\n",
    "#         self.main_part_name = config[\"main_part_name\"]\n",
    "#         self.label2id = config[\"label2id\"]\n",
    "#         self.label_num = len(self.label2id)\n",
    "#         self.device = config[\"device\"]\n",
    "#         if self.main_part_name == 'bert-base-uncased':\n",
    "        \n",
    "#             main_part = AutoModel.from_pretrained(self.main_part_name)\n",
    "#             self.embeddings = main_part.embeddings\n",
    "#             self.encoder = main_part.encoder\n",
    "#             self.pooler = main_part.pooler\n",
    "#         elif self.main_part_name == 'roberta-base':    \n",
    "#             main_part = RobertaModel.from_pretrained(self.main_part_name)\n",
    "#             self.embeddings = main_part.embeddings\n",
    "#             self.encoder = main_part.encoder\n",
    "#             self.pooler = main_part.pooler\n",
    "        \n",
    "#     def forward(self, main_text, paragraph, candidate):\n",
    "#         batch_size = paragraph.size(0)\n",
    "#         # print(\"x: \"+str(x.shape))\n",
    "#         # print(\"candidate: \"+str(candidate.shape))\n",
    "#         _paragraph = self.embeddings(paragraph)\n",
    "#         _paragraph = self.encoder(_paragraph)['last_hidden_state']\n",
    "#         _paragraph = self.pooler(_paragraph)\n",
    "\n",
    "#         h0 = torch.randn(4, batch_size, self.embedding_dim)\n",
    "#         c0 = torch.randn(4, batch_size, self.embedding_dim)\n",
    "#         print(\"paragraph: \"+str(paragraph.shape))\n",
    "#         output, (hn, cn) = self.rnn(paragraph, (h0, c0))\n",
    "#         print(\"output: \"+str(output.shape))\n",
    "\n",
    "#         _x = torch.cat([_paragraph, output], dim=-1)\n",
    "#         _x = self.cat_layer(_x)\n",
    "#         # print(\"_x: \"+str(_x.shape))\n",
    "#         _e_category = self.emotion_fc(_x)\n",
    "#         # _e_category = self.emotion_fc(_paragraph)\n",
    "\n",
    "#         # print(\"_e_category: \"+str(_e_category.shape))\n",
    "        \n",
    "#         _candidate = self.embeddings(candidate)\n",
    "#         _candidate = self.encoder(_candidate)['last_hidden_state']\n",
    "#         _candidate = self.pooler(_candidate)\n",
    "#         # print(\"_candidate: \"+str(_candidate.shape))\n",
    "\n",
    "\n",
    "#         _candidate = self.casual_feedforward(_candidate)\n",
    "#         # print(\"_candidate: \"+str(_candidate.shape))\n",
    "\n",
    "#         _similarity = torch.einsum('ij,ij->i', _paragraph, _candidate)\n",
    "        \n",
    "#         # print(\"_similarity: \"+str(_similarity.shape))\n",
    "#         return self.softmax(_e_category), torch.sigmoid(_similarity)\n",
    "#     def read_json(self, file_path):\n",
    "#         with open(file_path, 'r') as json_file:\n",
    "#             data = json.load(json_file)\n",
    "#         return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config.json\"\n",
    "model = TGG(config_path)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "trainer = Trainer(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(\"\\\"\" + name + \"\\\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_json(\"data/ECF 2.0/train/original_emotion_train.json\")\n",
    "n_train = int(0.8 * len(dataset))\n",
    "train_data, valid_data = dataset[:n_train], dataset[n_train:]\n",
    "train_dataset = ConversationDataset(config_path, train_data)\n",
    "valid_dataset = ConversationDataset(config_path, valid_data)\n",
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_head\", batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_json(\"data/ECF 2.0/train/train.json\")\n",
    "n_train = int(0.8 * len(dataset))\n",
    "train_data, valid_data = dataset[:n_train], dataset[n_train:]\n",
    "train_dataset = ConversationDataset(config_path, train_data)\n",
    "valid_dataset = ConversationDataset(config_path, valid_data)\n",
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"casual_head\", batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data = read_json(\"data/ECF 2.0/trial/trial.json\")\n",
    "raw_trial_data = read_json(\"data/ECF 2.0/trial/Subtask_1_trial.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dataset = ConversationDataset(config_path, trial_data, option=\"trial\")\n",
    "predict = Prediction(config_path)\n",
    "predict.predict(model, raw_trial_data, trial_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = read_json(\"data/ECF 2.0/train/train.json\")\n",
    "# n_train = int(0.8 * len(dataset))\n",
    "# train_data, valid_data = dataset[:n_train], dataset[n_train:]\n",
    "# train_dataset = ConversationDataset(config_path, train_data)\n",
    "# valid_dataset = ConversationDataset(config_path, valid_data)\n",
    "# trainer = Trainer(config_path)\n",
    "# trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"casual_emotion\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_data = read_json(\"data/ECF 2.0/trial/trial.json\")\n",
    "# raw_trial_data = read_json(\"data/ECF 2.0/trial/Subtask_1_trial.json\")\n",
    "\n",
    "# trial_dataset = ConversationDataset(config_path, trial_data, option=\"trial\")\n",
    "# predict = Prediction(config_path)\n",
    "# predict.predict(model, raw_trial_data, trial_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ga_aug_std",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
