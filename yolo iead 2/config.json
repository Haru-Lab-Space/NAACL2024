{
    "main_layers": [
        "embeddings.word_embeddings.weight",
        "embeddings.position_embeddings.weight",
        "embeddings.token_type_embeddings.weight",
        "embeddings.LayerNorm.weight",
        "embeddings.LayerNorm.bias",
        "encoder.layer.0.attention.self.query.weight",
        "encoder.layer.0.attention.self.query.bias",
        "encoder.layer.0.attention.self.key.weight",
        "encoder.layer.0.attention.self.key.bias",
        "encoder.layer.0.attention.self.value.weight",
        "encoder.layer.0.attention.self.value.bias",
        "encoder.layer.0.attention.output.dense.weight",
        "encoder.layer.0.attention.output.dense.bias",
        "encoder.layer.0.attention.output.LayerNorm.weight",
        "encoder.layer.0.attention.output.LayerNorm.bias",
        "encoder.layer.0.intermediate.dense.weight",
        "encoder.layer.0.intermediate.dense.bias",
        "encoder.layer.0.output.dense.weight",
        "encoder.layer.0.output.dense.bias",
        "encoder.layer.0.output.LayerNorm.weight",
        "encoder.layer.0.output.LayerNorm.bias",
        "encoder.layer.1.attention.self.query.weight",
        "encoder.layer.1.attention.self.query.bias",
        "encoder.layer.1.attention.self.key.weight",
        "encoder.layer.1.attention.self.key.bias",
        "encoder.layer.1.attention.self.value.weight",
        "encoder.layer.1.attention.self.value.bias",
        "encoder.layer.1.attention.output.dense.weight",
        "encoder.layer.1.attention.output.dense.bias",
        "encoder.layer.1.attention.output.LayerNorm.weight",
        "encoder.layer.1.attention.output.LayerNorm.bias",
        "encoder.layer.1.intermediate.dense.weight",
        "encoder.layer.1.intermediate.dense.bias",
        "encoder.layer.1.output.dense.weight",
        "encoder.layer.1.output.dense.bias",
        "encoder.layer.1.output.LayerNorm.weight",
        "encoder.layer.1.output.LayerNorm.bias",
        "encoder.layer.2.attention.self.query.weight",
        "encoder.layer.2.attention.self.query.bias",
        "encoder.layer.2.attention.self.key.weight",
        "encoder.layer.2.attention.self.key.bias",
        "encoder.layer.2.attention.self.value.weight",
        "encoder.layer.2.attention.self.value.bias",
        "encoder.layer.2.attention.output.dense.weight",
        "encoder.layer.2.attention.output.dense.bias",
        "encoder.layer.2.attention.output.LayerNorm.weight",
        "encoder.layer.2.attention.output.LayerNorm.bias",
        "encoder.layer.2.intermediate.dense.weight",
        "encoder.layer.2.intermediate.dense.bias",
        "encoder.layer.2.output.dense.weight",
        "encoder.layer.2.output.dense.bias",
        "encoder.layer.2.output.LayerNorm.weight",
        "encoder.layer.2.output.LayerNorm.bias",
        "encoder.layer.3.attention.self.query.weight",
        "encoder.layer.3.attention.self.query.bias",
        "encoder.layer.3.attention.self.key.weight",
        "encoder.layer.3.attention.self.key.bias",
        "encoder.layer.3.attention.self.value.weight",
        "encoder.layer.3.attention.self.value.bias",
        "encoder.layer.3.attention.output.dense.weight",
        "encoder.layer.3.attention.output.dense.bias",
        "encoder.layer.3.attention.output.LayerNorm.weight",
        "encoder.layer.3.attention.output.LayerNorm.bias",
        "encoder.layer.3.intermediate.dense.weight",
        "encoder.layer.3.intermediate.dense.bias",
        "encoder.layer.3.output.dense.weight",
        "encoder.layer.3.output.dense.bias",
        "encoder.layer.3.output.LayerNorm.weight",
        "encoder.layer.3.output.LayerNorm.bias",
        "encoder.layer.4.attention.self.query.weight",
        "encoder.layer.4.attention.self.query.bias",
        "encoder.layer.4.attention.self.key.weight",
        "encoder.layer.4.attention.self.key.bias",
        "encoder.layer.4.attention.self.value.weight",
        "encoder.layer.4.attention.self.value.bias",
        "encoder.layer.4.attention.output.dense.weight",
        "encoder.layer.4.attention.output.dense.bias",
        "encoder.layer.4.attention.output.LayerNorm.weight",
        "encoder.layer.4.attention.output.LayerNorm.bias",
        "encoder.layer.4.intermediate.dense.weight",
        "encoder.layer.4.intermediate.dense.bias",
        "encoder.layer.4.output.dense.weight",
        "encoder.layer.4.output.dense.bias",
        "encoder.layer.4.output.LayerNorm.weight",
        "encoder.layer.4.output.LayerNorm.bias",
        "encoder.layer.5.attention.self.query.weight",
        "encoder.layer.5.attention.self.query.bias",
        "encoder.layer.5.attention.self.key.weight",
        "encoder.layer.5.attention.self.key.bias",
        "encoder.layer.5.attention.self.value.weight",
        "encoder.layer.5.attention.self.value.bias",
        "encoder.layer.5.attention.output.dense.weight",
        "encoder.layer.5.attention.output.dense.bias",
        "encoder.layer.5.attention.output.LayerNorm.weight",
        "encoder.layer.5.attention.output.LayerNorm.bias",
        "encoder.layer.5.intermediate.dense.weight",
        "encoder.layer.5.intermediate.dense.bias",
        "encoder.layer.5.output.dense.weight",
        "encoder.layer.5.output.dense.bias",
        "encoder.layer.5.output.LayerNorm.weight",
        "encoder.layer.5.output.LayerNorm.bias",
        "encoder.layer.6.attention.self.query.weight",
        "encoder.layer.6.attention.self.query.bias",
        "encoder.layer.6.attention.self.key.weight",
        "encoder.layer.6.attention.self.key.bias",
        "encoder.layer.6.attention.self.value.weight",
        "encoder.layer.6.attention.self.value.bias",
        "encoder.layer.6.attention.output.dense.weight",
        "encoder.layer.6.attention.output.dense.bias",
        "encoder.layer.6.attention.output.LayerNorm.weight",
        "encoder.layer.6.attention.output.LayerNorm.bias",
        "encoder.layer.6.intermediate.dense.weight",
        "encoder.layer.6.intermediate.dense.bias",
        "encoder.layer.6.output.dense.weight",
        "encoder.layer.6.output.dense.bias",
        "encoder.layer.6.output.LayerNorm.weight",
        "encoder.layer.6.output.LayerNorm.bias",
        "encoder.layer.7.attention.self.query.weight",
        "encoder.layer.7.attention.self.query.bias",
        "encoder.layer.7.attention.self.key.weight",
        "encoder.layer.7.attention.self.key.bias",
        "encoder.layer.7.attention.self.value.weight",
        "encoder.layer.7.attention.self.value.bias",
        "encoder.layer.7.attention.output.dense.weight",
        "encoder.layer.7.attention.output.dense.bias",
        "encoder.layer.7.attention.output.LayerNorm.weight",
        "encoder.layer.7.attention.output.LayerNorm.bias",
        "encoder.layer.7.intermediate.dense.weight",
        "encoder.layer.7.intermediate.dense.bias",
        "encoder.layer.7.output.dense.weight",
        "encoder.layer.7.output.dense.bias",
        "encoder.layer.7.output.LayerNorm.weight",
        "encoder.layer.7.output.LayerNorm.bias",
        "encoder.layer.8.attention.self.query.weight",
        "encoder.layer.8.attention.self.query.bias",
        "encoder.layer.8.attention.self.key.weight",
        "encoder.layer.8.attention.self.key.bias",
        "encoder.layer.8.attention.self.value.weight",
        "encoder.layer.8.attention.self.value.bias",
        "encoder.layer.8.attention.output.dense.weight",
        "encoder.layer.8.attention.output.dense.bias",
        "encoder.layer.8.attention.output.LayerNorm.weight",
        "encoder.layer.8.attention.output.LayerNorm.bias",
        "encoder.layer.8.intermediate.dense.weight",
        "encoder.layer.8.intermediate.dense.bias",
        "encoder.layer.8.output.dense.weight",
        "encoder.layer.8.output.dense.bias",
        "encoder.layer.8.output.LayerNorm.weight",
        "encoder.layer.8.output.LayerNorm.bias",
        "encoder.layer.9.attention.self.query.weight",
        "encoder.layer.9.attention.self.query.bias",
        "encoder.layer.9.attention.self.key.weight",
        "encoder.layer.9.attention.self.key.bias",
        "encoder.layer.9.attention.self.value.weight",
        "encoder.layer.9.attention.self.value.bias",
        "encoder.layer.9.attention.output.dense.weight",
        "encoder.layer.9.attention.output.dense.bias",
        "encoder.layer.9.attention.output.LayerNorm.weight",
        "encoder.layer.9.attention.output.LayerNorm.bias",
        "encoder.layer.9.intermediate.dense.weight",
        "encoder.layer.9.intermediate.dense.bias",
        "encoder.layer.9.output.dense.weight",
        "encoder.layer.9.output.dense.bias",
        "encoder.layer.9.output.LayerNorm.weight",
        "encoder.layer.9.output.LayerNorm.bias",
        "encoder.layer.10.attention.self.query.weight",
        "encoder.layer.10.attention.self.query.bias",
        "encoder.layer.10.attention.self.key.weight",
        "encoder.layer.10.attention.self.key.bias",
        "encoder.layer.10.attention.self.value.weight",
        "encoder.layer.10.attention.self.value.bias",
        "encoder.layer.10.attention.output.dense.weight",
        "encoder.layer.10.attention.output.dense.bias",
        "encoder.layer.10.attention.output.LayerNorm.weight",
        "encoder.layer.10.attention.output.LayerNorm.bias",
        "encoder.layer.10.intermediate.dense.weight",
        "encoder.layer.10.intermediate.dense.bias",
        "encoder.layer.10.output.dense.weight",
        "encoder.layer.10.output.dense.bias",
        "encoder.layer.10.output.LayerNorm.weight",
        "encoder.layer.10.output.LayerNorm.bias",
        "encoder.layer.11.attention.self.query.weight",
        "encoder.layer.11.attention.self.query.bias",
        "encoder.layer.11.attention.self.key.weight",
        "encoder.layer.11.attention.self.key.bias",
        "encoder.layer.11.attention.self.value.weight",
        "encoder.layer.11.attention.self.value.bias",
        "encoder.layer.11.attention.output.dense.weight",
        "encoder.layer.11.attention.output.dense.bias",
        "encoder.layer.11.attention.output.LayerNorm.weight",
        "encoder.layer.11.attention.output.LayerNorm.bias",
        "encoder.layer.11.intermediate.dense.weight",
        "encoder.layer.11.intermediate.dense.bias",
        "encoder.layer.11.output.dense.weight",
        "encoder.layer.11.output.dense.bias",
        "encoder.layer.11.output.LayerNorm.weight",
        "encoder.layer.11.output.LayerNorm.bias"
    ],
    "emotion_head_layers": [
        "emotion_head.emotion_fc.weight",
        "emotion_head.emotion_fc.bias",
        "cat_layer.weight",
        "cat_layer.bias"
    ],
    "casual_emotion_layers": [
        "casual_feedforward.0.weight",
        "casual_feedforward.0.bias",
        "casual_feedforward.3.weight",
        "casual_feedforward.3.bias"
    ],
    "rnn_layers": [
        "rnn.weight_ih_l0",
        "rnn.weight_hh_l0",
        "rnn.bias_ih_l0",
        "rnn.bias_hh_l0",
        "rnn.weight_ih_l0_reverse",
        "rnn.weight_hh_l0_reverse",
        "rnn.bias_ih_l0_reverse",
        "rnn.bias_hh_l0_reverse",
        "rnn.weight_ih_l1",
        "rnn.weight_hh_l1",
        "rnn.bias_ih_l1",
        "rnn.bias_hh_l1",
        "rnn.weight_ih_l1_reverse",
        "rnn.weight_hh_l1_reverse",
        "rnn.bias_ih_l1_reverse",
        "rnn.bias_hh_l1_reverse"
    ],
    "coordinating conjunction": [
        "for",
        "and",
        "nor",
        "but",
        "or",
        "yet",
        "so"
    ],
    "id2label": {
        "0": "anger",
        "1": "disgust",
        "2": "fear",
        "3": "joy",
        "4": "neutral",
        "5": "sadness",
        "6": "surprise"
    },
    "label2id": {
        "anger": 0,
        "disgust": 1,
        "fear": 2,
        "joy": 3,
        "neutral": 4,
        "sadness": 5,
        "surprise": 6
    },
    "save folder": "data",
    "train": "data/ECF 2.0/train/train.json",
    "test": "data/ECF 2.0/test/test.json",
    "save_checkpoint_dir": "data/checkpoint",
    "save_history_dir": "data/history",
    "dataset folder": "data/ECF 2.0/",
    "submit path": "data/predict/submit.json",
    "tokenizer_name": "roberta-base",
    "main_part_name": "roberta-base",
    "loss_fn_name": [
        "BCE_Loss",
        "MSE_Loss",
        "CrossEntropyLoss"
    ],
    "alpha_focal_loss": [
        3.5,
        15,
        15,
        2.5,
        1,
        5,
        3
    ],
    "gamma_focal_loss": 2,
    "metrics": [
        "strict F1",
        "avg strict F1",
        "proportional F1",
        "avg proportional F1"
    ],
    "candidate_num": 8,
    "token_size": 256,
    "overlap_threshold": 0.8,
    "predict_threshold": 0.4,
    "top_k": 9,
    "bos_token_id": 101,
    "eos_token_id": 102,
    "padding": "max_length",
    "max_length": 256,
    "batch_size": 32,
    "device": "cuda:0",
    "embedding_dim": 768,
    "hidden_size": 1024,
    "max_emotion_cause_pairs_length": 64,
    "left_padding": 6,
    "right_padding": 2,
    "patience": 5,
    "min_delta": 0,
    "candidate num": 8,
    "replace tokens": [
        "⁇"
    ],
    "punctuation error": [
        "%"
    ],
    "unclear eos words": [
        "what",
        "for",
        "and",
        "nor",
        "but",
        "or",
        "yet",
        "so",
        "this",
        "a",
        "an",
        "I",
        "are",
        "is",
        "am",
        "if",
        "unless",
        ", you know",
        "you know ,",
        " mean ,",
        "You know ,",
        "you know",
        ". You know",
        ", it"
    ],
    "unclear bos words": [
        "him",
        "her",
        "me"
    ],
    "transition sentence": [
        "you know",
        "I mean"
    ]
}