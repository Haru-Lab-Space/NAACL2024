{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxility Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "# Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video, read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Model\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer, BertModel, BertTokenizer, BertGenerationDecoder\n",
    "\n",
    "# Training parameter\n",
    "from torch.optim import Adam\n",
    "# Training process\n",
    "from tqdm import tqdm\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "def write_json(path, data):\n",
    "    with open(path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, config_path, data, option=\"train\"):\n",
    "        super(ConversationDataset, self).__init__()\n",
    "        self.build(config_path)\n",
    "        self.data = data\n",
    "        self.option = option\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.id2label = config[\"id2label\"]\n",
    "        self.tokenizer_name = config[\"tokenizer_name\"]\n",
    "        self.padding = config[\"padding\"]\n",
    "        self.max_length = config[\"max_length\"]\n",
    "        self.candidate_num = config[\"candidate num\"]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        conversation_ID = self.data[index][\"conversation_ID\"]\n",
    "        paragraph = self.tokenizer(self.data[index][\"paragraph\"], padding=self.padding, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        paragraph = torch.squeeze(paragraph['input_ids'])\n",
    "        utterance_ID = self.data[index][\"utterance_ID\"]\n",
    "\n",
    "        casual_pool = self.tokenizer(self.data[index][\"casual_pool\"][i], padding=self.padding, max_length=self.max_length)['input_ids'] \n",
    "        casual_span_pool = torch.FloatTensor(self.data[index][\"casual_span_pool\"])\n",
    "\n",
    "        if self.option == \"train\":\n",
    "            emotion_label = self.label2id[self.data[index][\"emotion\"]]\n",
    "            span_label = torch.FloatTensor(self.data[index][\"span_label\"])\n",
    "            return {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"paragraph\": paragraph,\n",
    "                \"utterance_ID\": utterance_ID,\n",
    "                \"casual_pool\": casual_pool,\n",
    "                \"casual_span_pool\": casual_span_pool,\n",
    "                \"emotion_label\": emotion_label,\n",
    "                \"span_label\": span_label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"paragraph\": paragraph,\n",
    "                \"utterance_ID\": utterance_ID,\n",
    "                \"casual_pool\": casual_pool,\n",
    "                \"casual_span_pool\": casual_span_pool\n",
    "            }\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.layers_to_freeze = []\n",
    "        self.history = {\n",
    "            \"train loss\": [],\n",
    "            \"valid loss\": [],\n",
    "            \"valid metrics\": []\n",
    "        }\n",
    "        self.evaluator = Evaluation(config_path)\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        print(config_path)\n",
    "        config = read_json(config_path)\n",
    "        self.patience = config['patience']\n",
    "        self.min_delta = config['min_delta']\n",
    "        self.fn_loss_name = config['loss_fn_name']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.device = config['device']\n",
    "        self.save_checkpoint_dir = config['save_checkpoint_dir']\n",
    "        self.save_history_dir = config['save_history_dir']\n",
    "        self.emotion_head_layers = config['emotion_head_layers']\n",
    "        self.casual_emotion_layers = config['casual_emotion_layers']\n",
    "        self.main_layers = config['main_layers']\n",
    "        self.early_stopper = EarlyStopper(self.patience, self.min_delta)\n",
    "    def emotion_head_training(self):\n",
    "        self.layers_to_freeze = self.casual_emotion_layers + self.main_layers\n",
    "        self.fn_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def casual_emotion_training(self):\n",
    "        self.layers_to_freeze = self.emotion_head_layers + self.main_layers\n",
    "        self.fn_loss = nn.MSELoss()\n",
    "\n",
    "    def full_training(self):\n",
    "        self.layers_to_freeze = []\n",
    "    def freeze(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(layer in name for layer in self.layers_to_freeze):\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        return model\n",
    "    def train(self, model, optimizer, train_dataset, valid_dataset, epochs, option=\"emotion\", batch_size=None):\n",
    "        self.optimizer = optimizer\n",
    "        if option == \"emotion\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.emotion_head_training()\n",
    "        elif option == \"casual_emotion\":\n",
    "            print(\"TRAINING \" + option + \" MODEL\")\n",
    "            self.casual_emotion_training()\n",
    "\n",
    "        if batch_size != None:\n",
    "            batch_size = self.batch_size\n",
    "        else:\n",
    "            batch_size = batch_size\n",
    "\n",
    "\n",
    "        self.early_stopper.reset()\n",
    "        self.model = self.freeze(model)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.train_dataloader = DataLoader(dataset=train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        self.valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        \n",
    "        best_val_loss = np.inf\n",
    "        self.epochs = epochs\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self._train_epoch(epoch, option)\n",
    "            val_loss, emotion_correct, span_correct = self._val_epoch(epoch, option)\n",
    "            \n",
    "            self.history[\"train loss\"].append(train_loss)\n",
    "            self.history[\"valid loss\"].append(val_loss)\n",
    "            self.history[\"emotion_correct\"].append(emotion_correct)\n",
    "            self.history[\"span_correct\"].append(span_correct)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                self.save_checkpoint(epoch, option)\n",
    "                print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f},  Valid loss {val_loss:.4f},  emotion_correct {emotion_correct*100:.4f}%,  span_correct {span_correct*100:.4f}%\")\n",
    "            if self.early_stopper.early_stop(val_loss):\n",
    "                print(\"Early stop at epoch: \"+str(epoch) + \" with valid loss: \"+str(val_loss))\n",
    "                break\n",
    "        write_json(self.save_history_dir + \"/\" + option + \"_history.json\") \n",
    "    def _train_epoch(self, epoch, option):\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "        logger_message = f'Training epoch {epoch}/{self.epochs}'\n",
    "\n",
    "        progress_bar = tqdm(self.train_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        for batch, data in enumerate(progress_bar):\n",
    "            conversation_id = data[\"conversation_ID\"]\n",
    "            paragraph = data[\"paragraph\"].to(self.device)\n",
    "            utter_id = data[\"utterance_ID\"]\n",
    "            casual_pool = data[\"casual_pool\"].to(self.device)\n",
    "            casual_span_pool = data[\"casual_span_pool\"]\n",
    "            emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "            span_label = data[\"span_label\"].to(self.device)\n",
    "            # print(\"conversation_id:\"+str(conversation_id.shape))\n",
    "            # print(\"paragraph:\"+str(paragraph.shape))\n",
    "            # print(\"casual_pool:\"+str(casual_pool.shape))\n",
    "            # print(\"emotion_label:\"+str(emotion_label.shape))\n",
    "            # print(\"span_label:\"+str(span_label.shape))\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            emotion_pred, span_pred = self.model(paragraph, casual_pool)\n",
    "            \n",
    "            if option == \"emotion\":\n",
    "                loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "            elif option == \"casual_emotion\":\n",
    "                loss = self.fn_loss(span_pred, span_label)\n",
    "            else:\n",
    "                emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                span_loss = self.fn_loss(span_pred, span_label)\n",
    "                loss = emotion_loss + span_loss\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return train_loss / len(self.train_dataloader)\n",
    "\n",
    "    def _val_epoch(self, epoch, option):\n",
    "        self.model.eval()\n",
    "        valid_loss = 0.0\n",
    "        valid_metrics = {}\n",
    "        logger_message = f'Validation epoch {epoch}/{self.epochs}'\n",
    "        progress_bar = tqdm(self.valid_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(progress_bar):\n",
    "                conversation_id = data[\"conversation_ID\"]\n",
    "                paragraph = data[\"paragraph\"].to(self.device)\n",
    "                utter_id = data[\"utterance_ID\"]\n",
    "                casual_pool = data[\"casual_pool\"].to(self.device)\n",
    "                casual_span_pool = data[\"casual_span_pool\"]\n",
    "                emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "                span_label = data[\"span_label\"].to(self.device)\n",
    "                emotion_pred, span_pred = self.model(paragraph, casual_pool)\n",
    "                \n",
    "                \n",
    "                if option == \"emotion\":\n",
    "                    loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                elif option == \"casual_emotion\":\n",
    "                    loss = self.fn_loss(span_pred, span_label)\n",
    "                else:\n",
    "                    emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n",
    "                    span_loss = self.fn_loss(span_pred, span_label)\n",
    "                    loss = emotion_loss + span_loss\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                emotion_correct += (emotion_pred == emotion_label).sum().item()\n",
    "                span_correct += (span_pred == span_label).sum().item()\n",
    "                \n",
    "        return valid_loss / len(self.valid_dataloader), emotion_correct / len(self.valid_dataloader), span_correct / len(self.valid_dataloader)\n",
    "        \n",
    "    def save_checkpoint(self, epoch, option):\n",
    "        checkpoint_path = os.path.join(self.save_checkpoint_dir, option + '/checkpoint_{}.pth'.format(epoch))\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(Prediction, self).__init__()\n",
    "        self.build(config_path)\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.id2label = config[\"id2label\"]\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.submit_path = config[\"submit path\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "    def predict(self, model, raw_dataset, dataset):\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "        pred_list = []\n",
    "        \n",
    "        self.test_dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        logger_message = f'Predict '\n",
    "        progress_bar = tqdm(self.test_dataloader,\n",
    "                            desc=logger_message, initial=0, dynamic_ncols=True)\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(progress_bar):\n",
    "                conversation_id = data[\"conversation_ID\"]\n",
    "                paragraph = data[\"paragraph\"].to(self.device)\n",
    "                utter_id = data[\"utterance_ID\"]\n",
    "                casual_pool = data[\"casual_pool\"].to(self.device)\n",
    "                casual_span_pool = data[\"casual_span_pool\"]\n",
    "                # emotion_label = data[\"emotion_label\"].to(self.device)\n",
    "                # span_label = data[\"span_label\"].to(self.device)\n",
    "                emotion_pred, span_pred = self.model(paragraph, casual_pool)\n",
    "                \n",
    "                pred = self.convertpred2list(conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool)\n",
    "                pred_list.extend(pred)\n",
    "        submit = self.convertdict2submit(self.convertpred2dict(raw_dataset, pred_list))\n",
    "        self.save_json(self.submit_path, submit)\n",
    "    def convertpred2list(self, conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool):\n",
    "        pred_list = []\n",
    "        for i in range(len(emotion_pred)):\n",
    "            emotion = self.id2label[str(torch.argmax(emotion_pred[i]).item())]\n",
    "            emotion_string = str(utter_id[i].item()) + \"_\" + emotion\n",
    "            if emotion != \"neutral\":\n",
    "                # \n",
    "                span = torch.round(span_pred[i])\n",
    "                # print(\"span: \"+str(span))\n",
    "                # print(\"casual_span_pool: \"+str(casual_span_pool[i]))\n",
    "                for j in range(len(span)):\n",
    "                    if span[j] == 1:\n",
    "                        if casual_span_pool[i][j][0].item() == 0:\n",
    "                            break\n",
    "                        if casual_span_pool[i][j][1].item() == casual_span_pool[i][j][2].item():\n",
    "                            break\n",
    "                        span_string = str(int(casual_span_pool[i][j][0].item())) + \"_\" + str(int(casual_span_pool[i][j][1].item())) + \"_\" + str(int(casual_span_pool[i][j][2].item()))\n",
    "                        pred_dict = {}\n",
    "                        pred_dict[\"conversation_ID\"] = conversation_id[i].item()\n",
    "                        pred_dict[\"emotion-cause_pairs\"] = [emotion_string, span_string]\n",
    "                        # print(\"pred_dict: \"+str(pred_dict))\n",
    "                        pred_list.append(pred_dict)\n",
    "        return pred_list\n",
    "    def convertpred2dict(self, raw_dataset, pred_list):\n",
    "        dataset = self.convertlist2dict(raw_dataset)\n",
    "        # dict = \n",
    "        for i in range(len(pred_list)):\n",
    "            pred = pred_list[i]\n",
    "            conversation_id = pred[\"conversation_ID\"]\n",
    "            # print(\"conversation_id: \"+str(conversation_id))\n",
    "            emotion_cause_pairs = pred[\"emotion-cause_pairs\"]\n",
    "            # print(\"emotion_cause_pairs: \"+str(emotion_cause_pairs))\n",
    "            if \"emotion-cause_pairs\" in dataset[conversation_id]:\n",
    "                dataset[conversation_id][\"emotion-cause_pairs\"].append(emotion_cause_pairs)\n",
    "            else:\n",
    "                dataset[conversation_id][\"emotion-cause_pairs\"] = []\n",
    "        # print(dataset)\n",
    "        return dataset\n",
    "    def convertlist2dict(self, raw_dataset):\n",
    "        dataset = {}\n",
    "        for i in range(len(raw_dataset)):\n",
    "            conversation = raw_dataset[i]\n",
    "            conversation_ID = conversation[\"conversation_ID\"]\n",
    "            # if conversation_ID not in dataset.key\n",
    "            dataset[conversation_ID] = {\n",
    "                \"conversation_ID\": conversation_ID,\n",
    "                \"conversation\": conversation['conversation']\n",
    "            }\n",
    "        return dataset\n",
    "    def convertdict2submit(self, raw_dataset):\n",
    "        dataset = []\n",
    "        for key in raw_dataset.keys():\n",
    "            # print(\"key: \"+str(key))\n",
    "            sample = raw_dataset[key]\n",
    "            # print(\"sample: \"+str(sample))\n",
    "            dataset.append(sample)\n",
    "        return dataset\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    def save_json(self, file_path, data):\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGG(nn.Module):\n",
    "    def __init__(self, config_path):\n",
    "        super(TGG, self).__init__()\n",
    "        self.build(config_path)\n",
    "        self.emotion_fc = nn.Linear(self.embedding_dim, self.label_num)\n",
    "        self.casual_feedforward = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def build(self, config_path):\n",
    "        config = self.read_json(config_path)\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "        self.token_size = config[\"token_size\"]\n",
    "        self.main_part_name = config[\"main_part_name\"]\n",
    "        self.label2id = config[\"label2id\"]\n",
    "        self.device = config[\"device\"]\n",
    "        \n",
    "        main_part = AutoModel.from_pretrained(self.main_part_name)\n",
    "        self.embeddings = main_part.embeddings\n",
    "        self.encoder = main_part.encoder\n",
    "        self.pooler = main_part.pooler\n",
    "        \n",
    "    def forward(self, x, candidate):\n",
    "        batch_size = x.size(0)\n",
    "        # print(\"x: \"+str(x.shape))\n",
    "        _x = self.embeddings(x)\n",
    "        _x = self.encoder(_x)['last_hidden_state']\n",
    "        _x = self.pooler(_x)\n",
    "        _e_category = self.emotion_head(_x)\n",
    "\n",
    "        \n",
    "        _candidate = self.embeddings(candidate)\n",
    "        _candidate = self.encoder(_candidate)['last_hidden_state']\n",
    "        _candidate = self.pooler(_candidate)\n",
    "\n",
    "\n",
    "        _candidate = self.casual_feedforward(_candidate)\n",
    "\n",
    "        _similarity = _e_category @ _candidate\n",
    "        \n",
    "        return _e_category, _similarity\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config.json\"\n",
    "dataset = read_json(\"data/ECF 2.0/train/train.json\")\n",
    "trial_data = read_json(\"data/ECF 2.0/trial/trial.json\")\n",
    "raw_trial_data = read_json(\"data/ECF 2.0/trial/Subtask_1_trial.json\")\n",
    "\n",
    "n_train = int(0.8 * len(dataset))\n",
    "train_data, valid_data = dataset[:n_train], dataset[n_train:]\n",
    "train_dataset = ConversationDataset(config_path, train_data)\n",
    "valid_dataset = ConversationDataset(config_path, valid_data)\n",
    "model = TGG(config_path)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config_path)\n",
    "trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion\", batch_size=128)\n",
    "trial_dataset = ConversationDataset(config_path, trial_data, option=\"trial\")\n",
    "predict = Prediction(config_path)\n",
    "predict.predict(model, raw_trial_data, trial_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
