{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38291,"status":"ok","timestamp":1705474967977,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"js2vN83Z7KJ1","outputId":"6c0f4822-01aa-4abe-f86b-ac9b65ad2d66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (4.36.2)\n","Requirement already satisfied: filelock in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from transformers) (0.20.2)\n","Requirement already satisfied: numpy>=1.17 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from transformers) (1.24.1)\n","Requirement already satisfied: packaging>=20.0 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from transformers) (4.64.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from requests->transformers) (2023.11.17)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: scikit-learn in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (1.3.2)\n","Requirement already satisfied: numpy<2.0,>=1.17.3 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from scikit-learn) (1.24.1)\n","Requirement already satisfied: scipy>=1.5.0 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /root/anaconda3/envs/diffusion/lib/python3.8/site-packages (from scikit-learn) (3.2.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m^C\n","Traceback (most recent call last):\n","  File \"/root/anaconda3/envs/diffusion/bin/pip\", line 5, in <module>\n","    from pip._internal.cli.main import main\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\n","    from pip._internal.cli.autocompletion import autocomplete\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n","    from pip._internal.cli.main_parser import create_main_parser\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n","    from pip._internal.build_env import get_runnable_pip\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/pip/_internal/build_env.py\", line 19, in <module>\n","    from pip._internal.cli.spinners import open_spinner\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n","    from pip._internal.utils.logging import get_indentation\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 29, in <module>\n","    from pip._internal.utils.misc import ensure_dir\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/pip/_internal/utils/misc.py\", line 40, in <module>\n","    from pip._vendor.tenacity import retry, stop_after_delay, wait_fixed\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/pip/_vendor/tenacity/__init__.py\", line 548, in <module>\n","    from pip._vendor.tenacity._asyncio import AsyncRetrying  # noqa:E402,I100\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/site-packages/pip/_vendor/tenacity/_asyncio.py\", line 21, in <module>\n","    from asyncio import sleep\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/asyncio/__init__.py\", line 8, in <module>\n","    from .base_events import *\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/asyncio/base_events.py\", line 34, in <module>\n","    import ssl\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/ssl.py\", line 223, in <module>\n","    class _TLSMessageType(_IntEnum):\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/enum.py\", line 200, in __new__\n","    dynamic_attributes = {\n","  File \"/root/anaconda3/envs/diffusion/lib/python3.8/enum.py\", line 203, in <setcomp>\n","    if isinstance(v, DynamicClassAttribute)\n","KeyboardInterrupt\n"]}],"source":["!pip install transformers\n","!pip install scikit-learn\n","!pip install focal-loss-torch"]},{"cell_type":"markdown","metadata":{"id":"qCnIyS9n7KJ3"},"source":["# Auxility Class"]},{"cell_type":"markdown","metadata":{"id":"egBKoM3C7KJ3"},"source":["## Library"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8643,"status":"ok","timestamp":1705475047526,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"EGPFNkUd7KJ3"},"outputs":[],"source":["import json\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os\n","# Dataset\n","from PIL import Image\n","from torchvision import transforms\n","from torchvision.io import read_video, read_image\n","from torch.utils.data import Dataset, DataLoader\n","# Model\n","from transformers import AutoModel\n","from transformers import AutoTokenizer, BertModel, BertTokenizer, BertGenerationDecoder, GPT2Config, GPT2Model, GPT2LMHeadModel\n","\n","# Training parameter\n","from torch.optim import Adam\n","# Training process\n","from tqdm import tqdm\n","# Metrics\n","from sklearn.metrics import classification_report, accuracy_score\n","from collections import OrderedDict\n","import copy\n","import sys\n","from focal_loss.focal_loss import FocalLoss"]},{"cell_type":"markdown","metadata":{"id":"1uNNY3lV7KJ4"},"source":["## Function"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705475047526,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"YED0wxk07KJ4"},"outputs":[],"source":["def read_json(file_path):\n","    with open(file_path, 'r') as json_file:\n","        data = json.load(json_file)\n","    return data\n","def write_json(path, data):\n","    with open(path, 'w') as json_file:\n","        json.dump(data, json_file, indent=4)\n","def mkdir(path):\n","    isExist = os.path.exists(path)\n","    if not isExist:\n","        # Create a new directory because it does not exist\n","        os.makedirs(path)\n","        print(\"The new directory checkpoint is created!\")"]},{"cell_type":"markdown","metadata":{"id":"UJp4g49J7KJ4"},"source":["## Early Stopper"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705475047526,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"FBVQXrkQ7KJ5"},"outputs":[],"source":["class EarlyStopper:\n","    def __init__(self, patience=1, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.min_validation_loss = float('inf')\n","\n","    def reset(self):\n","        self.counter = 0\n","\n","    def early_stop(self, validation_loss):\n","        if validation_loss < self.min_validation_loss:\n","            self.min_validation_loss = validation_loss\n","            self.counter = 0\n","        elif validation_loss > (self.min_validation_loss + self.min_delta):\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True\n","        return False"]},{"cell_type":"markdown","metadata":{"id":"0aJeyNxV7KJ5"},"source":["# Train"]},{"cell_type":"markdown","metadata":{"id":"12Am5XoK7KJ5"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705475047526,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"uqGjC7o67KJ5"},"outputs":[],"source":["class ConversationDataset(Dataset):\n","    def __init__(self, config_path, data, option=\"train\"):\n","        super(ConversationDataset, self).__init__()\n","        self.build(config_path)\n","        self.data = data\n","        self.option = option\n","    def build(self, config_path):\n","        config = read_json(config_path)\n","        self.label2id = config[\"label2id\"]\n","        self.id2label = config[\"id2label\"]\n","        self.tokenizer_name = config[\"tokenizer_name\"]\n","        self.padding = config[\"padding\"]\n","        self.paragraph_max_length = config[\"paragraph_max_length\"]\n","        self.text_max_length = config[\"text_max_length\"]\n","        self.left_padding = config[\"left_padding\"]\n","        self.right_padding = config[\"right_padding\"]\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n","        self.text_pool_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","        # self.text_pool_tokenizer.add_special_tokens({'pad_token': 'eos_token_id'})\n","        self.text_pool_tokenizer.pad_token = self.text_pool_tokenizer.eos_token\n","    def __len__(self):\n","        return len(self.data)\n","    def __getitem__(self, index):\n","        conversation_ID = self.data[index][\"conversation_ID\"]\n","        paragraph = self.tokenizer(self.data[index][\"paragraph\"], padding=self.padding, max_length=self.paragraph_max_length, return_tensors=\"pt\")\n","        paragraph = torch.squeeze(paragraph['input_ids'])\n","        utterance_ID = self.data[index][\"utterance_ID\"]\n","        text_pool = self.data[index][\"text_pool\"]\n","        speaker_pool = self.data[index][\"speaker_pool\"]\n","        token_text_pool = []\n","        token_speaker_pool = []\n","\n","        for i in range(len(text_pool)):\n","            text = self.text_pool_tokenizer.encode(text_pool[i], padding=self.padding, max_length=self.text_max_length)\n","            speaker = self.tokenizer(speaker_pool[i], padding=self.padding, max_length=8)[\"input_ids\"]\n","            token_text_pool.append(text)\n","            token_speaker_pool.append(speaker)\n","\n","        if len(text_pool) < self.left_padding+self.right_padding+1:\n","            for i in range(len(text_pool), self.left_padding+self.right_padding+1):\n","                text = self.text_pool_tokenizer.encode(\"\", padding=self.padding, max_length=self.text_max_length)\n","                speaker = self.tokenizer(\"\", padding=self.padding, max_length=8)[\"input_ids\"]\n","                token_text_pool.insert(0, text)\n","                token_speaker_pool.insert(0, speaker)\n","        # print(\" token_text_pool: \"+str(len(token_text_pool)))\n","        # print(\" token_speaker_pool: \"+str(len(token_speaker_pool)))\n","        if self.option == \"train\":\n","            label = self.label2id[self.data[index][\"emotion\"][\"property\"]]\n","            emotion_label = [0] * len(self.id2label)\n","            emotion_label[label] = 1\n","            emotion_label = torch.FloatTensor(emotion_label)\n","            casual_pool = self.data[index][\"emotion\"][\"casual\"]\n","            token_casual = []\n","\n","            for i in range(len(casual_pool)):\n","                casual = [casual_pool[i][\"start\"], casual_pool[i][\"end\"]]\n","                token_casual.append(casual)\n","\n","            for i in range(len(casual_pool), self.left_padding+self.right_padding+1):\n","                casual = [0, 0]\n","                token_casual.insert(0, casual)\n","            return {\n","                \"conversation_ID\": conversation_ID,\n","                \"paragraph\": paragraph,\n","                \"utterance_ID\": utterance_ID,\n","                \"token_text_pool\": torch.FloatTensor(token_text_pool),\n","                \"token_speaker_pool\": torch.FloatTensor(token_speaker_pool),\n","                \"emotion_label\": emotion_label,\n","                \"span_label\": torch.FloatTensor(token_casual)\n","            }\n","        else:\n","            return {\n","                \"conversation_ID\": conversation_ID,\n","                \"paragraph\": paragraph,\n","                \"utterance_ID\": utterance_ID,\n","                \"token_text_pool\": torch.FloatTensor(token_text_pool),\n","                \"token_speaker_pool\": torch.FloatTensor(token_speaker_pool),\n","            }"]},{"cell_type":"markdown","metadata":{"id":"Cc6kZnzQ7KJ6"},"source":["## Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1705475047526,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"J3Grd59_7KJ6"},"outputs":[],"source":["class Trainer(nn.Module):\n","    def __init__(self, config_path):\n","        super(Trainer, self).__init__()\n","        self.layers_to_freeze = []\n","        self.history = {\n","            \"train loss\": [],\n","            \"valid loss\": [],\n","            \"emotion_correct\": [],\n","            \"span_correct\": []\n","        }\n","        self.build(config_path)\n","    def build(self, config_path):\n","        print(config_path)\n","        config = read_json(config_path)\n","        self.patience = config['patience']\n","        self.min_delta = config['min_delta']\n","        self.batch_size = config['batch_size']\n","        self.device = config['device']\n","        self.save_checkpoint_dir = config['output_dir'] + \"/checkpoint\"\n","        self.save_history_dir = config['output_dir'] + \"/history\"\n","        self.emotion_head_layers = config['emotion_head_layers']\n","        self.casual_head_layers = config['casual_head_layers']\n","        self.decoder_layers = config['decoder_layers']\n","        self.encoder_layers = config['encoder_layers']\n","        self.lstm_layers = config['lstm_layers']\n","        self.alpha_focal_loss = config['alpha_focal_loss']\n","        self.gamma_focal_loss = config['gamma_focal_loss']\n","        self.label2id = config['label2id']\n","        self.target_names = self.label2id.keys()\n","        self.early_stopper = EarlyStopper(self.patience, self.min_delta)\n","\n","        mkdir(self.save_history_dir)\n","\n","    def emotion_head_training(self):\n","        self.layers_to_freeze = self.casual_head_layers + self.encoder_layers + self.decoder_layers + self.lstm_layers\n","        self.fn_loss = nn.CrossEntropyLoss()\n","        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n","    def emotion_encoder_training(self):\n","        self.layers_to_freeze = self.casual_head_layers + self.lstm_layers + self.decoder_layers\n","        self.fn_loss = nn.CrossEntropyLoss()\n","        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n","    def emotion_encoder_lstm_training(self):\n","        self.layers_to_freeze = self.casual_head_layers + self.decoder_layers\n","        self.fn_loss = nn.CrossEntropyLoss()\n","        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n","    def emotion_lstm_training(self):\n","        self.layers_to_freeze = self.casual_head_layers + self.encoder_layers + self.decoder_layers\n","        self.fn_loss = nn.CrossEntropyLoss()\n","        # self.fn_loss = FocalLoss(gamma=self.gamma_focal_loss, weights=torch.tensor(self.alpha_focal_loss).to(self.device))\n","\n","    def casual_head_training(self):\n","        self.layers_to_freeze = self.emotion_head_layers + self.encoder_layers + self.lstm_layers + self.decoder_layers\n","        self.fn_loss = nn.MSELoss()\n","\n","    def full_training(self):\n","        self.layers_to_freeze = []\n","    def freeze(self, model):\n","        for name, param in model.named_parameters():\n","            if any(layer in name for layer in self.layers_to_freeze):\n","                param.requires_grad = False\n","            else:\n","                param.requires_grad = True\n","        return model\n","    def train(self, model, optimizer, train_dataset, valid_dataset, epochs, option=\"emotion_head\", batch_size=None):\n","        self.optimizer = optimizer\n","        if option == \"emotion_head\":\n","            print(\"TRAINING \" + option + \" MODEL\")\n","            self.emotion_head_training()\n","        elif option == \"emotion_encoder\":\n","            print(\"TRAINING \" + option + \" MODEL\")\n","            self.emotion_encoder_training()\n","        elif option == \"emotion_lstm\":\n","            print(\"TRAINING \" + option + \" MODEL\")\n","            self.emotion_lstm_training()\n","        elif option == \"emotion_encoder_lstm\":\n","            print(\"TRAINING \" + option + \" MODEL\")\n","            self.emotion_encoder_lstm_training()\n","        elif option == \"casual_head\":\n","            print(\"TRAINING \" + option + \" MODEL\")\n","            self.casual_head_training()\n","\n","        if batch_size == None:\n","            self.batch_size = self.batch_size_config\n","        else:\n","            self.batch_size = batch_size\n","        mkdir(self.save_checkpoint_dir + \"/\" + option)\n","\n","\n","        print(\"device: \"+str(self.device))\n","        print(\"batch_size: \"+str(self.batch_size))\n","        print(\"gamma_focal_loss: \"+str(self.gamma_focal_loss))\n","        print(\"alpha_focal_loss: \"+str(self.alpha_focal_loss))\n","\n","        self.early_stopper.reset()\n","        self.model = self.freeze(model)\n","        # if self.device == 'cuda' and torch.cuda.device_count() > 1:\n","        #     print(\"Training Parallel!\")\n","        #     self.model = nn.DataParallel(self.model)\n","        self.model = self.model.to(self.device)\n","\n","        self.train_dataloader = DataLoader(dataset=train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n","        self.valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False)\n","\n","        best_val_loss = np.inf\n","        self.epochs = epochs\n","        for epoch in range(epochs):\n","            train_loss = self._train_epoch(epoch, option)\n","            val_loss, emotion_correct, span_correct, emotion_label_list, emotion_pred_list = self._val_epoch(epoch, option)\n","\n","            self.history[\"train loss\"].append(train_loss)\n","            self.history[\"valid loss\"].append(val_loss)\n","            self.history[\"emotion_correct\"].append(emotion_correct)\n","            self.history[\"span_correct\"].append(span_correct)\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                self.save_checkpoint(epoch, option)\n","                print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f},  Valid loss {val_loss:.4f},  emotion_correct {emotion_correct*100:.4f}%,  span_correct {span_correct*100:.4f}%\")\n","                print(classification_report(emotion_label_list, emotion_pred_list, target_names=self.target_names))\n","            if self.early_stopper.early_stop(val_loss):\n","                print(\"Early stop at epoch: \"+str(epoch) + \" with valid loss: \"+str(val_loss))\n","                break\n","\n","\n","        write_json(self.save_history_dir + \"/\" + option + \"_history.json\", self.history)\n","\n","    def _train_epoch(self, epoch, option):\n","        self.model.train()\n","        train_loss = 0.0\n","        logger_message = f'Training epoch {epoch}/{self.epochs}'\n","\n","        progress_bar = tqdm(self.train_dataloader,\n","                            desc=logger_message, initial=0, dynamic_ncols=True)\n","        for batch, data in enumerate(progress_bar):\n","            conversation_id = data[\"conversation_ID\"]\n","            paragraph = data[\"paragraph\"].to(self.device)\n","            utter_id = data[\"utterance_ID\"]\n","            token_text_pool = data[\"token_text_pool\"].to(self.device)\n","            token_speaker_pool = data[\"token_speaker_pool\"].to(self.device)\n","            emotion_label = data[\"emotion_label\"].to(self.device)\n","            span_label = data[\"span_label\"].to(self.device)\n","            # print(\"conversation_id:\"+str(conversation_id.shape))\n","            # print(\"paragraph:\"+str(paragraph.shape))\n","            # print(\"casual_pool:\"+str(casual_pool.shape))\n","            # print(\"emotion_label:\"+str(emotion_label.shape))\n","            # print(\"span_label:\"+str(span_label.shape))\n","            self.optimizer.zero_grad()\n","\n","            emotion_pred, span_pred = self.model(paragraph, token_text_pool, token_speaker_pool)\n","            # print(\"emotion_label : \"+str(emotion_label.shape))\n","            # print(\"span_label : \"+str(span_label.shape))\n","            # print(\"emotion_pred : \"+str(emotion_pred.shape))\n","            # print(\"span_pred : \"+str(span_pred.shape))\n","\n","            emotion_label = torch.argmax(emotion_label, dim=-1)\n","            if option == \"emotion_head\":\n","                loss = self.fn_loss(emotion_pred, emotion_label)\n","            elif option == \"emotion_lstm\":\n","                loss = self.fn_loss(emotion_pred, emotion_label)\n","            elif option == \"emotion_encoder\":\n","                loss = self.fn_loss(emotion_pred, emotion_label)\n","            elif option == \"emotion_encoder_lstm\":\n","                loss = self.fn_loss(emotion_pred, emotion_label)\n","            elif option == \"casual_head\":\n","                loss = self.fn_loss(span_pred, span_label)\n","            else:\n","                emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n","                span_loss = self.fn_loss(span_pred, span_label)\n","                loss = emotion_loss + span_loss\n","\n","            train_loss += loss.item()\n","            loss.backward()\n","            self.optimizer.step()\n","        return train_loss / len(self.train_dataloader)\n","\n","    def _val_epoch(self, epoch, option):\n","        self.model.eval()\n","        valid_loss = 0.0\n","        emotion_correct=0\n","        span_correct=0\n","        emotion_pred_list = []\n","        emotion_label_list = []\n","        logger_message = f'Validation epoch {epoch}/{self.epochs}'\n","        progress_bar = tqdm(self.valid_dataloader,\n","                            desc=logger_message, initial=0, dynamic_ncols=True)\n","        with torch.no_grad():\n","            for _, data in enumerate(progress_bar):\n","                conversation_id = data[\"conversation_ID\"]\n","                paragraph = data[\"paragraph\"].to(self.device)\n","                utter_id = data[\"utterance_ID\"]\n","                token_text_pool = data[\"token_text_pool\"].to(self.device)\n","                token_speaker_pool = data[\"token_speaker_pool\"].to(self.device)\n","                emotion_label = data[\"emotion_label\"].to(self.device)\n","                span_label = data[\"span_label\"].to(self.device)\n","                emotion_pred, span_pred = self.model(paragraph, token_text_pool, token_speaker_pool)\n","\n","\n","\n","\n","                emotion_label = torch.argmax(emotion_label, dim=-1)\n","                if option == \"emotion_head\":\n","                    loss = self.fn_loss(emotion_pred, emotion_label)\n","                elif option == \"emotion_lstm\":\n","                    loss = self.fn_loss(emotion_pred, emotion_label)\n","                elif option == \"emotion_encoder\":\n","                    loss = self.fn_loss(emotion_pred, emotion_label)\n","                elif option == \"emotion_encoder_lstm\":\n","                    loss = self.fn_loss(emotion_pred, emotion_label)\n","                elif option == \"casual_head\":\n","                    loss = self.fn_loss(span_pred, span_label)\n","                else:\n","                    emotion_loss = self.fn_loss(emotion_pred, emotion_label)\n","                    span_loss = self.fn_loss(span_pred, span_label)\n","                    loss = emotion_loss + span_loss\n","\n","                # emotion_label_ = torch.argmax(emotion_label, dim=-1)\n","                emotion_pred_ = torch.argmax(emotion_pred, dim=-1)\n","                emotion_pred_list.extend(emotion_pred_.cpu().tolist())\n","                emotion_label_list.extend(emotion_label.cpu().tolist())\n","                # print(\"emotion_label_: \"+ str(emotion_label_))\n","                # print(\"emotion_pred_: \"+ str(emotion_pred_))\n","                valid_loss += loss.item()\n","                emotion_correct += (emotion_pred_ == emotion_label).sum().item()\n","                span_correct += (span_pred == span_label).sum().item()\n","\n","        return valid_loss / len(self.valid_dataloader), emotion_correct / len(self.valid_dataloader) / self.batch_size, span_correct / len(self.valid_dataloader) / self.batch_size, emotion_label_list, emotion_pred_list\n","\n","    def save_checkpoint(self, epoch, option):\n","        checkpoint_path = os.path.join(self.save_checkpoint_dir, option + '/checkpoint_{}.pth'.format(epoch))\n","        torch.save({\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'epoch': epoch\n","        }, checkpoint_path)"]},{"cell_type":"markdown","metadata":{"id":"hwtZfsAL7KJ6"},"source":["## Predictor"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705475047526,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"SPS_n89H7KJ6"},"outputs":[],"source":["class Prediction(nn.Module):\n","    def __init__(self, config_path):\n","        super(Prediction, self).__init__()\n","        self.build(config_path)\n","    def build(self, config_path):\n","        config = self.read_json(config_path)\n","        self.id2label = config[\"id2label\"]\n","        self.label2id = config[\"label2id\"]\n","        self.submit_path = config[\"submit path\"]\n","        self.device = config[\"device\"]\n","        self.batch_size = config[\"batch_size\"]\n","    def predict(self, model, raw_dataset, dataset):\n","        self.model = model.to(self.device)\n","        self.model.eval()\n","        pred_list = []\n","\n","        self.test_dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=False)\n","        logger_message = f'Predict '\n","        progress_bar = tqdm(self.test_dataloader,\n","                            desc=logger_message, initial=0, dynamic_ncols=True)\n","        with torch.no_grad():\n","            for _, data in enumerate(progress_bar):\n","                conversation_id = data[\"conversation_ID\"]\n","                paragraph = data[\"paragraph\"].to(self.device)\n","                utter_id = data[\"utterance_ID\"]\n","                token_text_pool = data[\"token_text_pool\"].to(self.device)\n","                token_speaker_pool = data[\"token_speaker_pool\"].to(self.device)\n","                emotion_pred, span_pred = self.model(paragraph, token_text_pool, token_speaker_pool)\n","\n","                pred = self.convertpred2list(conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool)\n","                pred_list.extend(pred)\n","        submit = self.convertdict2submit(self.convertpred2dict(raw_dataset, pred_list))\n","        self.save_json(self.submit_path, submit)\n","    def convertpred2list(self, conversation_id, utter_id, emotion_pred, span_pred, casual_span_pool):\n","        pred_list = []\n","        for i in range(len(emotion_pred)):\n","            emotion = self.id2label[str(torch.argmax(emotion_pred[i]).item())]\n","            emotion_string = str(utter_id[i].item()) + \"_\" + emotion\n","            if emotion != \"neutral\":\n","                #\n","                span = torch.round(span_pred[i])\n","                # print(\"span: \"+str(span))\n","                # print(\"casual_span_pool: \"+str(casual_span_pool[i]))\n","                for j in range(len(span)):\n","                    if span[j] == 1:\n","                        if casual_span_pool[i][j][0].item() == 0:\n","                            break\n","                        if casual_span_pool[i][j][1].item() == casual_span_pool[i][j][2].item():\n","                            break\n","                        span_string = str(int(casual_span_pool[i][j][0].item())) + \"_\" + str(int(casual_span_pool[i][j][1].item())) + \"_\" + str(int(casual_span_pool[i][j][2].item()))\n","                        pred_dict = {}\n","                        pred_dict[\"conversation_ID\"] = conversation_id[i].item()\n","                        pred_dict[\"emotion-cause_pairs\"] = [emotion_string, span_string]\n","                        # print(\"pred_dict: \"+str(pred_dict))\n","                        pred_list.append(pred_dict)\n","        return pred_list\n","    def convertpred2dict(self, raw_dataset, pred_list):\n","        dataset = self.convertlist2dict(raw_dataset)\n","        # dict =\n","        for i in range(len(pred_list)):\n","            pred = pred_list[i]\n","            conversation_id = pred[\"conversation_ID\"]\n","            # print(\"conversation_id: \"+str(conversation_id))\n","            emotion_cause_pairs = pred[\"emotion-cause_pairs\"]\n","            # print(\"emotion_cause_pairs: \"+str(emotion_cause_pairs))\n","            if \"emotion-cause_pairs\" in dataset[conversation_id]:\n","                dataset[conversation_id][\"emotion-cause_pairs\"].append(emotion_cause_pairs)\n","            else:\n","                dataset[conversation_id][\"emotion-cause_pairs\"] = []\n","        # print(dataset)\n","        return dataset\n","    def convertlist2dict(self, raw_dataset):\n","        dataset = {}\n","        for i in range(len(raw_dataset)):\n","            conversation = raw_dataset[i]\n","            conversation_ID = conversation[\"conversation_ID\"]\n","            # if conversation_ID not in dataset.key\n","            dataset[conversation_ID] = {\n","                \"conversation_ID\": conversation_ID,\n","                \"conversation\": conversation['conversation']\n","            }\n","        return dataset\n","    def convertdict2submit(self, raw_dataset):\n","        dataset = []\n","        for key in raw_dataset.keys():\n","            # print(\"key: \"+str(key))\n","            sample = raw_dataset[key]\n","            # print(\"sample: \"+str(sample))\n","            dataset.append(sample)\n","        return dataset\n","    def read_json(self, file_path):\n","        with open(file_path, 'r') as json_file:\n","            data = json.load(json_file)\n","        return data\n","    def save_json(self, file_path, data):\n","        with open(file_path, 'w') as json_file:\n","            json.dump(data, json_file)"]},{"cell_type":"markdown","metadata":{"id":"CWmCyMAY7KJ6"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705475047527,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"Q6EV8KsJ7KJ6"},"outputs":[],"source":["class TGG(nn.Module):\n","    def __init__(self, config_path):\n","        super(TGG, self).__init__()\n","        self.build(config_path)\n","        self.lstm = nn.LSTM(self.text_max_length, self.hidden_size, self.num_layers, batch_first=True, dropout=0.2, bidirectional=True)\n","        self.lstma_projection = nn.Linear(2*self.num_layers*self.hidden_size, self.embedding_dim)\n","        self.cat_layer = nn.Linear(2*self.embedding_dim, self.embedding_dim)\n","        self.emotion_fc = nn.Linear(self.embedding_dim, self.label_num)\n","        self.casual_feedforward = nn.Sequential(\n","            nn.Linear(self.embedding_dim * self.text_max_length, self.embedding_dim)\n","        )\n","        self.casual_fc = nn.Linear(self.embedding_dim, 2)\n","        self.softmax = nn.Softmax(dim=-1)\n","    def build(self, config_path):\n","        config = self.read_json(config_path)\n","        self.hidden_size = config[\"hidden_size\"]\n","        self.embedding_dim = config[\"embedding_dim\"]\n","        self.encoder_name = config[\"encoder_name\"]\n","        self.decoder_name = config[\"decoder_name\"]\n","        self.label2id = config[\"label2id\"]\n","        self.label_num = len(self.label2id)\n","        self.num_layers = config[\"num_layers\"]\n","        self.paragraph_max_length = config[\"paragraph_max_length\"]\n","        self.text_max_length = config[\"text_max_length\"]\n","        self.device = config[\"device\"]\n","\n","        main_part = AutoModel.from_pretrained(self.encoder_name)\n","        self.embeddings = main_part.embeddings\n","        self.encoder = main_part.encoder\n","        self.pooler = main_part.pooler\n","\n","        self.decoder = GPT2LMHeadModel.from_pretrained(self.decoder_name, output_hidden_states =True)\n","\n","    def forward(self, paragraph, token_text_pool, token_speaker_pool):\n","        batch_size = paragraph.size(0)\n","        candidate_size = token_text_pool.size(1)\n","        t = token_text_pool.size(2)\n","        device = paragraph.get_device()\n","\n","        h0 = torch.randn((2 * self.num_layers, batch_size, self.hidden_size), device=device)\n","        c0 = torch.randn((2 * self.num_layers, batch_size, self.hidden_size), device=device)\n","        output, (hn, cn) = self.lstm(token_text_pool, (h0, c0))\n","        hn = hn.permute(1,0,2)\n","        hn = self.lstma_projection(hn.reshape(batch_size, -1))\n","        print(\"paragraph:\"+str(paragraph.shape))\n","\n","        _paragraph = self.embeddings(paragraph)\n","        _paragraph = self.encoder(_paragraph)['last_hidden_state']\n","        _paragraph = self.pooler(_paragraph)\n","        print(\"_paragraph:\"+str(_paragraph.shape))\n","\n","        hidden_state = torch.cat([hn, _paragraph], dim=-1)\n","\n","        _x = self.cat_layer(hidden_state)\n","        _e_category = self.emotion_fc(_x)\n","\n","        token_text_pool = token_text_pool.permute(1,0,2).long()\n","        print(\"token_text_pool:\"+str(token_text_pool.shape))\n","        candidate = torch.rand((candidate_size, batch_size, 2), device=device)\n","        for i in range(candidate_size):\n","            print(\"token_text_pool[i] : \"+str(token_text_pool[i].shape))\n","            # print(self.decoder(token_text_pool[i]).keys())\n","            print(\"self.decoder(token_text_pool[i])['hidden_states'][12]:\"+str(self.decoder(token_text_pool[i])['hidden_states'][12].shape))\n","            temp = self.decoder(token_text_pool[i])['hidden_states'][12]\n","            print(\"temp:\"+str(temp.shape))\n","            temp = self.casual_feedforward(temp.reshape(batch_size, -1))\n","            print(\"temp:\"+str(temp.shape))\n","            candidate[i] = self.casual_fc(temp)\n","            print(\"candidate:\"+str(candidate[i].shape))\n","        candidate = candidate.permute(1,0,2)\n","        candidate = self.casual_feedforward(candidate)\n","        # print()\n","\n","        return self.softmax(_e_category), candidate\n","    def read_json(self, file_path):\n","        with open(file_path, 'r') as json_file:\n","            data = json.load(json_file)\n","        return data"]},{"cell_type":"markdown","metadata":{"id":"oJ85bPaz7KJ6"},"source":["# Test"]},{"cell_type":"markdown","metadata":{"id":"6BVEbwM47KJ7"},"source":["## Load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11025,"status":"ok","timestamp":1705475152682,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"MLNUGqAV7KJ7"},"outputs":[],"source":["config_path = \"/content/drive/MyDrive/NAACL/model/config.json\"\n","train_data = read_json(\"/content/drive/MyDrive/HaruLab/Dataset/ECF 2.0/train/augment_train.json\")\n","valid_data = read_json(\"/content/drive/MyDrive/HaruLab/Dataset/ECF 2.0/train/augment_valid.json\")\n","# trial_data = read_json(\"../data/ECF 2.0/trial/trial.json\")\n","# raw_trial_data = read_json(\"../data/ECF 2.0/trial/Subtask_1_trial.json\")\n","\n","train_dataset = ConversationDataset(config_path, train_data)\n","valid_dataset = ConversationDataset(config_path, valid_data)\n","model = TGG(config_path)\n","optimizer = Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1705475092415,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"m0efN99A7KJ7"},"outputs":[],"source":["# for name, param in model.named_parameters():\n","#     print(\"\\\"\" + name + \"\\\",\")"]},{"cell_type":"markdown","metadata":{"id":"t8sYLdYB7KJ7"},"source":["## Predict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705475152682,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"rkZCLxR57KJ7","outputId":"ac4696b7-d361-4029-cbc6-695e111f08a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/NAACL/model/config.json\n"]}],"source":["trainer = Trainer(config_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"elapsed":3,"status":"error","timestamp":1705475190501,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"BPCva9D67KJ7","outputId":"2a54bd1d-662f-459b-96fe-9b5fb907dc13"},"outputs":[{"name":"stdout","output_type":"stream","text":["TRAINING emotion_head MODEL\n","device: cuda\n","batch_size: 32\n","gamma_focal_loss: 2\n","alpha_focal_loss: [1, 5, 5, 1, 1, 3, 1]\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 1.06 MiB is free. Process 2175 has 14.74 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 135.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-e9ee1ab33323>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"emotion_head\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-066761598622>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, optimizer, train_dataset, valid_dataset, epochs, option, batch_size)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#     print(\"Training Parallel!\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m#     self.model = nn.DataParallel(self.model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# Note: be v. careful before removing this, as 3rd party device types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_flat_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_init_flat_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m         self._flat_weight_refs = [weakref.ref(w) if w is not None else None\n\u001b[1;32m    154\u001b[0m                                   for w in self._flat_weights]\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                         \u001b[0mnum_weights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     torch._cudnn_rnn_flatten_weight(\n\u001b[0m\u001b[1;32m    207\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cudnn_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 1.06 MiB is free. Process 2175 has 14.74 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 135.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_head\", batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1705475097417,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"rpqYS6m07KJ7"},"outputs":[],"source":["trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_lstm\", batch_size=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1705475097418,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"FCKo6CcQ7KJ7"},"outputs":[],"source":["trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_head\", batch_size=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1705475097418,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"zs9WAN5C7KJ7"},"outputs":[],"source":["trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_encoder\", batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1705475097418,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"IBQdLW1S7KJ7"},"outputs":[],"source":["trainer.train(model, optimizer, train_dataset=train_dataset, valid_dataset=valid_dataset, epochs=100, option=\"emotion_encoder_lstm\", batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1705475097418,"user":{"displayName":"Đức Lê Ngọc","userId":"00638485192384344216"},"user_tz":-420},"id":"rz6DnBOJ7KJ7"},"outputs":[],"source":["\n","trial_dataset = ConversationDataset(config_path, trial_data, option=\"trial\")\n","predict = Prediction(config_path)\n","predict.predict(model, raw_trial_data, trial_dataset)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
