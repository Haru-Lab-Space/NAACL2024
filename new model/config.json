{
    "data_dir": "../data",
    "output_dir": "../output",
    "submit path": "../data/ECF 2.0/trial/submit.json",
    "encoder_name": "meta-llama/Llama-2-7b-hf",
    "decoder_name": "gpt2",
    "tokenizer_name": "meta-llama/Llama-2-7b-hf",
    "id2label": {
        "0": "anger",
        "1": "disgust",
        "2": "fear",
        "3": "joy",
        "4": "neutral",
        "5": "sadness",
        "6": "surprise"
    },
    "label2id": {
        "anger": 0,
        "disgust": 1,
        "fear": 2,
        "joy": 3,
        "neutral": 4,
        "sadness": 5,
        "surprise": 6
    },
    "bos_token_id": 101,
    "eos_token_id": 102,
    "padding": "max_length",
    "left_padding": 5,
    "right_padding": 2,
    "batch_size": 8,
    "device": "cuda",
    "embedding_dim": 4096,
    "hidden_size": 768,
    "vocab_size": 768,
    "paragraph_max_length": 256,
    "text_max_length": 128,
    "num_layers": 2,
    "patience": 5,
    "min_delta": 0,
    "alpha_focal_loss": [
        1,
        5,
        5,
        1,
        1,
        3,
        1
    ],
    "gamma_focal_loss": 2,
    "punctuation_error": [
        "%"
    ],
    "emotion_head_layers": [
        "cat_layer.weight",
        "cat_layer.bias",
        "emotion_fc.weight",
        "emotion_fc.bias"
    ],
    "lstm_layers": [
        "lstm.weight_ih_l0",
        "lstm.weight_hh_l0",
        "lstm.bias_ih_l0",
        "lstm.bias_hh_l0",
        "lstm.weight_ih_l0_reverse",
        "lstm.weight_hh_l0_reverse",
        "lstm.bias_ih_l0_reverse",
        "lstm.bias_hh_l0_reverse",
        "lstm.weight_ih_l1",
        "lstm.weight_hh_l1",
        "lstm.bias_ih_l1",
        "lstm.bias_hh_l1",
        "lstm.weight_ih_l1_reverse",
        "lstm.weight_hh_l1_reverse",
        "lstm.bias_ih_l1_reverse",
        "lstm.bias_hh_l1_reverse"
    ],
    "casual_head_layers": [
        "casual_feedforward.0.weight",
        "casual_feedforward.0.bias",
        "casual_fc.weight",
        "casual_fc.bias"
    ],
    "encoder_layers": [
        "embeddings.weight",
        "encoder.0.self_attn.q_proj.weight",
        "encoder.0.self_attn.k_proj.weight",
        "encoder.0.self_attn.v_proj.weight",
        "encoder.0.self_attn.o_proj.weight",
        "encoder.0.mlp.gate_proj.weight",
        "encoder.0.mlp.up_proj.weight",
        "encoder.0.mlp.down_proj.weight",
        "encoder.0.input_layernorm.weight",
        "encoder.0.post_attention_layernorm.weight",
        "encoder.1.self_attn.q_proj.weight",
        "encoder.1.self_attn.k_proj.weight",
        "encoder.1.self_attn.v_proj.weight",
        "encoder.1.self_attn.o_proj.weight",
        "encoder.1.mlp.gate_proj.weight",
        "encoder.1.mlp.up_proj.weight",
        "encoder.1.mlp.down_proj.weight",
        "encoder.1.input_layernorm.weight",
        "encoder.1.post_attention_layernorm.weight",
        "encoder.2.self_attn.q_proj.weight",
        "encoder.2.self_attn.k_proj.weight",
        "encoder.2.self_attn.v_proj.weight",
        "encoder.2.self_attn.o_proj.weight",
        "encoder.2.mlp.gate_proj.weight",
        "encoder.2.mlp.up_proj.weight",
        "encoder.2.mlp.down_proj.weight",
        "encoder.2.input_layernorm.weight",
        "encoder.2.post_attention_layernorm.weight",
        "encoder.3.self_attn.q_proj.weight",
        "encoder.3.self_attn.k_proj.weight",
        "encoder.3.self_attn.v_proj.weight",
        "encoder.3.self_attn.o_proj.weight",
        "encoder.3.mlp.gate_proj.weight",
        "encoder.3.mlp.up_proj.weight",
        "encoder.3.mlp.down_proj.weight",
        "encoder.3.input_layernorm.weight",
        "encoder.3.post_attention_layernorm.weight",
        "encoder.4.self_attn.q_proj.weight",
        "encoder.4.self_attn.k_proj.weight",
        "encoder.4.self_attn.v_proj.weight",
        "encoder.4.self_attn.o_proj.weight",
        "encoder.4.mlp.gate_proj.weight",
        "encoder.4.mlp.up_proj.weight",
        "encoder.4.mlp.down_proj.weight",
        "encoder.4.input_layernorm.weight",
        "encoder.4.post_attention_layernorm.weight",
        "encoder.5.self_attn.q_proj.weight",
        "encoder.5.self_attn.k_proj.weight",
        "encoder.5.self_attn.v_proj.weight",
        "encoder.5.self_attn.o_proj.weight",
        "encoder.5.mlp.gate_proj.weight",
        "encoder.5.mlp.up_proj.weight",
        "encoder.5.mlp.down_proj.weight",
        "encoder.5.input_layernorm.weight",
        "encoder.5.post_attention_layernorm.weight",
        "encoder.6.self_attn.q_proj.weight",
        "encoder.6.self_attn.k_proj.weight",
        "encoder.6.self_attn.v_proj.weight",
        "encoder.6.self_attn.o_proj.weight",
        "encoder.6.mlp.gate_proj.weight",
        "encoder.6.mlp.up_proj.weight",
        "encoder.6.mlp.down_proj.weight",
        "encoder.6.input_layernorm.weight",
        "encoder.6.post_attention_layernorm.weight",
        "encoder.7.self_attn.q_proj.weight",
        "encoder.7.self_attn.k_proj.weight",
        "encoder.7.self_attn.v_proj.weight",
        "encoder.7.self_attn.o_proj.weight",
        "encoder.7.mlp.gate_proj.weight",
        "encoder.7.mlp.up_proj.weight",
        "encoder.7.mlp.down_proj.weight",
        "encoder.7.input_layernorm.weight",
        "encoder.7.post_attention_layernorm.weight",
        "encoder.8.self_attn.q_proj.weight",
        "encoder.8.self_attn.k_proj.weight",
        "encoder.8.self_attn.v_proj.weight",
        "encoder.8.self_attn.o_proj.weight",
        "encoder.8.mlp.gate_proj.weight",
        "encoder.8.mlp.up_proj.weight",
        "encoder.8.mlp.down_proj.weight",
        "encoder.8.input_layernorm.weight",
        "encoder.8.post_attention_layernorm.weight",
        "encoder.9.self_attn.q_proj.weight",
        "encoder.9.self_attn.k_proj.weight",
        "encoder.9.self_attn.v_proj.weight",
        "encoder.9.self_attn.o_proj.weight",
        "encoder.9.mlp.gate_proj.weight",
        "encoder.9.mlp.up_proj.weight",
        "encoder.9.mlp.down_proj.weight",
        "encoder.9.input_layernorm.weight",
        "encoder.9.post_attention_layernorm.weight",
        "encoder.10.self_attn.q_proj.weight",
        "encoder.10.self_attn.k_proj.weight",
        "encoder.10.self_attn.v_proj.weight",
        "encoder.10.self_attn.o_proj.weight",
        "encoder.10.mlp.gate_proj.weight",
        "encoder.10.mlp.up_proj.weight",
        "encoder.10.mlp.down_proj.weight",
        "encoder.10.input_layernorm.weight",
        "encoder.10.post_attention_layernorm.weight",
        "encoder.11.self_attn.q_proj.weight",
        "encoder.11.self_attn.k_proj.weight",
        "encoder.11.self_attn.v_proj.weight",
        "encoder.11.self_attn.o_proj.weight",
        "encoder.11.mlp.gate_proj.weight",
        "encoder.11.mlp.up_proj.weight",
        "encoder.11.mlp.down_proj.weight",
        "encoder.11.input_layernorm.weight",
        "encoder.11.post_attention_layernorm.weight",
        "encoder.12.self_attn.q_proj.weight",
        "encoder.12.self_attn.k_proj.weight",
        "encoder.12.self_attn.v_proj.weight",
        "encoder.12.self_attn.o_proj.weight",
        "encoder.12.mlp.gate_proj.weight",
        "encoder.12.mlp.up_proj.weight",
        "encoder.12.mlp.down_proj.weight",
        "encoder.12.input_layernorm.weight",
        "encoder.12.post_attention_layernorm.weight",
        "encoder.13.self_attn.q_proj.weight",
        "encoder.13.self_attn.k_proj.weight",
        "encoder.13.self_attn.v_proj.weight",
        "encoder.13.self_attn.o_proj.weight",
        "encoder.13.mlp.gate_proj.weight",
        "encoder.13.mlp.up_proj.weight",
        "encoder.13.mlp.down_proj.weight",
        "encoder.13.input_layernorm.weight",
        "encoder.13.post_attention_layernorm.weight",
        "encoder.14.self_attn.q_proj.weight",
        "encoder.14.self_attn.k_proj.weight",
        "encoder.14.self_attn.v_proj.weight",
        "encoder.14.self_attn.o_proj.weight",
        "encoder.14.mlp.gate_proj.weight",
        "encoder.14.mlp.up_proj.weight",
        "encoder.14.mlp.down_proj.weight",
        "encoder.14.input_layernorm.weight",
        "encoder.14.post_attention_layernorm.weight",
        "encoder.15.self_attn.q_proj.weight",
        "encoder.15.self_attn.k_proj.weight",
        "encoder.15.self_attn.v_proj.weight",
        "encoder.15.self_attn.o_proj.weight",
        "encoder.15.mlp.gate_proj.weight",
        "encoder.15.mlp.up_proj.weight",
        "encoder.15.mlp.down_proj.weight",
        "encoder.15.input_layernorm.weight",
        "encoder.15.post_attention_layernorm.weight",
        "encoder.16.self_attn.q_proj.weight",
        "encoder.16.self_attn.k_proj.weight",
        "encoder.16.self_attn.v_proj.weight",
        "encoder.16.self_attn.o_proj.weight",
        "encoder.16.mlp.gate_proj.weight",
        "encoder.16.mlp.up_proj.weight",
        "encoder.16.mlp.down_proj.weight",
        "encoder.16.input_layernorm.weight",
        "encoder.16.post_attention_layernorm.weight",
        "encoder.17.self_attn.q_proj.weight",
        "encoder.17.self_attn.k_proj.weight",
        "encoder.17.self_attn.v_proj.weight",
        "encoder.17.self_attn.o_proj.weight",
        "encoder.17.mlp.gate_proj.weight",
        "encoder.17.mlp.up_proj.weight",
        "encoder.17.mlp.down_proj.weight",
        "encoder.17.input_layernorm.weight",
        "encoder.17.post_attention_layernorm.weight",
        "encoder.18.self_attn.q_proj.weight",
        "encoder.18.self_attn.k_proj.weight",
        "encoder.18.self_attn.v_proj.weight",
        "encoder.18.self_attn.o_proj.weight",
        "encoder.18.mlp.gate_proj.weight",
        "encoder.18.mlp.up_proj.weight",
        "encoder.18.mlp.down_proj.weight",
        "encoder.18.input_layernorm.weight",
        "encoder.18.post_attention_layernorm.weight",
        "encoder.19.self_attn.q_proj.weight",
        "encoder.19.self_attn.k_proj.weight",
        "encoder.19.self_attn.v_proj.weight",
        "encoder.19.self_attn.o_proj.weight",
        "encoder.19.mlp.gate_proj.weight",
        "encoder.19.mlp.up_proj.weight",
        "encoder.19.mlp.down_proj.weight",
        "encoder.19.input_layernorm.weight",
        "encoder.19.post_attention_layernorm.weight",
        "encoder.20.self_attn.q_proj.weight",
        "encoder.20.self_attn.k_proj.weight",
        "encoder.20.self_attn.v_proj.weight",
        "encoder.20.self_attn.o_proj.weight",
        "encoder.20.mlp.gate_proj.weight",
        "encoder.20.mlp.up_proj.weight",
        "encoder.20.mlp.down_proj.weight",
        "encoder.20.input_layernorm.weight",
        "encoder.20.post_attention_layernorm.weight",
        "encoder.21.self_attn.q_proj.weight",
        "encoder.21.self_attn.k_proj.weight",
        "encoder.21.self_attn.v_proj.weight",
        "encoder.21.self_attn.o_proj.weight",
        "encoder.21.mlp.gate_proj.weight",
        "encoder.21.mlp.up_proj.weight",
        "encoder.21.mlp.down_proj.weight",
        "encoder.21.input_layernorm.weight",
        "encoder.21.post_attention_layernorm.weight",
        "encoder.22.self_attn.q_proj.weight",
        "encoder.22.self_attn.k_proj.weight",
        "encoder.22.self_attn.v_proj.weight",
        "encoder.22.self_attn.o_proj.weight",
        "encoder.22.mlp.gate_proj.weight",
        "encoder.22.mlp.up_proj.weight",
        "encoder.22.mlp.down_proj.weight",
        "encoder.22.input_layernorm.weight",
        "encoder.22.post_attention_layernorm.weight",
        "encoder.23.self_attn.q_proj.weight",
        "encoder.23.self_attn.k_proj.weight",
        "encoder.23.self_attn.v_proj.weight",
        "encoder.23.self_attn.o_proj.weight",
        "encoder.23.mlp.gate_proj.weight",
        "encoder.23.mlp.up_proj.weight",
        "encoder.23.mlp.down_proj.weight",
        "encoder.23.input_layernorm.weight",
        "encoder.23.post_attention_layernorm.weight",
        "encoder.24.self_attn.q_proj.weight",
        "encoder.24.self_attn.k_proj.weight",
        "encoder.24.self_attn.v_proj.weight",
        "encoder.24.self_attn.o_proj.weight",
        "encoder.24.mlp.gate_proj.weight",
        "encoder.24.mlp.up_proj.weight",
        "encoder.24.mlp.down_proj.weight",
        "encoder.24.input_layernorm.weight",
        "encoder.24.post_attention_layernorm.weight",
        "encoder.25.self_attn.q_proj.weight",
        "encoder.25.self_attn.k_proj.weight",
        "encoder.25.self_attn.v_proj.weight",
        "encoder.25.self_attn.o_proj.weight",
        "encoder.25.mlp.gate_proj.weight",
        "encoder.25.mlp.up_proj.weight",
        "encoder.25.mlp.down_proj.weight",
        "encoder.25.input_layernorm.weight",
        "encoder.25.post_attention_layernorm.weight",
        "encoder.26.self_attn.q_proj.weight",
        "encoder.26.self_attn.k_proj.weight",
        "encoder.26.self_attn.v_proj.weight",
        "encoder.26.self_attn.o_proj.weight",
        "encoder.26.mlp.gate_proj.weight",
        "encoder.26.mlp.up_proj.weight",
        "encoder.26.mlp.down_proj.weight",
        "encoder.26.input_layernorm.weight",
        "encoder.26.post_attention_layernorm.weight",
        "encoder.27.self_attn.q_proj.weight",
        "encoder.27.self_attn.k_proj.weight",
        "encoder.27.self_attn.v_proj.weight",
        "encoder.27.self_attn.o_proj.weight",
        "encoder.27.mlp.gate_proj.weight",
        "encoder.27.mlp.up_proj.weight",
        "encoder.27.mlp.down_proj.weight",
        "encoder.27.input_layernorm.weight",
        "encoder.27.post_attention_layernorm.weight",
        "encoder.28.self_attn.q_proj.weight",
        "encoder.28.self_attn.k_proj.weight",
        "encoder.28.self_attn.v_proj.weight",
        "encoder.28.self_attn.o_proj.weight",
        "encoder.28.mlp.gate_proj.weight",
        "encoder.28.mlp.up_proj.weight",
        "encoder.28.mlp.down_proj.weight",
        "encoder.28.input_layernorm.weight",
        "encoder.28.post_attention_layernorm.weight",
        "encoder.29.self_attn.q_proj.weight",
        "encoder.29.self_attn.k_proj.weight",
        "encoder.29.self_attn.v_proj.weight",
        "encoder.29.self_attn.o_proj.weight",
        "encoder.29.mlp.gate_proj.weight",
        "encoder.29.mlp.up_proj.weight",
        "encoder.29.mlp.down_proj.weight",
        "encoder.29.input_layernorm.weight",
        "encoder.29.post_attention_layernorm.weight",
        "encoder.30.self_attn.q_proj.weight",
        "encoder.30.self_attn.k_proj.weight",
        "encoder.30.self_attn.v_proj.weight",
        "encoder.30.self_attn.o_proj.weight",
        "encoder.30.mlp.gate_proj.weight",
        "encoder.30.mlp.up_proj.weight",
        "encoder.30.mlp.down_proj.weight",
        "encoder.30.input_layernorm.weight",
        "encoder.30.post_attention_layernorm.weight",
        "encoder.31.self_attn.q_proj.weight",
        "encoder.31.self_attn.k_proj.weight",
        "encoder.31.self_attn.v_proj.weight",
        "encoder.31.self_attn.o_proj.weight",
        "encoder.31.mlp.gate_proj.weight",
        "encoder.31.mlp.up_proj.weight",
        "encoder.31.mlp.down_proj.weight",
        "encoder.31.input_layernorm.weight",
        "encoder.31.post_attention_layernorm.weight",
        "pooler.weight"
    ],
    "decoder_layers": [
        "pooler.dense.weight",
        "pooler.dense.bias",
        "wte.weight",
        "wpe.weight",
        "decoder.0.ln_1.weight",
        "decoder.0.ln_1.bias",
        "decoder.0.attn.c_attn.weight",
        "decoder.0.attn.c_attn.bias",
        "decoder.0.attn.c_proj.weight",
        "decoder.0.attn.c_proj.bias",
        "decoder.0.ln_2.weight",
        "decoder.0.ln_2.bias",
        "decoder.0.mlp.c_fc.weight",
        "decoder.0.mlp.c_fc.bias",
        "decoder.0.mlp.c_proj.weight",
        "decoder.0.mlp.c_proj.bias",
        "decoder.1.ln_1.weight",
        "decoder.1.ln_1.bias",
        "decoder.1.attn.c_attn.weight",
        "decoder.1.attn.c_attn.bias",
        "decoder.1.attn.c_proj.weight",
        "decoder.1.attn.c_proj.bias",
        "decoder.1.ln_2.weight",
        "decoder.1.ln_2.bias",
        "decoder.1.mlp.c_fc.weight",
        "decoder.1.mlp.c_fc.bias",
        "decoder.1.mlp.c_proj.weight",
        "decoder.1.mlp.c_proj.bias",
        "decoder.2.ln_1.weight",
        "decoder.2.ln_1.bias",
        "decoder.2.attn.c_attn.weight",
        "decoder.2.attn.c_attn.bias",
        "decoder.2.attn.c_proj.weight",
        "decoder.2.attn.c_proj.bias",
        "decoder.2.ln_2.weight",
        "decoder.2.ln_2.bias",
        "decoder.2.mlp.c_fc.weight",
        "decoder.2.mlp.c_fc.bias",
        "decoder.2.mlp.c_proj.weight",
        "decoder.2.mlp.c_proj.bias",
        "decoder.3.ln_1.weight",
        "decoder.3.ln_1.bias",
        "decoder.3.attn.c_attn.weight",
        "decoder.3.attn.c_attn.bias",
        "decoder.3.attn.c_proj.weight",
        "decoder.3.attn.c_proj.bias",
        "decoder.3.ln_2.weight",
        "decoder.3.ln_2.bias",
        "decoder.3.mlp.c_fc.weight",
        "decoder.3.mlp.c_fc.bias",
        "decoder.3.mlp.c_proj.weight",
        "decoder.3.mlp.c_proj.bias",
        "decoder.4.ln_1.weight",
        "decoder.4.ln_1.bias",
        "decoder.4.attn.c_attn.weight",
        "decoder.4.attn.c_attn.bias",
        "decoder.4.attn.c_proj.weight",
        "decoder.4.attn.c_proj.bias",
        "decoder.4.ln_2.weight",
        "decoder.4.ln_2.bias",
        "decoder.4.mlp.c_fc.weight",
        "decoder.4.mlp.c_fc.bias",
        "decoder.4.mlp.c_proj.weight",
        "decoder.4.mlp.c_proj.bias",
        "decoder.5.ln_1.weight",
        "decoder.5.ln_1.bias",
        "decoder.5.attn.c_attn.weight",
        "decoder.5.attn.c_attn.bias",
        "decoder.5.attn.c_proj.weight",
        "decoder.5.attn.c_proj.bias",
        "decoder.5.ln_2.weight",
        "decoder.5.ln_2.bias",
        "decoder.5.mlp.c_fc.weight",
        "decoder.5.mlp.c_fc.bias",
        "decoder.5.mlp.c_proj.weight",
        "decoder.5.mlp.c_proj.bias",
        "decoder.6.ln_1.weight",
        "decoder.6.ln_1.bias",
        "decoder.6.attn.c_attn.weight",
        "decoder.6.attn.c_attn.bias",
        "decoder.6.attn.c_proj.weight",
        "decoder.6.attn.c_proj.bias",
        "decoder.6.ln_2.weight",
        "decoder.6.ln_2.bias",
        "decoder.6.mlp.c_fc.weight",
        "decoder.6.mlp.c_fc.bias",
        "decoder.6.mlp.c_proj.weight",
        "decoder.6.mlp.c_proj.bias",
        "decoder.7.ln_1.weight",
        "decoder.7.ln_1.bias",
        "decoder.7.attn.c_attn.weight",
        "decoder.7.attn.c_attn.bias",
        "decoder.7.attn.c_proj.weight",
        "decoder.7.attn.c_proj.bias",
        "decoder.7.ln_2.weight",
        "decoder.7.ln_2.bias",
        "decoder.7.mlp.c_fc.weight",
        "decoder.7.mlp.c_fc.bias",
        "decoder.7.mlp.c_proj.weight",
        "decoder.7.mlp.c_proj.bias",
        "decoder.8.ln_1.weight",
        "decoder.8.ln_1.bias",
        "decoder.8.attn.c_attn.weight",
        "decoder.8.attn.c_attn.bias",
        "decoder.8.attn.c_proj.weight",
        "decoder.8.attn.c_proj.bias",
        "decoder.8.ln_2.weight",
        "decoder.8.ln_2.bias",
        "decoder.8.mlp.c_fc.weight",
        "decoder.8.mlp.c_fc.bias",
        "decoder.8.mlp.c_proj.weight",
        "decoder.8.mlp.c_proj.bias",
        "decoder.9.ln_1.weight",
        "decoder.9.ln_1.bias",
        "decoder.9.attn.c_attn.weight",
        "decoder.9.attn.c_attn.bias",
        "decoder.9.attn.c_proj.weight",
        "decoder.9.attn.c_proj.bias",
        "decoder.9.ln_2.weight",
        "decoder.9.ln_2.bias",
        "decoder.9.mlp.c_fc.weight",
        "decoder.9.mlp.c_fc.bias",
        "decoder.9.mlp.c_proj.weight",
        "decoder.9.mlp.c_proj.bias",
        "decoder.10.ln_1.weight",
        "decoder.10.ln_1.bias",
        "decoder.10.attn.c_attn.weight",
        "decoder.10.attn.c_attn.bias",
        "decoder.10.attn.c_proj.weight",
        "decoder.10.attn.c_proj.bias",
        "decoder.10.ln_2.weight",
        "decoder.10.ln_2.bias",
        "decoder.10.mlp.c_fc.weight",
        "decoder.10.mlp.c_fc.bias",
        "decoder.10.mlp.c_proj.weight",
        "decoder.10.mlp.c_proj.bias",
        "decoder.11.ln_1.weight",
        "decoder.11.ln_1.bias",
        "decoder.11.attn.c_attn.weight",
        "decoder.11.attn.c_attn.bias",
        "decoder.11.attn.c_proj.weight",
        "decoder.11.attn.c_proj.bias",
        "decoder.11.ln_2.weight",
        "decoder.11.ln_2.bias",
        "decoder.11.mlp.c_fc.weight",
        "decoder.11.mlp.c_fc.bias",
        "decoder.11.mlp.c_proj.weight",
        "decoder.11.mlp.c_proj.bias"
    ]
}