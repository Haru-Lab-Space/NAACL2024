{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install punctuators\n",
    "# !pip install flair\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "# from punctuators.models import SBDModelONNX\n",
    "from augment import AugmentData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = AugmentData()\n",
    "# sentence_pool, span_sentence_pool = ag.augment(\"/media/s24gb1/90a7e21c-edf4-4782-a0eb-731b73c521c2/DUC/NAACL2024/data/ECF 2.0/test/Subtask_1_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NOUN', 'white'), ('PUNCT', '.'), ('INTJ', 'okay'), ('PUNCT', '.')]\n",
      "spacy:['Whites .', 'Whites . Okay .', 'Whites . Okay . ', 'Okay .', 'Okay . ', 'Whites . Okay .', 'Whites .', 'Whites . Okay .', 'Okay .', 'Whites', 'Whites .', 'Whites . Okay .', 'Okay .']\n",
      "remove_list_start_by_coordinating_conjunction:['Whites .', 'Whites . Okay .', 'Whites . Okay . ', 'Okay .', 'Okay . ', 'Whites . Okay .', 'Whites .', 'Whites . Okay .', 'Okay .', 'Whites', 'Whites .', 'Whites . Okay .', 'Okay .']\n",
      "remove_duplicate:['Whites .', 'Whites . Okay .', 'Whites . Okay . ', 'Okay .', 'Okay . ', 'Whites']\n",
      "remove_unclear_content:['Whites .']\n",
      "remove_end_by_unclear_sentence:['Whites .']\n",
      "remove_dup_words:['Whites .']\n",
      "remove_dup_sentences:['Whites .']\n",
      "remove_unclear_last_sentence:['Whites .']\n",
      "remove_unclear_first_sentence:['Whites .']\n",
      "remove_interjections_in_the_end:['Whites .']\n",
      "remove_more_adj:['Whites .']\n",
      "remove_more_intj:['Whites .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Whites .']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Whites . Okay .\"\n",
    "# g.spacy(text)\n",
    "# l, x = ag.augment_sentence(text)\n",
    "# for i in range(len(l)):\n",
    "#     print(l[i])\n",
    "#     # if len(x[i])>2:\n",
    "#     print(x[i])\n",
    "ag.augment_sentence(text, test=True)\n",
    "# pool = ag.split_comma(text)\n",
    "# ag.concat_comma(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Hey ! I did ! But the store would not take her back ! So then I took her to the shelter , and you know what I found out ? If they can not find a home for her , they kill her ! And I am not gonna let that happen to little Yasmine ! Okay , good , good , good , cause , good , cause I was kinda having second thoughts too . Okay . And it is not just chicks you know ? It is all kinds of other animals !\"\n",
    "# g.spacy(text)\n",
    "# l, x = ag.augment_sentence(text)\n",
    "# for i in range(len(l)):\n",
    "#     print(l[i])\n",
    "#     # if len(x[i])>2:\n",
    "#     print(x[i])\n",
    "# s, u = ag.augment_sentence(text, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# # Load the English language model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Input sentence\n",
    "# sentence = \"You know you probably did not know this , but back in high school , I had a , um , major crush on you .\"\n",
    "\n",
    "# # Process the sentence with spaCy\n",
    "# doc = nlp(sentence)\n",
    "\n",
    "# # Split the sentence by pronouns\n",
    "# split_sentences = []\n",
    "# current_sentence = []\n",
    "\n",
    "# for token in doc:\n",
    "#     # Check if the token is a pronoun\n",
    "#     if token.pos_ == 'PRON':\n",
    "#         split_sentences.append(' '.join(current_sentence))\n",
    "#         current_sentence = [token.text]\n",
    "#     else:\n",
    "#         current_sentence.append(token.text)\n",
    "\n",
    "# # Add the remaining part of the sentence if any\n",
    "# if current_sentence:\n",
    "#     split_sentences.append(' '.join(current_sentence))\n",
    "\n",
    "# # Print the split sentences\n",
    "# for i, split_sentence in enumerate(split_sentences):\n",
    "#     print(f\"Sentence {i + 1}: {split_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: GraphEmbedding/: No such file or directory\n",
      "python: can't open file 'setup.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cd GraphEmbedding/\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Well , you might try accidentally breaking something valuable of hers , say her\"\n",
    "# ag.spacy(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation_chars = string.punctuation\n",
    "\n",
    "#         # Remove punctuation at the beginning and end of the string\n",
    "# cleaned_text = text.strip(punctuation_chars)\n",
    "# cleaned_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ga_aug_std",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
