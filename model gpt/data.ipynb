{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random\n",
    "from textattack.augmentation import WordNetAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    with open(path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "def write_json(path, data):\n",
    "    with open(path, 'w') as file:\n",
    "        # Write the Python data structure as JSON to the file\n",
    "        json.dump(data, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "config_path = \"config.json\"\n",
    "config = read_json(config_path)\n",
    "left_padding = config[\"left_padding\"]\n",
    "right_padding = config[\"right_padding\"]\n",
    "\n",
    "punctuation_error = [\n",
    "        \"%\"\n",
    "    ]\n",
    "wordnet_aug = WordNetAugmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # Create a string of all punctuation characters\n",
    "    punctuation_chars = string.punctuation\n",
    "\n",
    "    # Remove punctuation at the beginning and end of the string\n",
    "    cleaned_text = text.strip(punctuation_chars).strip().strip(punctuation_chars).strip().strip(punctuation_chars).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "def span(text, sub_text, s_ind=0):\n",
    "    sub_text = remove_punctuation(sub_text)\n",
    "    new_text = text.split()\n",
    "    if text == sub_text:\n",
    "        ss = s_ind\n",
    "        se = s_ind+len(new_text)\n",
    "        return ss, se, text\n",
    "    new_sub_text = sub_text\n",
    "    new_sub_text = new_sub_text.split()\n",
    "    check = True\n",
    "    for i in range(len(new_text)):\n",
    "        temp = i\n",
    "        for j in range(len(new_sub_text)):\n",
    "            if new_sub_text[j] != new_text[i + j]:\n",
    "                if j == len(new_sub_text)-1:\n",
    "                    for punctuation in punctuation_error:\n",
    "\n",
    "                        if new_sub_text[j]+punctuation == new_text[i + j]:\n",
    "                            # print(new_sub_text)\n",
    "                            new_sub_text[j] = new_sub_text[j]+punctuation \n",
    "                            ss = i\n",
    "                            se = i+j\n",
    "                            print(new_sub_text)\n",
    "                            return s_ind+ss, s_ind+se+1, ' '.join(new_sub_text)\n",
    "                check = False\n",
    "                break\n",
    "            else:\n",
    "                temp = i + j\n",
    "                check = True\n",
    "        if check == True:\n",
    "            ss = i\n",
    "            se = temp\n",
    "            return s_ind+ss, s_ind+se+1, sub_text\n",
    "\n",
    "def convertraw_to_train(data):\n",
    "    new_data = []\n",
    "    max_len = 0\n",
    "    for i in range(len(data)):\n",
    "        conversation_ID = data[i]['conversation_ID']\n",
    "        conversation = data[i]['conversation']\n",
    "\n",
    "        utterance_ID_pool = []\n",
    "        text_pool = []\n",
    "        speaker_pool = []\n",
    "        emotion_pool = {}\n",
    "        for j in range(len(conversation)):\n",
    "            utterance_ID_pool.append(conversation[j]['utterance_ID'])\n",
    "            text_pool.append(conversation[j]['text'])\n",
    "            if max_len < len(conversation[j]['text'].split()):\n",
    "                max_len = len(conversation[j]['text'].split())\n",
    "            speaker_pool.append(conversation[j]['speaker'])\n",
    "            emotion_pool[j+1] = {}\n",
    "            emotion_pool[j+1][\"property\"] = {}\n",
    "            emotion_pool[j+1][\"casual\"] = {}\n",
    "            if \"emotion\" in conversation[j].keys():\n",
    "                emotion_pool[j+1][\"property\"] = conversation[j]['emotion']\n",
    "\n",
    "        \n",
    "        if \"emotion-cause_pairs\" in data[i].keys():\n",
    "            cause_pairs_pool = []\n",
    "            cause_pairs = data[i]['emotion-cause_pairs']\n",
    "            for j in range(len(cause_pairs)):\n",
    "                emotion_utter = cause_pairs[j][0]\n",
    "                casual_utter = cause_pairs[j][1]\n",
    "                utter, emotion = emotion_utter.split(\"_\")\n",
    "                casual_utter, casual_text = casual_utter.split(\"_\")\n",
    "                # if conversation_ID == 1049:\n",
    "                #     print(\"cause_pairs[j][0]:\" +str(cause_pairs[j][0]))\n",
    "                #     print(\"cause_pairs[j][1]:\" +str(cause_pairs[j][1]))\n",
    "                #     print(\"utter:\" +str(utter))\n",
    "                #     print(\"casual_utter:\" +str(casual_utter))\n",
    "                #     print(\"int(utter) - left_padding :\" +str(int(utter) - left_padding))\n",
    "                #     print(\"int(utter) + right_padding :\" +str(int(utter) + right_padding))\n",
    "                if int(utter) - left_padding > int(casual_utter):\n",
    "                    continue\n",
    "                if int(utter) + right_padding < int(casual_utter):\n",
    "                    continue\n",
    "                start, end, _ = span(text_pool[int(casual_utter)-1], casual_text)\n",
    "                emotion_pool[int(utter)][\"casual\"][int(casual_utter)] = {\n",
    "                    \"casual_text\": casual_text,\n",
    "                    \"start\": start,\n",
    "                    \"end\": end\n",
    "                }\n",
    "\n",
    "\n",
    "        for j in range(len(conversation)):\n",
    "            start_ids = max(1, j-left_padding+1)\n",
    "            end_ids = min(j+right_padding+2, len(conversation)+1)\n",
    "            for k in range(start_ids, end_ids):\n",
    "                if k not in emotion_pool[j+1][\"casual\"].keys():\n",
    "                    emotion_pool[j+1][\"casual\"][k] = {\n",
    "                        \"casual_text\": \"\",\n",
    "                        \"start\": 0,\n",
    "                        \"end\": 0\n",
    "                    }\n",
    "\n",
    "            emotion_pool[j+1][\"casual\"] = dict(sorted(emotion_pool[j+1][\"casual\"].items()))\n",
    "\n",
    "            pool = []\n",
    "            for k in emotion_pool[j+1][\"casual\"].keys():\n",
    "                pool.append({\n",
    "                    \"utter_ID\": k,\n",
    "                    \"casual_text\": emotion_pool[j+1][\"casual\"][k][\"casual_text\"],\n",
    "                    \"start\": emotion_pool[j+1][\"casual\"][k][\"start\"],\n",
    "                    \"end\": emotion_pool[j+1][\"casual\"][k][\"end\"]\n",
    "                })\n",
    "            emotion_pool[j+1][\"casual\"] = pool\n",
    "\n",
    "            start_ids = max(0, j-left_padding)\n",
    "            end_ids = min(j+right_padding+1, len(conversation))\n",
    "            paragraph = \" \".join(text_pool[start_ids: end_ids])\n",
    "            \n",
    "            new_data.append({\n",
    "                \"conversation_ID\" : conversation_ID,\n",
    "                \"utterance_ID\" : utterance_ID_pool[j],\n",
    "                \"paragraph\": paragraph,\n",
    "                \"emotion\": emotion_pool[j+1],\n",
    "                \"text_pool\": text_pool[start_ids: end_ids],\n",
    "                \"speaker_pool\": speaker_pool[start_ids: end_ids],\n",
    "            })\n",
    "    print(max_len)\n",
    "    return new_data\n",
    "\n",
    "def count(data):\n",
    "    dictionary = {}\n",
    "\n",
    "    for item in data:\n",
    "        # print( dictionary.keys())\n",
    "        emotion = item['emotion']['property']\n",
    "        if emotion in dictionary.keys():\n",
    "            # print(emotion)\n",
    "            dictionary[emotion] +=1\n",
    "        else:\n",
    "            # print(emotion)\n",
    "            dictionary[emotion] =1\n",
    "    return dictionary\n",
    "\n",
    "def count_dict(data):\n",
    "    dictionary = {}\n",
    "\n",
    "    for item in data.keys():\n",
    "        dictionary[item] = len(data[item])\n",
    "    return dictionary\n",
    "\n",
    "def count_dis(data):\n",
    "    dict_dis = {}\n",
    "    num = 0\n",
    "    for i in range(len(data)):\n",
    "        emotion_casual_pairs = data[i]['emotion-cause_pairs']\n",
    "        for j in range(len(emotion_casual_pairs)):\n",
    "            emotion_utter = emotion_casual_pairs[j][0]\n",
    "            casual_utter = emotion_casual_pairs[j][1]\n",
    "            utter, emotion = emotion_utter.split(\"_\")\n",
    "            casual_utter, casual_text = casual_utter.split(\"_\")\n",
    "            utter = int(utter)\n",
    "            casual_utter = int(casual_utter)\n",
    "            num += 1\n",
    "            if utter - casual_utter not in dict_dis.keys():\n",
    "                dict_dis[utter - casual_utter] = 1\n",
    "            else:\n",
    "                dict_dis[utter - casual_utter] += 1\n",
    "\n",
    "    dict_dis = dict(sorted(dict_dis.items(), key=lambda item: item[1]))\n",
    "    dict_r = {}\n",
    "    for j in dict_dis.keys():\n",
    "        dict_r[j] = dict_dis[j]/num\n",
    "    \n",
    "    return dict_dis, dict_r\n",
    "def augment_data(data):\n",
    "    dict = {}\n",
    "    for item in data:\n",
    "        property = item['emotion']['property']\n",
    "        if property not in dict:\n",
    "            dict[property] = [item]\n",
    "        else:\n",
    "            dict[property].append(item)\n",
    "\n",
    "    num_elements_to_choose = len(dict['neutral']) // 3\n",
    "\n",
    "    # Randomly choose one-third of the elements\n",
    "    dict['neutral'] = random.sample(dict['neutral'], num_elements_to_choose)\n",
    "\n",
    "    count_appear = count_dict(dict)\n",
    "    max_appear = max(count_appear.values())\n",
    "\n",
    "    num_augment = {}\n",
    "    for emotion in count_appear.keys():\n",
    "        num_augment[emotion] = max_appear - count_appear[emotion] \n",
    "    print(\"num_augment : \"+str(num_augment))\n",
    "\n",
    "    augment_dict = {\n",
    "        \"joy\": [],\n",
    "        \"neutral\": []\n",
    "    }\n",
    "    \n",
    "    for emotion in dict.keys():\n",
    "        for i in range(num_augment[emotion]):\n",
    "            index = i % len(dict[emotion])\n",
    "            sample = augment_sample(dict[emotion][index])\n",
    "            if emotion not in augment_dict:\n",
    "                augment_dict[emotion]=[sample]\n",
    "            else:\n",
    "                augment_dict[emotion].append(sample)\n",
    "            # print(dict[emotion][index])\n",
    "            # print(sample)\n",
    "            # print(\"====\")\n",
    "\n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    print(dict.keys())\n",
    "    print(augment_dict.keys())\n",
    "    for emotion in dict.keys():\n",
    "        print(emotion)\n",
    "        list = dict[emotion] + augment_dict[emotion]\n",
    "        n_train = int(0.8*len(list))\n",
    "        train_dict[emotion] = list[:n_train]\n",
    "        test_dict[emotion] = list[n_train:]\n",
    "        train_data.extend(train_dict[emotion])\n",
    "        test_data.extend(test_dict[emotion])\n",
    "    # new_data = neutral_data[0:int(len(neutral_data)/3)] + other_data\n",
    "\n",
    "\n",
    "\n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(test_data)\n",
    "    return train_data, test_data\n",
    "def augment_sample(sample):\n",
    "    emotion = sample[\"emotion\"]\n",
    "    text_pool = sample[\"text_pool\"]\n",
    "\n",
    "    for i in range(len(text_pool)):\n",
    "        if emotion['casual'][i][\"casual_text\"] == \"\":\n",
    "            text_pool[i] = augment_text(text_pool[i])[0]\n",
    "            # print(augment_text(text_pool[i]))\n",
    "\n",
    "    gpt_paragraph = \"\"\n",
    "    paragraph = \"\"\n",
    "    for i in range(len(text_pool)):\n",
    "        # print(new_text_pool[i])\n",
    "        paragraph += \" \" + text_pool[i]\n",
    "        gpt_paragraph += \" \\\" \" + text_pool[i] + \" \\\"\"\n",
    "\n",
    "    new_sample = {\n",
    "        'conversation_ID': sample['conversation_ID'],\n",
    "        'utterance_ID': sample['utterance_ID'],\n",
    "        'paragraph': paragraph,\n",
    "        'gpt_paragraph':  gpt_paragraph,\n",
    "        'emotion': \n",
    "        {\n",
    "            'property': sample['emotion']['property'],\n",
    "            'casual': sample['emotion']['casual']\n",
    "        },\n",
    "        'text_pool': text_pool,\n",
    "        'speaker_pool': sample['speaker_pool']\n",
    "    }\n",
    "    return new_sample\n",
    "def augment_text(text):\n",
    "    return wordnet_aug.augment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({-10: 1,\n",
       "  13: 1,\n",
       "  14: 1,\n",
       "  -9: 1,\n",
       "  -8: 2,\n",
       "  12: 2,\n",
       "  11: 3,\n",
       "  -6: 4,\n",
       "  -5: 4,\n",
       "  10: 5,\n",
       "  9: 13,\n",
       "  -4: 15,\n",
       "  8: 17,\n",
       "  7: 26,\n",
       "  -3: 27,\n",
       "  6: 44,\n",
       "  5: 101,\n",
       "  -2: 122,\n",
       "  4: 160,\n",
       "  -1: 201,\n",
       "  3: 332,\n",
       "  2: 813,\n",
       "  1: 2762,\n",
       "  0: 4707},\n",
       " {-10: 0.00010679196924391286,\n",
       "  13: 0.00010679196924391286,\n",
       "  14: 0.00010679196924391286,\n",
       "  -9: 0.00010679196924391286,\n",
       "  -8: 0.00021358393848782572,\n",
       "  12: 0.00021358393848782572,\n",
       "  11: 0.0003203759077317386,\n",
       "  -6: 0.00042716787697565144,\n",
       "  -5: 0.00042716787697565144,\n",
       "  10: 0.0005339598462195643,\n",
       "  9: 0.001388295600170867,\n",
       "  -4: 0.0016018795386586928,\n",
       "  8: 0.0018154634771465185,\n",
       "  7: 0.002776591200341734,\n",
       "  -3: 0.002883383169585647,\n",
       "  6: 0.004698846646732166,\n",
       "  5: 0.010785988893635199,\n",
       "  -2: 0.013028620247757368,\n",
       "  4: 0.01708671507902606,\n",
       "  -1: 0.021465185818026485,\n",
       "  3: 0.03545493378897907,\n",
       "  2: 0.08682187099530116,\n",
       "  1: 0.2949594190516873,\n",
       "  0: 0.5026697992310978})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_json(\"../data/ECF 2.0/train/Subtask_1_train.json\")\n",
    "count_dis(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "data = read_json(\"../data/ECF 2.0/train/Subtask_1_train.json\")\n",
    "# count_dis(data)\n",
    "new_data= convertraw_to_train(data)\n",
    "# write_json(\"../data/ECF 2.0/train/train.json\", new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_augment : {'neutral': 325, 'surprise': 461, 'anger': 686, 'sadness': 1154, 'joy': 0, 'disgust': 1887, 'fear': 1928}\n",
      "dict_keys(['neutral', 'surprise', 'anger', 'sadness', 'joy', 'disgust', 'fear'])\n",
      "dict_keys(['joy', 'neutral', 'surprise', 'anger', 'sadness', 'disgust', 'fear'])\n",
      "neutral\n",
      "surprise\n",
      "anger\n",
      "sadness\n",
      "joy\n",
      "disgust\n",
      "fear\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = augment_data(new_data)\n",
    "write_json(\"../data/ECF 2.0/train/augment_train.json\", train_data)\n",
    "write_json(\"../data/ECF 2.0/train/augment_valid.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'surprise': 1840,\n",
       " 'neutral': 1840,\n",
       " 'joy': 1840,\n",
       " 'sadness': 1840,\n",
       " 'fear': 1840,\n",
       " 'anger': 1840,\n",
       " 'disgust': 1840}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 461,\n",
       " 'fear': 461,\n",
       " 'neutral': 461,\n",
       " 'disgust': 461,\n",
       " 'surprise': 461,\n",
       " 'sadness': 461,\n",
       " 'joy': 461}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "data = read_json(\"../data/ECF 2.0/test/Subtask_1_test.json\")\n",
    "new_data = convertraw_to_train(data)\n",
    "write_json(\"../data/ECF 2.0/test/test.json\", new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
